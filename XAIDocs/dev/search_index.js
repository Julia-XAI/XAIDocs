var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API Reference","title":"API reference","text":"","category":"section"},{"location":"api/#Using-analyzers","page":"API Reference","title":"Using analyzers","text":"Most methods in the Julia-XAI ecosystem work by calling analyze on an input and an analyzer:\n\nThe return type of analyze is an Explanation:","category":"section"},{"location":"api/#Visualizing-explanations","page":"API Reference","title":"Visualizing explanations","text":"Explanations can be visualized using heatmap:","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#XAIBase.analyze","page":"API Reference","title":"XAIBase.analyze","text":"analyze(input, method)\nanalyze(input, method, output_selection)\n\nApply the analyzer method for the given input, returning an Explanation. If output_selection is specified, the explanation will be calculated for that output. Otherwise, the output with the highest activation is automatically chosen.\n\nSee also Explanation.\n\n\n\n\n\n","category":"function"},{"location":"api/#XAIBase.Explanation","page":"API Reference","title":"XAIBase.Explanation","text":"Explanation(val, output, output_selection, analyzer, heatmap, extras)\n\nReturn type of analyzers when calling analyze.\n\nFields\n\nval: numerical output of the analyzer, e.g. an attribution or gradient\noutput: model output for the given analyzer input\noutput_selection: index of the output used for the explanation\nanalyzer: symbol corresponding the used analyzer, e.g. :Gradient or :LRP\nheatmap: symbol indicating a preset heatmapping style,   e.g. :attribution, :sensitivity or :cam\nextras: optional named tuple that can be used by analyzers   to return additional information.\n\n\n\n\n\n","category":"type"},{"location":"api/#VisionHeatmaps.heatmap","page":"API Reference","title":"VisionHeatmaps.heatmap","text":"heatmap(x::AbstractArray)\nheatmap(x::AbstractArray, pipeline)\nheatmap(x::AbstractArray, image)\nheatmap(x::AbstractArray, image, pipeline)\n\nVisualize 4D arrays as heatmaps, assuming the WHCN convention for input array dimensions (width, height, color channels, batch dimension).\n\n\n\n\n\nheatmap(expl::Explanation)\nheatmap(expl::Explanation, pipeline)\nheatmap(expl::Explanation, image)   \nheatmap(expl::Explanation, image, pipeline)\n\nVisualize Explanation from XAIBase as a vision heatmap. Assumes WHCN convention (width, height, channels, batch dimension) for explanation.val. This will use the default heatmapping style for the given type of explanation.\n\n\n\n\n\nheatmap(input::AbstractArray, analyzer::AbstractXAIMethod)\nheatmap(input::AbstractArray, analyzer::AbstractXAIMethod, image)\n\nCompute an Explanation for a given input using the XAI method analyzer and visualize it as a vision heatmap. This will use the default heatmapping style for the given type of explanation.\n\n\n\n\n\n","category":"function"},{"location":"api/#TextHeatmaps.heatmap","page":"API Reference","title":"TextHeatmaps.heatmap","text":"heatmap(values::AbstractArray, words)\n\nCreate a heatmap of words where the background color of each word is determined by its corresponding value. Arguments values and words (and optionally colors) must have the same size.\n\nKeyword arguments\n\ncolorscheme::Union{ColorScheme,Symbol}: color scheme from ColorSchemes.jl. Defaults to ColorSchemes.seismic.\nrangescale::Symbol: selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either :extrema or :centered. Defaults to :centered for use with the default color scheme seismic.\n\n\n\n\n\nheatmap(expl::Explanation, text)\n\nVisualize Explanation from XAIBase as text heatmap. Text should be a vector containing vectors of strings, one for each input in the batched explanation.\n\nThis will use the default heatmapping style for the given type of explanation. Defaults can be overridden via keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"generated/example/#docs-getting-started","page":"Getting Started","title":"Using Analyzers","text":"For this first example, we loaded a pre-trained LeNet5 model to look at explanations on the MNIST dataset.\n\nNote that most of the methods in the Julia-XAI ecosystem are not limited to Flux.jl models.\n\nusing ExplainableAI\nusing RelevancePropagation\nusing Zygote\nusing Flux\n\nusing BSON # hide\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model\n\nnote: Loading XAI methods\nCurrently, the following methods are available in the Julia-XAI ecosystem:ExplainableAI.jl – applicable on any differentiable classifier\nGradient\nInputTimesGradient\nSmoothGrad\nIntegratedGradients\nGradCAM\nRelevancePropagation.jl – applicable on Flux.jl models\nLRP\nCRPFor a complete list of available methods, refer to the individual package documentations.","category":"section"},{"location":"generated/example/#Preparing-the-input-data","page":"Getting Started","title":"Preparing the input data","text":"We use MLDatasets to load a single image from the MNIST dataset:\n\nusing MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\n\nconvert2image(MNIST, x)\n\nBy convention in Flux.jl, this input needs to be resized to WHCN format by adding a color channel and batch dimensions.\n\ninput = reshape(x, 28, 28, 1, :);\nnothing #hide\n\nnote: Input format\nFor any explanation of a model, Julia-XAI methods assume the batch dimension to come last in the input.For vision models, the input is assumed to be in WHCN order (width, height, channels, batch dimension), which matches Flux.jl's convention.","category":"section"},{"location":"generated/example/#Explanations","page":"Getting Started","title":"Explanations","text":"We can now select an analyzer of our choice and call analyze to get an Explanation. Here, we use the LRP method from RelevancePropagation.jl:\n\nanalyzer = LRP(model)\nexpl = analyze(input, analyzer);\nnothing #hide\n\nThe return value expl is of type Explanation and bundles the following data:\n\nexpl.val: numerical output of the analyzer, e.g. an attribution or gradient\nexpl.output: model output for the given analyzer input\nexpl.output_selection: index of the output used for the explanation\nexpl.analyzer: symbol corresponding the used analyzer, e.g. :Gradient or :LRP\nexpl.heatmap: symbol indicating a preset heatmapping style,   e.g. :attribution, :sensitivity or :cam\nexpl.extras: optional named tuple that can be used by analyzers   to return additional information.\n\nWe used an LRP analyzer, so expl.analyzer is :LRP.\n\nexpl.analyzer\n\nWhich by default uses the :attribution preset for heatmapping:\n\nexpl.heatmap\n\nBy default, the explanation is computed for the maximally activated output. Since our digit is a 9 and Julia's indexing is 1-based, the output at index 10 of our trained model is maximally activated.\n\nFinally, we obtain the result of the analyzer in form of an array.\n\nexpl.val","category":"section"},{"location":"generated/example/#Heatmapping-basics","page":"Getting Started","title":"Heatmapping basics","text":"Since the array expl.val is not very informative at first sight, we can visualize Explanations by computing a heatmap using either VisionHeatmaps.jl or TextHeatmaps.jl.\n\nusing VisionHeatmaps\n\nheatmap(expl)\n\nBy default, to support batched explanations, a vector of heatmaps is returned. Since the following examples don't use batches, we will use the only function to unpack singleton heatmaps:\n\nheatmap(expl) |> only\n\nIf we are only interested in the heatmap, we can combine analysis and heatmapping into a single function call:\n\nheatmap(input, analyzer) |> only","category":"section"},{"location":"generated/example/#Output-selection","page":"Getting Started","title":"Output selection","text":"By passing an additional index to our call to analyze, we can compute an explanation with respect to a specific output index. Let's see why the output wasn't interpreted as a 4 (output at index 5)\n\nexpl = analyze(input, analyzer, 5)\nheatmap(expl) |> only\n\nThis heatmap shows us that the \"upper loop\" of the hand-drawn 9 has negative relevance with respect to the output corresponding to digit 4!\n\nThe output index can also be specified when calling heatmap:\n\nheatmap(input, analyzer, 5) |> only","category":"section"},{"location":"generated/example/#Analyzing-batches","page":"Getting Started","title":"Analyzing batches","text":"ExplainableAI also supports explanations of input batches:\n\nbatchsize = 20\nxs, _ = MNIST(Float32, :test)[1:batchsize]\nbatch = reshape(xs, 28, 28, 1, :) # reshape to WHCN format\nexpl = analyze(batch, analyzer);\nnothing #hide\n\nThis will return a single Explanation expl for the entire batch. Calling heatmap on expl will detect the batch dimension and return a vector of heatmaps.\n\nheatmap(expl)","category":"section"},{"location":"generated/example/#Heatmapping-presets","page":"Getting Started","title":"Heatmapping presets","text":"The function heatmap automatically applies common presets for each method. Since LRP computes attributions, previous heatmaps were shown in a divergent blue-black-red color scheme. Methods based on input sensitivities like SmoothGrad however are typically shown in a sequential color scheme:\n\nanalyzer = SmoothGrad(model)\nheatmap(input, analyzer) |> only","category":"section"},{"location":"generated/example/#Custom-heatmaps","page":"Getting Started","title":"Custom heatmaps","text":"tip: Check out the VisionHeatmaps.jl documentation\nUsing VisionHeatmaps.jl, heatmaps can be heavily customized. Check out the heatmapping documentation for more information.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#Explainable-AI-in-Julia","page":"Home","title":"Explainable AI in Julia","text":"The Julia-XAI ecosystem hosts Explainable AI (XAI)  methods written in the Julia programming language, with a focus on post-hoc, local input-space explanations of black-box models. In simpler terms, methods that try to answer the question  \"Which part of the input is responsible for the model's output?\".\n\n(Image: Julia-XAI organization)\n\nThe ecosystem is organized into several packages. As a user, you only need to install the packages that implement the methods you want to use.\n\nAs a developer, you can make use of the XAIBase.jl interface to quickly implement or prototype new methods without having to write boilerplate code.","category":"section"},{"location":"#New-Users","page":"Home","title":"New Users","text":"Our recommended starting point into the Julia-XAI ecosystem is the  Getting Started guide.","category":"section"},{"location":"#New-Developers","page":"Home","title":"New Developers","text":"If you want to implement an XAI method, take a look at the common interface defined in XAIBase.jl.","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"We welcome all contributions to the Julia-XAI ecosystem! Please contact us if you want your package to be included in this organization.","category":"section"}]
}

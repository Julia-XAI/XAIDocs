<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/XAIDocs/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Basic usage · RelevancePropagation.jl</title><meta content="Basic usage · RelevancePropagation.jl" name="title"/><meta content="Basic usage · RelevancePropagation.jl" property="og:title"/><meta content="Basic usage · RelevancePropagation.jl" property="twitter:title"/><meta content="Documentation for RelevancePropagation.jl." name="description"/><meta content="Documentation for RelevancePropagation.jl." property="og:description"/><meta content="Documentation for RelevancePropagation.jl." property="twitter:description"/><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="https://julia-xai.github.io/XAIDocs/RelevancePropagation/stable/generated/basics/" rel="canonical"/><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../../XAIDocs/">Getting Started</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Methods</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../../../ExplainableAI/">ExplainableAI.jl</a><a class="nav-link active nav-item" href="../../../">RelevancePropagation.jl</a></ul></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Heatmapping</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../../../VisionHeatmaps/">VisionHeatmaps.jl</a><a class="nav-link nav-item" href="../../../../TextHeatmaps/">TextHeatmaps.jl</a></ul></div><a class="nav-link nav-item" href="../../../../XAIBase/">Interface</a><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="RelevancePropagation.jl logo" src="../../assets/logo.png"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RelevancePropagation.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li class="is-active"><a class="tocitem" href="">Basic usage</a><ul class="internal"><li><a class="tocitem" href="#docs-lrp-model-prep"><span>Model preparation</span></a></li><li><a class="tocitem" href="#LRP-rules"><span>LRP rules</span></a></li><li><a class="tocitem" href="#docs-lrp-layerwise"><span>Computing layerwise relevances</span></a></li><li><a class="tocitem" href="#docs-lrp-performance"><span>Performance tips</span></a></li></ul></li><li><span class="tocitem">Advanced usage</span><ul><li><a class="tocitem" href="../composites/">Assigning rules to layers</a></li><li><a class="tocitem" href="../custom_layer/">Supporting new layer types</a></li><li><a class="tocitem" href="../custom_rules/">Custom LRP rules</a></li><li><a class="tocitem" href="../crp/">Concept Relevance Propagation</a></li><li><a class="tocitem" href="../../developer/">Developer documentation</a></li></ul></li><li><a class="tocitem" href="../../api/">API reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="">Basic usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Basic usage</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Julia-XAI/RelevancePropagation.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Julia-XAI/RelevancePropagation.jl/blob/main/docs/src/literate/basics.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" href="javascript:;" id="documenter-article-toggle-button" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="docs-lrp-basics"><a class="docs-heading-anchor" href="#docs-lrp-basics">Basic usage of LRP</a><a id="docs-lrp-basics-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-basics" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This package is part the <a href="https://github.com/Julia-XAI">Julia-XAI ecosystem</a>. For an introduction to the ecosystem, please refer to the <a href="https://julia-xai.github.io/XAIDocs/"><em>Getting started</em> guide</a>.</p></div></div><p>We start out by loading a small convolutional neural network:</p><pre><code class="language-julia hljs">using RelevancePropagation
using Flux

model = Chain(
    Chain(
        Conv((3, 3), 3 =&gt; 8, relu; pad=1),
        Conv((3, 3), 8 =&gt; 8, relu; pad=1),
        MaxPool((2, 2)),
        Conv((3, 3), 8 =&gt; 16; pad=1),
        BatchNorm(16, relu),
        Conv((3, 3), 16 =&gt; 8, relu; pad=1),
        BatchNorm(8, relu),
    ),
    Chain(Flux.flatten, Dense(2048 =&gt; 512, relu), Dropout(0.5), Dense(512 =&gt; 100, softmax)),
);</code></pre><p>This model contains two chains: the convolutional layers and the fully connected layers.</p><h2 id="docs-lrp-model-prep"><a class="docs-heading-anchor" href="#docs-lrp-model-prep">Model preparation</a><a id="docs-lrp-model-prep-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-model-prep" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">TLDR</header><div class="admonition-body"><ol><li>Use <a href="../../api/#RelevancePropagation.strip_softmax"><code>strip_softmax</code></a> to strip the output softmax from your model. Otherwise <a href="../custom_layer/#docs-lrp-model-checks">model checks</a> will fail.</li><li>Use <a href="../../api/#RelevancePropagation.canonize"><code>canonize</code></a> to fuse linear layers.</li><li>Don't just call <code>LRP(model)</code>, instead use a <a href="../../api/#RelevancePropagation.Composite"><code>Composite</code></a> to apply LRP rules to your model. Read <a href="../composites/#docs-composites"><em>Assigning rules to layers</em></a>.</li><li>By default, <code>LRP</code> will call <a href="../../api/#RelevancePropagation.flatten_model"><code>flatten_model</code></a> to flatten your model. This reduces computational overhead.</li></ol></div></div><h3 id="docs-lrp-strip-softmax"><a class="docs-heading-anchor" href="#docs-lrp-strip-softmax">Stripping the output softmax</a><a id="docs-lrp-strip-softmax-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-strip-softmax" title="Permalink"></a></h3><p>When using LRP, it is recommended to explain output logits instead of probabilities. This can be done by stripping the output softmax activation from the model using the <a href="../../api/#RelevancePropagation.strip_softmax"><code>strip_softmax</code></a> function:</p><pre><code class="language-julia hljs">model = strip_softmax(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Chain(
    Conv((3, 3), 3 =&gt; 8, relu, pad=1),  <span class="sgr90"># 224 parameters</span>
    Conv((3, 3), 8 =&gt; 8, relu, pad=1),  <span class="sgr90"># 584 parameters</span>
    MaxPool((2, 2)),
    Conv((3, 3), 8 =&gt; 16, pad=1),       <span class="sgr90"># 1_168 parameters</span>
    BatchNorm(16, relu),                <span class="sgr90"># 32 parameters, plus 32</span>
    Conv((3, 3), 16 =&gt; 8, relu, pad=1),  <span class="sgr90"># 1_160 parameters</span>
    BatchNorm(8, relu),                 <span class="sgr90"># 16 parameters, plus 16</span>
  ),
  Chain(
    Flux.flatten,
    Dense(2048 =&gt; 512, relu),           <span class="sgr90"># 1_049_088 parameters</span>
    Dropout(0.5),
    Dense(512 =&gt; 100),                  <span class="sgr90"># 51_300 parameters</span>
  ),
) <span class="sgr90">        # Total: 16 trainable arrays, </span>1_103_572 parameters,
<span class="sgr90">          # plus 4 non-trainable, 48 parameters, summarysize </span>4.213 MiB.</code></pre><p>If you don't remove the output softmax, <a href="../custom_layer/#docs-lrp-model-checks">model checks</a> will fail.</p><h3 id="docs-lrp-canonization"><a class="docs-heading-anchor" href="#docs-lrp-canonization">Canonizing the model</a><a id="docs-lrp-canonization-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-canonization" title="Permalink"></a></h3><p>LRP is not invariant to a model's implementation. Applying the <a href="../../api/#RelevancePropagation.GammaRule"><code>GammaRule</code></a> to two linear layers in a row will yield different results than first fusing the two layers into one linear layer and then applying the rule. This fusing is called "canonization" and can be done using the <a href="../../api/#RelevancePropagation.canonize"><code>canonize</code></a> function:</p><pre><code class="language-julia hljs">model_canonized = canonize(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Conv((3, 3), 3 =&gt; 8, relu, pad=1),    <span class="sgr90"># 224 parameters</span>
  Conv((3, 3), 8 =&gt; 8, relu, pad=1),    <span class="sgr90"># 584 parameters</span>
  MaxPool((2, 2)),
  Conv((3, 3), 8 =&gt; 16, relu, pad=1),   <span class="sgr90"># 1_168 parameters</span>
  Conv((3, 3), 16 =&gt; 8, relu, pad=1),   <span class="sgr90"># 1_160 parameters</span>
  BatchNorm(8, relu),                   <span class="sgr90"># 16 parameters, plus 16</span>
  Flux.flatten,
  Dense(2048 =&gt; 512, relu),             <span class="sgr90"># 1_049_088 parameters</span>
  Dropout(0.5),
  Dense(512 =&gt; 100),                    <span class="sgr90"># 51_300 parameters</span>
) <span class="sgr90">        # Total: 14 trainable arrays, </span>1_103_540 parameters,
<span class="sgr90">          # plus 2 non-trainable, 16 parameters, summarysize </span>4.212 MiB.</code></pre><p>After canonization, the first <code>BatchNorm</code> layer has been fused into the preceding <code>Conv</code> layer. The second <code>BatchNorm</code> layer wasn't fused since its preceding <code>Conv</code> layer has a ReLU activation function.</p><h3 id="docs-lrp-flatten-model"><a class="docs-heading-anchor" href="#docs-lrp-flatten-model">Flattening the model</a><a id="docs-lrp-flatten-model-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-flatten-model" title="Permalink"></a></h3><p>RelevancePropagation.jl's LRP implementation supports nested Flux Chains and Parallel layers. However, it is recommended to flatten the model before analyzing it.</p><p>LRP is implemented by first running a forward pass through the model, keeping track of the intermediate activations, followed by a backward pass that computes the relevances.</p><p>To keep the LRP implementation simple and maintainable, RelevancePropagation.jl does not pre-compute "nested" activations. Instead, for every internal chain, a new forward pass is run to compute activations.</p><p>By "flattening" a model, this overhead can be avoided. For this purpose, RelevancePropagation.jl provides the function <a href="../../api/#RelevancePropagation.flatten_model"><code>flatten_model</code></a>:</p><pre><code class="language-julia hljs">model_flat = flatten_model(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Conv((3, 3), 3 =&gt; 8, relu, pad=1),    <span class="sgr90"># 224 parameters</span>
  Conv((3, 3), 8 =&gt; 8, relu, pad=1),    <span class="sgr90"># 584 parameters</span>
  MaxPool((2, 2)),
  Conv((3, 3), 8 =&gt; 16, pad=1),         <span class="sgr90"># 1_168 parameters</span>
  BatchNorm(16, relu),                  <span class="sgr90"># 32 parameters, plus 32</span>
  Conv((3, 3), 16 =&gt; 8, relu, pad=1),   <span class="sgr90"># 1_160 parameters</span>
  BatchNorm(8, relu),                   <span class="sgr90"># 16 parameters, plus 16</span>
  Flux.flatten,
  Dense(2048 =&gt; 512, relu),             <span class="sgr90"># 1_049_088 parameters</span>
  Dropout(0.5),
  Dense(512 =&gt; 100),                    <span class="sgr90"># 51_300 parameters</span>
) <span class="sgr90">        # Total: 16 trainable arrays, </span>1_103_572 parameters,
<span class="sgr90">          # plus 4 non-trainable, 48 parameters, summarysize </span>4.212 MiB.</code></pre><p>This function is called by default when creating an LRP analyzer. Note that we pass the unflattened model to the analyzer, but <code>analyzer.model</code> is flattened:</p><pre><code class="language-julia hljs">analyzer = LRP(model)
analyzer.model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Conv((3, 3), 3 =&gt; 8, relu, pad=1),    <span class="sgr90"># 224 parameters</span>
  Conv((3, 3), 8 =&gt; 8, relu, pad=1),    <span class="sgr90"># 584 parameters</span>
  MaxPool((2, 2)),
  Conv((3, 3), 8 =&gt; 16, pad=1),         <span class="sgr90"># 1_168 parameters</span>
  BatchNorm(16, relu),                  <span class="sgr90"># 32 parameters, plus 32</span>
  Conv((3, 3), 16 =&gt; 8, relu, pad=1),   <span class="sgr90"># 1_160 parameters</span>
  BatchNorm(8, relu),                   <span class="sgr90"># 16 parameters, plus 16</span>
  Flux.flatten,
  Dense(2048 =&gt; 512, relu),             <span class="sgr90"># 1_049_088 parameters</span>
  Dropout(0.5),
  Dense(512 =&gt; 100),                    <span class="sgr90"># 51_300 parameters</span>
) <span class="sgr90">        # Total: 16 trainable arrays, </span>1_103_572 parameters,
<span class="sgr90">          # plus 4 non-trainable, 48 parameters, summarysize </span>4.212 MiB.</code></pre><p>If this flattening is not desired, it can be disabled by passing the keyword argument <code>flatten=false</code> to the <code>LRP</code> constructor.</p><h2 id="LRP-rules"><a class="docs-heading-anchor" href="#LRP-rules">LRP rules</a><a id="LRP-rules-1"></a><a class="docs-heading-anchor-permalink" href="#LRP-rules" title="Permalink"></a></h2><p>By default, the <code>LRP</code> constructor will assign the <a href="../../api/#RelevancePropagation.ZeroRule"><code>ZeroRule</code></a> to all layers.</p><pre><code class="language-julia hljs">LRP(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Conv((3, 3), 3 =&gt; 8, relu, pad=1) <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Conv((3, 3), 8 =&gt; 8, relu, pad=1) <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  MaxPool((2, 2))                   <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Conv((3, 3), 8 =&gt; 16, pad=1)      <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  BatchNorm(16, relu)               <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Conv((3, 3), 16 =&gt; 8, relu, pad=1)<span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  BatchNorm(8, relu)                <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Flux.flatten                      <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Dense(2048 =&gt; 512, relu)          <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Dropout(0.5)                      <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Dense(512 =&gt; 100)                 <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
)</code></pre><p>This analyzer will return heatmaps that look identical to the <code>InputTimesGradient</code> analyzer from <a href="https://github.com/Julia-XAI/ExplainableAI.jl">ExplainableAI.jl</a>.</p><p>LRP's strength lies in assigning different rules to different layers, based on their functionality in the neural network<sup class="footnote-reference"><a href="#footnote-1" id="citeref-1">[1]</a></sup>. RelevancePropagation.jl <a href="../../api/#api-lrp-rules">implements many LRP rules out of the box</a>, but it is also possible to <a href="../custom_rules/#docs-custom-rules"><em>implement custom rules</em></a>.</p><p>To assign different rules to different layers, use one of the <a href="../../api/#api-composite-presets">composites presets</a>, or create your own composite, as described in <a href="../composites/#docs-composites"><em>Assigning rules to layers</em></a>.</p><pre><code class="language-julia hljs">composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Composite(
  GlobalTypeMap(  <span class="sgr90"># all layers</span>
<span class="sgr94">    Flux.Conv              </span> =&gt; <span class="sgr33">ZPlusRule()</span>,
<span class="sgr94">    Flux.ConvTranspose     </span> =&gt; <span class="sgr33">ZPlusRule()</span>,
<span class="sgr94">    Flux.CrossCor          </span> =&gt; <span class="sgr33">ZPlusRule()</span>,
<span class="sgr94">    Flux.Dense             </span> =&gt; <span class="sgr33">EpsilonRule{Float32}(1.0f-6)</span>,
<span class="sgr94">    typeof(NNlib.dropout)  </span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    Flux.AlphaDropout      </span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    Flux.Dropout           </span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    Flux.BatchNorm         </span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    typeof(Flux.flatten)   </span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    typeof(MLUtils.flatten)</span> =&gt; <span class="sgr33">PassRule()</span>,
<span class="sgr94">    typeof(identity)       </span> =&gt; <span class="sgr33">PassRule()</span>,
 ),
  FirstLayerTypeMap(  <span class="sgr90"># first layer</span>
<span class="sgr94">    Flux.Conv         </span> =&gt; <span class="sgr33">FlatRule()</span>,
<span class="sgr94">    Flux.ConvTranspose</span> =&gt; <span class="sgr33">FlatRule()</span>,
<span class="sgr94">    Flux.CrossCor     </span> =&gt; <span class="sgr33">FlatRule()</span>,
 ),
)</code></pre><pre><code class="language-julia hljs">LRP(model, composite)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Conv((3, 3), 3 =&gt; 8, relu, pad=1) <span class="sgr90"> =&gt; </span><span class="sgr33">FlatRule()</span>,
  Conv((3, 3), 8 =&gt; 8, relu, pad=1) <span class="sgr90"> =&gt; </span><span class="sgr33">ZPlusRule()</span>,
  MaxPool((2, 2))                   <span class="sgr90"> =&gt; </span><span class="sgr33">ZeroRule()</span>,
  Conv((3, 3), 8 =&gt; 16, pad=1)      <span class="sgr90"> =&gt; </span><span class="sgr33">ZPlusRule()</span>,
  BatchNorm(16, relu)               <span class="sgr90"> =&gt; </span><span class="sgr33">PassRule()</span>,
  Conv((3, 3), 16 =&gt; 8, relu, pad=1)<span class="sgr90"> =&gt; </span><span class="sgr33">ZPlusRule()</span>,
  BatchNorm(8, relu)                <span class="sgr90"> =&gt; </span><span class="sgr33">PassRule()</span>,
  Flux.flatten                      <span class="sgr90"> =&gt; </span><span class="sgr33">PassRule()</span>,
  Dense(2048 =&gt; 512, relu)          <span class="sgr90"> =&gt; </span><span class="sgr33">EpsilonRule{Float32}(1.0f-6)</span>,
  Dropout(0.5)                      <span class="sgr90"> =&gt; </span><span class="sgr33">PassRule()</span>,
  Dense(512 =&gt; 100)                 <span class="sgr90"> =&gt; </span><span class="sgr33">EpsilonRule{Float32}(1.0f-6)</span>,
)</code></pre><h2 id="docs-lrp-layerwise"><a class="docs-heading-anchor" href="#docs-lrp-layerwise">Computing layerwise relevances</a><a id="docs-lrp-layerwise-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-layerwise" title="Permalink"></a></h2><p>If you are interested in computing layerwise relevances, call <code>analyze</code> with an LRP analyzer and the keyword argument <code>layerwise_relevances=true</code>.</p><p>The layerwise relevances can be accessed in the <code>extras</code> field of the returned <code>Explanation</code>:</p><pre><code class="language-julia hljs">input = rand(Float32, 32, 32, 3, 1) # dummy input for our convolutional neural network

expl = analyze(input, analyzer; layerwise_relevances=true)
expl.extras.layerwise_relevances</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(Float32[8.327308 -7.83097 … 20.22971 -3.7166505; 8.963654 -5.9993386 … -2.928864 -3.4936154; … ; 1.2683306 3.3820424 … 24.60917 -7.21482; 15.214648 -8.884898 … 2.6456695 -10.390884;;; 4.011874 -4.0590324 … -4.616354 3.9734259; -2.9656606 6.912936 … -13.802339 5.239159; … ; -6.3620853 -2.6684556 … -26.411573 -0.39939696; -3.799637 0.24409898 … -0.08097203 1.577045;;; 2.1186552 8.112977 … 12.724606 -8.0426; 1.0737268 1.3032237 … 0.3346576 -28.026342; … ; 15.932969 -26.070225 … -5.7292414 2.5399013; -5.9140425 -0.0013468717 … -1.5723354 5.2093186;;;;], Float32[-0.0 -2.7200859 … -0.0 -0.5623542; 0.14131893 -0.0 … -0.0 8.032345; … ; -15.622283 -0.0 … 0.7954757 -2.2643433; 0.0 -0.0 … 0.0 2.634971;;; -0.0 -0.0 … -0.0 0.7987479; -0.0 -0.0 … 0.0 -2.5468924; … ; 2.3494885 -0.0 … -0.0 2.8960185; 3.7750728 1.3592004 … -1.4962444 -0.0;;; 0.0 -0.95559543 … -0.0 -0.0; -0.0 9.400769 … 0.0 -0.0; … ; -0.0 -0.78302914 … -0.0 0.0; -0.0 0.0 … 0.0 0.74188423;;; 0.0 0.0 … 0.0 0.0; -0.0 0.0 … 0.0 -0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.0;;; -0.0 0.0 … 0.0 0.0; -0.0 0.0 … -0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; 0.0 -0.0 … -0.0 -0.0; 1.6911445 0.0 … 0.0 4.1343575; … ; 2.2103014 0.0 … 0.0 -4.2544937; -0.77148944 -0.0 … -0.0 -0.0;;; -0.16688322 0.0 … 0.0 -2.8568013; -0.9833772 1.6663531 … -0.0 3.2834246; … ; 7.074368 -0.0 … -0.0 10.086847; 0.0 15.6836405 … -0.0 -0.0;;; 0.61481476 -2.24196 … -4.2152233 0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 0.0 … -0.0 -0.0;;;;], Float32[0.0 4.57914 … -6.3799706 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … -3.847128 0.0; 0.0 -2.757442 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 2.9550445 … 0.8068923 0.0; … ; 0.0 0.0 … -25.800358 0.0; -8.005194 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 12.121355; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 5.191336; 0.0 17.847282 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 5.0604405 0.0; … ; 0.0 0.0 … 8.109103 0.0; 8.328043 0.0 … 0.0 0.0;;; 0.0 4.795353 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 -3.7019901; 0.27488425 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; -3.6776745 0.0 … 0.0 1.2166417; … ; 0.0 0.0 … 2.3930213 0.0; -2.4171185 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … -0.4562028 0.0;;; 0.0 0.0 … 0.0 1.665124; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 1.1807004; 0.0 -3.801413 … 0.0 0.0;;;;], Float32[4.57914 2.0232563 … -0.0 -6.3799706; 4.5803576 5.656276 … 11.23432 -22.163029; … ; 4.842777 5.1135373 … 6.1951456 -18.28472; -2.757442 -8.9625225 … -8.651633 -3.8471284;;; 2.9550445 3.6525686 … -0.7383915 0.8068923; -0.0 -34.63811 … -6.431445 -17.909248; … ; -0.57381976 -21.939602 … -19.713642 -31.770304; -8.005194 -34.281742 … -39.40382 -25.800358;;; 0.0 5.3098335 … 0.0 12.121355; 0.16628794 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 17.847282 0.0 … 18.527046 5.191336;;; 0.0 1.1742975 … 2.7663443 5.0604405; 0.07974223 7.609131 … 4.751021 -0.0; … ; 3.7229264 11.253115 … 2.3452513 2.4331777; 8.328043 25.327337 … 13.882841 8.109103;;; 4.795353 -0.0 … 0.0 0.0; 0.0 -0.0 … 0.0 -0.0; … ; 0.0 -0.0 … -0.0 -0.0; 0.2748843 0.0 … 0.39339855 -3.7019901;;; -3.6776745 -1.5233933 … -1.7027856 1.2166418; 9.393424 10.514384 … 43.98021 12.046564; … ; 8.180141 6.774875 … 18.244902 27.00564; -2.4171185 4.886017 … 3.4954503 2.3930216;;; 0.0 1.0023761 … 0.0 0.0; 4.5314546 0.0 … 0.0 -0.0; … ; 3.8853288 0.0 … 20.68431 -0.3025496; 0.0 1.333253 … 0.4195981 -0.45620283;;; 0.0 0.8607781 … 0.0 1.6651243; -0.99727976 0.4735456 … 2.546403 17.9714; … ; -1.9010018 1.2183529 … 1.2070284 8.317717; -3.801413 -0.32261914 … 0.060320973 1.1807005;;;;], Float32[-0.0006117489 0.00021519868 … -0.000906484 1.0315989f-5; -0.00051562104 -0.00095367385 … -0.0009620695 -0.014612194; … ; 0.001337966 -0.00033287704 … -7.070966f-5 -0.00071956916; -0.00096954807 0.0012422366 … -0.0007115063 0.0007381511;;; -0.0051851366 -0.007856156 … 0.008978085 -0.0072933338; -0.00012250224 -0.006060506 … 0.026576811 -0.002645589; … ; -0.0038863965 0.0032866872 … -0.001181394 -0.005540036; 0.006715212 -0.004037242 … -0.007653008 -0.0010693887;;; -0.0011274469 -0.0014640973 … -0.0018788398 -0.0013461235; -0.00065485854 -0.001478855 … 0.0015346803 -0.0017216321; … ; 0.005715894 -0.0019151388 … -0.0015882591 -0.0018423795; -0.0011445136 -0.0011259448 … -0.0002880264 -0.0013962964;;; … ;;; -14.022386 -51.09481 … -17.470604 -5.585806; -4.1865044 -21.117615 … 6.8632774 25.058342; … ; 11.725198 15.252797 … -10.581841 -30.092524; 13.584241 10.497882 … 51.16966 59.64862;;; -0.13575593 0.115785345 … -35.193317 -0.15976667; -0.0034891113 -0.08571102 … -0.41366 -0.18013765; … ; -0.14947948 1.3195324 … 0.103383444 -0.8149192; -0.7908136 0.079687156 … -0.44392246 0.10112273;;; 0.0019876533 -0.0031273647 … -0.06349881 -0.00040097107; 0.0018098768 0.027225392 … 0.0018558621 -0.12330594; … ; -0.21457447 1.425 … -0.04649991 -0.10824808; 0.0015909689 -0.078088574 … -0.10634443 -0.088618174;;;;], Float32[0.0 -0.0 … 0.0 0.00014444557; 0.0 0.0 … 0.0 -0.0396553; … ; -0.0 0.0 … -0.0 -0.0; -0.0 0.0 … 0.0 -0.0;;; -0.0 -0.0 … 0.0 -0.0; -0.0 0.0 … 0.0 -0.0; … ; 0.0 0.0 … 0.0 -0.0; -0.023557935 -0.0 … 0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … 0.0 0.0; … ; 0.0 0.0 … -0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; … ;;; 0.0 -0.0 … -0.0 0.0; -0.0 0.0 … 0.004188955 0.08219464; … ; 0.115124375 0.015430466 … 0.0 0.0; -0.007304755 -0.07837329 … 0.17979364 0.093243584;;; -0.0 -0.015859297 … -0.050240967 -0.03634484; -0.0 0.016877607 … 0.03675985 -0.09808747; … ; 0.0 0.2665269 … -0.11570973 -0.1896007; -0.0 -0.041975457 … -0.0 -0.029947914;;; 0.0072853933 0.0 … 0.0 -0.0; 0.009273345 0.045343313 … -0.008227661 -0.0377172; … ; -0.115657285 0.08448729 … -0.0 -0.052653473; 0.02609525 -0.027460065 … -0.051418282 0.020150276;;;;], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.0037998108; … ; 0.00969996 -0.0068425178 … 0.0 0.0; -0.0029727046 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 -0.0003948384; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.001353812; 0.0 -0.009965459 … 0.0 0.0;;; 0.0 0.011332786 … 0.0 0.0; 0.0 0.0 … 0.0 0.01981112; … ; 0.02070136 0.10034685 … 0.16956423 0.08017855; 0.0 0.0969891 … 0.08950577 0.033947203;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0007155704 0.0; … ; 0.14044715 0.22864783 … 0.0 0.0256909; -0.35980445 0.076369286 … 0.08120247 0.03137468;;; 0.0 0.0 … 0.0 0.0; 0.0 0.002646107 … 0.021507844 0.0; … ; 0.04508819 0.097512625 … 0.026110465 0.0; 0.045098875 -0.096956596 … 0.09794691 0.022801066;;; -0.017484874 -0.021590482 … 0.0033609718 -0.10531239; -0.0003285319 0.040350176 … 0.048364475 -0.0009820143; … ; -0.0 -0.0 … 0.01886814 0.013649188; 0.07851025 -0.0 … 0.03781729 -0.00042505015;;; -0.0 -0.0 … -0.0 -0.0; -0.0192193 0.012569417 … -0.003137269 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.00012417234 … -0.029398102 -0.004092352;;; 0.025973804 -0.0 … -0.018073926 -0.0010227859; 0.018205665 -0.011601795 … 0.010633885 -0.0; … ; 0.018103514 -0.0 … -0.0 -0.0; -0.026933072 -0.0 … 0.031375717 -0.022076707;;;;], Float32[0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0039723334; … ; 0.008767475 -0.008078173 … -0.0 -0.0; -0.0028092365 -0.0 … -0.0 0.0;;; -0.0 -0.0 … -0.0 1.33902495f-5; -0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0008733339; -0.0 -0.008959949 … 0.0 0.0;;; 0.0 -0.0026320005 … 0.0 -0.0; -0.0 0.0 … 0.0 -0.021781597; … ; -0.0076322416 0.008802434 … 0.031181267 -0.018381499; 0.0 -0.0060957703 … 0.0017477524 -0.004788824;;; -0.0 0.0 … 0.0 -0.0; 0.0 -0.0 … 6.4086846f-5 0.0; … ; 0.007395381 0.022416104 … 0.0 0.0008290763; -0.0066261915 0.0067268224 … 0.004018948 -0.0033523412;;; -0.0 -0.0 … 0.0 0.0; 0.0 -0.00031485877 … -0.012030382 0.0; … ; 0.0038220088 0.026231606 … -1.8129813f-5 0.0; 0.0041472283 -0.035086147 … 0.031724554 0.010119597;;; -0.0014256705 -0.012816689 … 0.0029711064 -0.0044658766; -0.0025405234 0.015641585 … 0.029247666 0.0032603152; … ; 0.0 -0.0 … 0.004178582 0.011036619; 0.0036167072 0.0 … 0.024250142 -0.00523355;;; -0.0 0.0 … 0.0 -0.0; -0.0045481087 0.011536867 … 0.0020235414 0.0; … ; 0.0 0.0 … -0.0 0.0; 0.0 -0.0009167506 … -0.016523432 -0.0012255838;;; 0.0030295074 -0.0 … -0.0015426872 -0.0055334033; 0.010720322 -0.0016994367 … 0.0030408464 0.0; … ; 0.013795362 -0.0 … -0.0 -0.0; -0.012482019 -0.0 … 0.030494832 -0.007282867;;;;], Float32[0.0; 0.0; … ; -0.0; -0.007282867;;], Float32[-0.0; 0.0; … ; 0.0; -0.0;;], Float32[-0.0; 0.0; … ; 0.0; -0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;])</code></pre><p>Note that the layerwise relevances are only kept for layers in the outermost <code>Chain</code> of the model. When using our unflattened model, we only obtain three layerwise relevances, one for each chain in the model and the output relevance:</p><pre><code class="language-julia hljs">analyzer = LRP(model; flatten=false) # use unflattened model

expl = analyze(input, analyzer; layerwise_relevances=true)
expl.extras.layerwise_relevances</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(Float32[0.1960582 -0.15411384 … -11.57062 -2.011442; 0.01880353 -0.003916721 … 3.40788 4.557889; … ; 1.5249896 0.09706643 … 0.035254557 -0.12084636; -1.1756896 -0.42424256 … 0.020654567 -0.4876383;;; -0.2775201 -0.0955955 … 1.8459821 8.416919; -0.00021444712 -0.026219355 … -10.091517 -0.5518057; … ; 1.3808483 -0.10030223 … -0.20419 -0.014242577; 0.14297286 0.00085867266 … 0.098073766 -0.029015189;;; -0.3122843 0.35415983 … 9.78052 -18.570946; -0.09579516 0.25360116 … 0.42155766 -2.7034986; … ; -1.5503724 -0.8221216 … -0.45758134 0.23653653; 0.24396542 -0.0048256963 … -0.48013076 0.16992503;;;;], Float32[0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0031959377; … ; 0.010020404 -0.005846454 … -0.0 0.0; -0.0038638408 -0.0 … -0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0004997425; -0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0010762725; -0.0 -0.01216915 … 0.0 0.0;;; 0.0 -0.0024901559 … 0.0 -0.0; -0.0 0.0 … -0.0 -0.023070497; … ; -0.0075005265 0.009888828 … 0.026142605 -0.011985789; 0.0 -0.014241133 … 0.0041673067 -0.0026029942;;; -0.0 -0.0 … 0.0 -0.0; 0.0 -0.0 … 0.0 0.0; … ; 0.0089091705 0.023063825 … 0.0 0.00068108604; -0.0063524125 0.01003483 … 0.00521952 -0.0027068933;;; -0.0 -0.0 … 0.0 0.0; 0.0 -0.0 … -0.010595257 0.0; … ; 0.002217615 0.023121541 … 0.008180201 0.0; 0.0042005447 -0.038086157 … 0.040440984 0.0114883175;;; -0.0028496794 -0.007859429 … 0.00017290808 -0.0062207673; -0.0043729884 0.007264631 … 0.027005874 0.0029822104; … ; 0.0 -0.0 … 0.0012533697 0.016646156; 0.004351355 0.0 … 0.030371943 -0.0054110815;;; -0.0 0.0 … 0.0 -0.0; -0.0040852036 0.009619381 … 0.00022223328 0.0; … ; 0.0 0.0 … -0.0 0.0; 0.0 -0.0 … -0.018390564 -0.0058159726;;; 0.00344227 -0.0 … -0.0018052001 -0.003402148; 0.015944706 -0.0021163563 … 0.0022007816 0.0; … ; 0.007882758 -0.0 … -0.0 -0.0; -0.020930655 -0.0 … 0.031153943 -0.006764625;;;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;])</code></pre><h2 id="docs-lrp-performance"><a class="docs-heading-anchor" href="#docs-lrp-performance">Performance tips</a><a id="docs-lrp-performance-1"></a><a class="docs-heading-anchor-permalink" href="#docs-lrp-performance" title="Permalink"></a></h2><h3 id="gpu-docs"><a class="docs-heading-anchor" href="#gpu-docs">Using LRP with a GPU</a><a id="gpu-docs-1"></a><a class="docs-heading-anchor-permalink" href="#gpu-docs" title="Permalink"></a></h3><p>All LRP analyzers support GPU backends, building on top of <a href="https://fluxml.ai/Flux.jl/stable/gpu/">Flux.jl's GPU support</a>. Using a GPU only requires moving the input array and model weights to the GPU.</p><p>For example, using <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>:</p><pre><code class="language-julia hljs">using CUDA, cuDNN
using Flux
using ExplainableAI

# move input array and model weights to GPU
input = input |&gt; gpu # or gpu(input)
model = model |&gt; gpu # or gpu(model)

# analyzers don't require calling `gpu`
analyzer = LRP(model)

# explanations are computed on the GPU
expl = analyze(input, analyzer)</code></pre><p>Some operations, like saving, require moving explanations back to the CPU. This can be done using Flux's <code>cpu</code> function:</p><pre><code class="language-julia hljs">val = expl.val |&gt; cpu # or cpu(expl.val)

using BSON
BSON.@save "explanation.bson" val</code></pre><h3 id="Using-LRP-without-a-GPU"><a class="docs-heading-anchor" href="#Using-LRP-without-a-GPU">Using LRP without a GPU</a><a id="Using-LRP-without-a-GPU-1"></a><a class="docs-heading-anchor-permalink" href="#Using-LRP-without-a-GPU" title="Permalink"></a></h3><p>Using Julia's package extension mechanism, RelevancePropagation.jl's LRP implementation can optionally make use of <a href="https://github.com/mcabbott/Tullio.jl">Tullio.jl</a> and <a href="https://github.com/JuliaSIMD/LoopVectorization.jl">LoopVectorization.jl</a> for faster LRP rules on dense layers.</p><p>This only requires loading the packages before loading RelevancePropagation.jl:</p><pre><code class="language-julia hljs">using LoopVectorization, Tullio
using RelevancePropagation</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>G. Montavon et al., <a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10">Layer-Wise Relevance Propagation: An Overview</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../composites/">Assigning rules to layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Monday 29 January 2024 23:20">Monday 29 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic usage of LRP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start out by loading a small convolutional neural network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using RelevancePropagation\n",
    "using Flux\n",
    "\n",
    "model = Chain(\n",
    "    Chain(\n",
    "        Conv((3, 3), 3 => 8, relu; pad=1),\n",
    "        Conv((3, 3), 8 => 8, relu; pad=1),\n",
    "        MaxPool((2, 2)),\n",
    "        Conv((3, 3), 8 => 16; pad=1),\n",
    "        BatchNorm(16, relu),\n",
    "        Conv((3, 3), 16 => 8, relu; pad=1),\n",
    "        BatchNorm(8, relu),\n",
    "    ),\n",
    "    Chain(Flux.flatten, Dense(2048 => 512, relu), Dropout(0.5), Dense(512 => 100, softmax)),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model contains two chains: the convolutional layers and the fully connected layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation\n",
    "### Stripping the output softmax\n",
    "When using LRP, it is recommended to explain output logits instead of probabilities.\n",
    "This can be done by stripping the output softmax activation from the model\n",
    "using the `strip_softmax` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Chain(\n    Conv((3, 3), 3 => 8, relu, pad=1),  \u001b[90m# 224 parameters\u001b[39m\n    Conv((3, 3), 8 => 8, relu, pad=1),  \u001b[90m# 584 parameters\u001b[39m\n    MaxPool((2, 2)),\n    Conv((3, 3), 8 => 16, pad=1),       \u001b[90m# 1_168 parameters\u001b[39m\n    BatchNorm(16, relu),                \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n    Conv((3, 3), 16 => 8, relu, pad=1),  \u001b[90m# 1_160 parameters\u001b[39m\n    BatchNorm(8, relu),                 \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  ),\n  Chain(\n    Flux.flatten,\n    Dense(2048 => 512, relu),           \u001b[90m# 1_049_088 parameters\u001b[39m\n    Dropout(0.5),\n    Dense(512 => 100),                  \u001b[90m# 51_300 parameters\u001b[39m\n  ),\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.213 MiB."
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "model = strip_softmax(model)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't remove the output softmax,\n",
    "model checks will fail."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Canonizing the model\n",
    "LRP is not invariant to a model's implementation.\n",
    "Applying the `GammaRule` to two linear layers in a row will yield different results\n",
    "than first fusing the two layers into one linear layer and then applying the rule.\n",
    "This fusing is called \"canonization\" and can be done using the `canonize` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),   \u001b[90m# 1_168 parameters\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_103_540 parameters,\n\u001b[90m          # plus 2 non-trainable, 16 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "model_canonized = canonize(model)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "After canonization, the first `BatchNorm` layer has been fused into the preceding `Conv` layer.\n",
    "The second `BatchNorm` layer wasn't fused\n",
    "since its preceding `Conv` layer has a ReLU activation function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flattening the model\n",
    "RelevancePropagation.jl's LRP implementation supports nested Flux Chains and Parallel layers.\n",
    "However, it is recommended to flatten the model before analyzing it.\n",
    "\n",
    "LRP is implemented by first running a forward pass through the model,\n",
    "keeping track of the intermediate activations, followed by a backward pass\n",
    "that computes the relevances.\n",
    "\n",
    "To keep the LRP implementation simple and maintainable,\n",
    "RelevancePropagation.jl does not pre-compute \"nested\" activations.\n",
    "Instead, for every internal chain, a new forward pass is run to compute activations.\n",
    "\n",
    "By \"flattening\" a model, this overhead can be avoided.\n",
    "For this purpose, RelevancePropagation.jl provides the function `flatten_model`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),         \u001b[90m# 1_168 parameters\u001b[39m\n  BatchNorm(16, relu),                  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "model_flat = flatten_model(model)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is called by default when creating an LRP analyzer.\n",
    "Note that we pass the unflattened model to the analyzer, but `analyzer.model` is flattened:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),         \u001b[90m# 1_168 parameters\u001b[39m\n  BatchNorm(16, relu),                  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model)\n",
    "analyzer.model"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "If this flattening is not desired, it can be disabled\n",
    "by passing the keyword argument `flatten=false` to the `LRP` constructor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LRP rules\n",
    "By default, the `LRP` constructor will assign the `ZeroRule` to all layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, pad=1)      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  BatchNorm(16, relu)               \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This analyzer will return heatmaps that look identical to the `InputTimesGradient` analyzer\n",
    "from [ExplainableAI.jl](https://github.com/Julia-XAI/ExplainableAI.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LRP's strength lies in assigning different rules to different layers,\n",
    "based on their functionality in the neural network[^1].\n",
    "RelevancePropagation.jl implements many LRP rules out of the box,\n",
    "but it is also possible to *implement custom rules*.\n",
    "\n",
    "To assign different rules to different layers,\n",
    "use one of the composites presets,\n",
    "or create your own composite, as described in\n",
    "*Assigning rules to layers*."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Composite(\n  GlobalTypeMap(  \u001b[90m# all layers\u001b[39m\n\u001b[94m    Flux.Conv              \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose     \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor          \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.Dense             \u001b[39m => \u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n\u001b[94m    typeof(NNlib.dropout)  \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.AlphaDropout      \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.Dropout           \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.BatchNorm         \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(Flux.flatten)   \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(MLUtils.flatten)\u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(identity)       \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n ),\n  FirstLayerTypeMap(  \u001b[90m# first layer\u001b[39m\n\u001b[94m    Flux.Conv         \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose\u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor     \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n ),\n)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mFlatRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, pad=1)      \u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  BatchNorm(16, relu)               \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model, composite)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing layerwise relevances\n",
    "If you are interested in computing layerwise relevances,\n",
    "call `analyze` with an LRP analyzer and the keyword argument\n",
    "`layerwise_relevances=true`.\n",
    "\n",
    "The layerwise relevances can be accessed in the `extras` field\n",
    "of the returned `Explanation`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(Float32[1.6136707 0.06725449 … -0.290393 -0.3288121; 0.4091628 0.13591135 … 0.070138656 0.040290557; … ; 2.894336 9.398814 … -0.44242433 -2.7079551; -9.176036 -0.21442468 … 0.64689165 0.37515458;;; -0.4027044 0.058032963 … 0.0078234505 0.23882432; -0.77446204 -0.0064372728 … 0.082191005 0.26113787; … ; 8.016382 -0.5027747 … -3.2860088 0.86670864; 2.1908891 0.059754673 … -0.7444249 -0.001509129;;; 0.84555066 -0.091513395 … 0.0111797 -0.099057086; 2.7035801 -0.4171376 … 0.070346564 -0.089421846; … ; -2.3638806 -1.8503306 … 11.206497 1.0505357; -6.864281 -0.12686348 … 0.017017564 -0.40753108;;;;], Float32[0.0 0.0 … 0.0 0.0; 0.0 -0.0 … 0.0 -0.0; … ; -0.0 -0.0 … 0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; 1.1172233 -0.0944648 … -0.25064185 -0.22235802; 1.1384376 -0.23317415 … 0.6964104 0.0051701507; … ; 4.8782315 2.1144059 … -0.39594322 -0.7279849; -9.705594 0.5884744 … -0.0031066842 -0.017877376;;; 0.24447797 1.0724282 … -0.47592637 -0.1716174; -0.0 1.0027115 … -0.0074586766 -0.18110761; … ; -0.0 -11.213593 … 0.45243078 2.7016299; -0.0 -5.749 … -0.039787866 0.082695104;;; 0.34302947 -0.21892357 … 0.0 -0.003252951; 0.22606093 0.026074756 … 0.032916952 -0.118283585; … ; 1.1341585 1.7028989 … 8.199736 0.6763657; -4.8399835 3.0266428 … 0.020811768 -0.0070179543;;; 0.0 -0.0020297428 … -0.047464717 -0.041014273; -0.0 0.0 … 0.0 -0.009732077; … ; -0.02982143 2.8977501 … 0.0 2.5687795; -0.14255421 0.3281742 … -0.04274701 0.10628022;;; -0.0 0.0 … 0.0 -0.0; 0.0 0.00011171698 … 0.20415437 0.0; … ; -0.0 -0.0 … -0.072739534 -0.0; 0.0 -0.0 … 0.0 0.0;;; -0.047263157 -0.5434332 … 0.10984155 0.08623553; 0.18222465 -0.48257464 … -0.12484302 -0.003357561; … ; 0.36149573 -6.3592477 … -2.075023 3.330716; 1.4295444 3.1540165 … 0.023850003 0.0;;; -0.027023148 0.0 … 0.0 0.0; 0.1517865 0.0 … 0.0 0.019844268; … ; 0.9103149 0.0 … 0.0 2.4530184; 2.3202891 0.0 … -0.0 -0.0;;;;], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.24584024 … 0.21728002 0.0; … ; 0.0 0.0 … 0.0 0.5261155; 0.0 -18.111118 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 1.9130104 … 0.0 -0.53220654; … ; 0.0 21.267744 … 0.0 -0.4425569; 0.0 0.0 … 0.0 0.0;;; 0.0 -0.5249903 … 0.0 0.0; 0.0 0.0 … 0.0 -0.41687605; … ; 6.229497 0.0 … -0.009631611 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 1.1640505 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -7.999894 0.0 … 0.0 -0.014329973;;; 0.0 0.0 … -0.12608834 0.0; 0.0 -0.20647076 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.9922388 … 0.1848966 0.0; … ; 4.8741364 0.0 … 0.37386525 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 -2.1519587 … -0.4244856 0.0; … ; 0.0 -26.89321 … 0.1536864 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 1.0884956 … 0.04894513 0.0; … ; 0.0 0.0 … 0.32793295 0.0; 0.0 12.94462 … 0.0 0.0;;;;], Float32[0.24584024 0.9645786 … 0.06403621 0.21728002; 1.2238077 0.51416606 … -2.3802407 0.1830005; … ; -7.912799 8.562818 … 8.219878 -11.651862; -18.111118 -33.457245 … 0.117495336 0.5261155;;; 1.9130104 -0.04219117 … -0.4842101 -0.53220654; 0.73347855 -5.4485316 … -2.435767 -0.59487706; … ; -1.0401436 19.6053 … 24.933935 23.068686; 21.267744 11.993106 … -1.5405444 -0.4425569;;; -0.5249903 0.0 … -0.10885744 -0.41687605; -0.029075308 0.13711084 … -0.0 -0.06459258; … ; -0.0 -0.0 … -1.3006002 -1.2965341; 6.229497 1.4442341 … -0.05394165 -0.009631611;;; 1.1640505 0.0 … 0.0 -0.0; 0.019869281 -0.0 … -0.0 0.0; … ; 0.0 -0.0 … -0.0 0.0; -7.999894 3.8964086 … 0.0 -0.014329974;;; -0.20647076 0.8931951 … -0.32871014 -0.12608834; -0.3676547 0.061247468 … -0.10936261 -0.09282585; … ; 0.0 0.35268798 … -1.7107931 9.061116; 0.0 -0.0 … -0.0 -0.0;;; 0.9922388 0.5814955 … 0.4503481 0.1848966; 1.6448723 0.109377205 … -3.5170777 0.48369822; … ; -2.973314 -6.840854 … -4.5037575 4.93581; 4.8741364 10.209949 … 0.2243592 0.37386525;;; -2.1519587 -3.7174969 … -1.5499474 -0.4244856; -0.07606434 0.42818582 … 1.4785002 -0.17305617; … ; 14.724196 3.9099553 … -27.491173 11.014162; -26.89321 -2.2105715 … 0.20849621 0.1536864;;; 1.0884956 0.12892644 … 0.0959124 0.04894513; 1.3104382 -4.100988 … -2.5969832 -0.08572449; … ; 10.423474 15.673039 … 17.26095 -5.6536746; 12.94462 -12.151159 … -0.090677805 0.32793295;;;;], Float32[0.040480427 0.027549943 … 0.13810694 0.098040245; 0.12910408 0.030919375 … 0.17472781 0.061951727; … ; 0.15933582 0.33394897 … 0.6753338 0.010271333; 0.50631964 0.09617973 … 0.838168 -0.025726678;;; 0.020289795 -0.069706365 … 0.011968586 0.0046183397; 0.50061184 1.0156578 … 0.112704255 0.008105573; … ; -0.23560864 0.039039478 … 0.074085996 0.028404957; 0.23054636 -0.0019944953 … 0.0071994797 -0.11835049;;; 0.0 -0.0 … 0.0 0.0; -0.0 0.0 … -0.0 0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … 0.0 -0.0;;; … ;;; -0.000894249 -0.005599391 … -0.007742587 -0.0001540484; 0.014654102 -0.00045386562 … 6.181978f-5 3.7347672f-5; … ; 0.036517378 -0.042770706 … -0.025150176 -5.1013427f-5; -0.088357724 0.047004618 … -0.031247761 -0.05432478;;; -0.0006731195 0.00014905041 … -0.00065073627 0.006513896; -0.010408376 -0.00059852033 … -0.0006325917 -0.00060389156; … ; -2.6684733f-5 -0.00062770443 … 5.9253136f-5 0.0016661946; -0.009382268 -0.012888109 … 0.040830284 -0.029397817;;; 0.0 0.0 … -0.0 -0.0; 0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … 0.0 -0.0;;;;], Float32[-0.005214326 0.02732714 … -0.028501438 -0.0040238425; -0.12590711 0.03242992 … -0.0027986725 -0.0015332432; … ; -0.055493504 -0.016896818 … 0.12559345 -0.0014993127; 0.056651633 0.031856615 … 0.20622402 -0.0;;; 0.034853052 0.0 … 0.0012328062 -0.000120191464; 0.14380096 0.16468848 … 0.007503615 -0.0016029261; … ; -0.17987347 0.002173863 … -0.013010375 0.013470337; 0.06681255 0.0 … -0.004939442 0.0;;; -0.0 0.0 … -0.0 -0.0; 0.0 -0.0 … 0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0; -0.0 0.0 … -0.0 0.0;;; … ;;; -0.0 -0.0 … 0.0 0.0; 0.01701212 -0.008160269 … -0.0 0.0; … ; 0.013323818 -0.020876532 … -0.014983039 5.499619f-5; -0.056168355 0.07302151 … -0.0049527236 -0.0376123;;; -0.0 -0.0 … 0.0 0.0; -0.029101836 -0.0 … 0.0 0.0; … ; 0.0 -0.0 … 0.00042514032 -0.0; -0.019549284 -0.024871945 … 0.057102367 -0.04850852;;; -0.0 -0.0 … 0.0 0.0; -0.0 0.0 … -0.0 0.0; … ; 0.0 -0.0 … -0.0 0.0; -0.0 0.0 … -0.0 0.0;;;;], Float32[-0.0 -0.0 … -0.0 0.0024558767; -0.0 -0.0 … -0.0 -0.0036142655; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; 0.0 9.8785895f-6 … 0.002999678 -0.0044683726; 0.0 0.0 … 1.0175216f-5 0.00082771893; … ; 0.0 0.0 … 0.0 0.00097774; 0.0 0.0 … 0.0 0.0;;; 0.0 0.023907045 … 0.0007668661 0.0; -0.009908384 0.0 … 0.0 0.0; … ; -0.007814235 0.0 … -0.077316694 0.0; -0.03082273 0.05377108 … 0.0 0.0007125245;;; 0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0; … ; -0.0 -0.0 … 0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0006333235 0.011149741 … 0.00027129924 -0.0039702593;;; -0.2981589 1.1019368 … -0.017417107 -4.8907805f-5; 0.06992908 -0.0039626644 … -0.013601949 -0.0040797833; … ; -0.028148763 -0.009348884 … -0.013383135 -0.0046747406; -0.0040712096 0.48203966 … -0.016048625 -0.004104602;;; 0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … -0.0 -0.0;;; -0.029955324 -0.0 … -0.0020613603 -0.0; 0.012932002 -0.0 … -0.0006272056 -0.0; … ; 0.029776534 -0.0 … -0.0 -0.0; 0.01666988 -0.0 … -0.0 -0.0;;;;], Float32[0.0 0.0 … -0.0 0.0033878237; -0.0 0.0 … -0.0 -0.0017507664; … ; -0.0 0.0 … 0.0 -0.0; 0.0 -0.0 … -0.0 0.0;;; 0.0 -0.0005081081 … 0.0015194503 -0.005235227; -0.0 -0.0 … -0.0006656279 0.00045580923; … ; -0.0 -0.0 … -0.0 0.00044165528; -0.0 -0.0 … 0.0 0.0;;; -0.0 0.012360122 … -0.0031600622 0.0; -0.009896039 0.0 … 0.0 0.0; … ; -0.0042624148 0.0 … -0.0040631467 0.0; -0.015625414 0.034690414 … -0.0 -0.0016216654;;; 0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0; … ; -0.0 -0.0 … 0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0; 0.0 0.0 … -0.0 0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0015943887 0.014128036 … 0.010193878 -0.0023218128;;; -0.057203807 0.022623006 … -0.008801784 -3.7659986f-5; 0.027776442 0.016079076 … -0.009591229 -0.0010260574; … ; -0.016302958 0.008098533 … -0.019386966 0.0026145468; 5.7452802f-5 0.048797917 … -0.0009164907 0.0023409966;;; 0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … -0.0 -0.0;;; -0.0138909025 0.0 … -0.00455376 -0.0; 0.024728332 -0.0 … 0.00074598065 0.0; … ; 0.031376004 0.0 … -0.0 0.0; 0.015285575 -0.0 … -0.0 0.0;;;;], Float32[0.0; -0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.028668875; … ; -0.0; -0.0;;], Float32[0.0; 0.028668875; … ; -0.0; -0.0;;], Float32[0.0; 0.0; … ; 1.0; 0.0;;])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "input = rand(Float32, 32, 32, 3, 1) # dummy input for our convolutional neural network\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the layerwise relevances are only kept for layers in the outermost `Chain` of the model.\n",
    "When using our unflattened model, we only obtain three layerwise relevances,\n",
    "one for each chain in the model and the output relevance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(Float32[-1.6834018 0.08243677 … 4.7381997 -1.7831188; -0.21857585 -0.054024678 … 1.7439295 6.2386847; … ; -0.09945045 5.7841253 … -0.19085495 -0.4172961; -3.6906374 0.39359462 … -0.34047225 0.356174;;; -0.055439077 0.27467686 … -0.18393752 -0.087915406; -0.46827966 0.08803082 … -8.742067 -1.5298343; … ; 3.9922173 0.15836154 … -1.2489359 0.32894918; 1.5363016 0.027065868 … -0.015153881 -0.00028751526;;; 0.18724476 -0.14218418 … -0.19191198 1.9243609; -1.1377876 -0.07128942 … 6.077425 5.197641; … ; -1.1893035 -1.0039828 … 2.929041 0.37148607; -2.7889526 -0.020103091 … 0.15215376 -0.096267454;;;;], Float32[0.0 0.0 … -0.0 0.0034312233; -0.0 0.0 … -0.0 -0.0028386621; … ; -0.0 0.003770389 … 0.0 -0.0; 0.0 -0.0 … -0.0 0.0;;; 0.0 -0.0 … 0.00018016246 0.0041855825; -0.0 -0.0 … -0.002430645 -0.0013358076; … ; -0.0 -0.0 … -0.0 0.002355911; -0.0 -0.0 … 0.0 0.0;;; -0.0 0.01805787 … -0.003147013 0.0; -0.0053236424 0.0 … 0.0 0.0; … ; -0.0033398806 -0.0 … -0.0014961131 0.0; -0.013350539 0.044591643 … -0.0 -0.0005713787;;; 0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0; … ; -0.0 -0.0 … 0.0 0.0; -0.0 -0.0 … 0.0 0.0;;; -0.0 0.0 … -0.0 -0.0; 0.0 0.0 … -0.0004612394 0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.002359846 0.013184605 … 0.004662017 -0.003124969;;; -0.038857512 0.026460469 … -0.007030702 -0.0; 0.041505333 0.022728361 … -0.0053220163 -0.0; … ; -0.011901293 0.0027348457 … -0.008417894 0.0; -0.0011356908 0.043290336 … -0.00028132577 0.0;;; 0.0 0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 0.0 … 0.0 -0.0; 0.0 -0.0 … -0.0 -0.0;;; -0.016823731 0.0 … -0.0040600966 -0.0; 0.03799295 -0.0 … 0.000560196 0.0; … ; 0.039203722 -0.0 … -0.0 0.0; 0.02037919 -0.0 … 0.0 -0.0;;;;], Float32[0.0; 0.0; … ; 1.0; 0.0;;])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model; flatten=false) # use unflattened model\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance tips\n",
    "### Using LRP with a GPU\n",
    "All LRP analyzers support GPU backends,\n",
    "building on top of [Flux.jl's GPU support](https://fluxml.ai/Flux.jl/stable/gpu/).\n",
    "Using a GPU only requires moving the input array and model weights to the GPU.\n",
    "\n",
    "For example, using [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```julia\n",
    "using CUDA, cuDNN\n",
    "using Flux\n",
    "using ExplainableAI\n",
    "\n",
    "# move input array and model weights to GPU\n",
    "input = input |> gpu # or gpu(input)\n",
    "model = model |> gpu # or gpu(model)\n",
    "\n",
    "# analyzers don't require calling `gpu`\n",
    "analyzer = LRP(model)\n",
    "\n",
    "# explanations are computed on the GPU\n",
    "expl = analyze(input, analyzer)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some operations, like saving, require moving explanations back to the CPU.\n",
    "This can be done using Flux's `cpu` function:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```julia\n",
    "val = expl.val |> cpu # or cpu(expl.val)\n",
    "\n",
    "using BSON\n",
    "BSON.@save \"explanation.bson\" val\n",
    "```\n",
    "\n",
    "### Using LRP without a GPU\n",
    "Using Julia's package extension mechanism,\n",
    "RelevancePropagation.jl's LRP implementation can optionally make use of\n",
    "[Tullio.jl](https://github.com/mcabbott/Tullio.jl) and\n",
    "[LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "for faster LRP rules on dense layers.\n",
    "\n",
    "This only requires loading the packages before loading RelevancePropagation.jl:\n",
    "```julia\n",
    "using LoopVectorization, Tullio\n",
    "using RelevancePropagation\n",
    "```\n",
    "\n",
    "[^1]: G. Montavon et al., [Layer-Wise Relevance Propagation: An Overview](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}

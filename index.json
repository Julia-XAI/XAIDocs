[{"id":3,"pagetitle":"Home","title":"ExplainableAI.jl","ref":"/XAIDocs/ExplainableAI/stable/#ExplainableAI.jl","content":" ExplainableAI.jl Explainable AI methods in Julia. Note This package is part of a wider  Julia XAI ecosystem . For an introduction to this ecosystem, please refer to the   Getting started guide ."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/ExplainableAI/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run  julia> ]add ExplainableAI"},{"id":5,"pagetitle":"Home","title":"Manual","ref":"/XAIDocs/ExplainableAI/stable/#Manual","content":" Manual Getting started Preparing the input data Explanations Heatmapping basics Neuron selection Analyzing batches Analyzer augmentations Noise augmentation Integration augmentation"},{"id":6,"pagetitle":"Home","title":"API reference","ref":"/XAIDocs/ExplainableAI/stable/#API-reference","content":" API reference Basic API Analyzers Input augmentations Index"},{"id":9,"pagetitle":"API Reference","title":"Basic API","ref":"/XAIDocs/ExplainableAI/stable/api/#Basic-API","content":" Basic API All methods in ExplainableAI.jl work by calling  analyze  on an input and an analyzer:"},{"id":10,"pagetitle":"API Reference","title":"XAIBase.analyze","ref":"/XAIDocs/ExplainableAI/stable/api/#XAIBase.analyze","content":" XAIBase.analyze  —  Function analyze(input, method)\nanalyze(input, method, output_selection) Apply the analyzer  method  for the given input, returning an  Explanation . If  output_selection  is specified, the explanation will be calculated for that output. Otherwise, the output with the highest activation is automatically chosen. See also  Explanation . source"},{"id":11,"pagetitle":"API Reference","title":"XAIBase.Explanation","ref":"/XAIDocs/ExplainableAI/stable/api/#XAIBase.Explanation","content":" XAIBase.Explanation  —  Type Explanation(val, output, output_selection, analyzer, heatmap, extras) Return type of analyzers when calling  analyze . Fields val : numerical output of the analyzer, e.g. an attribution or gradient output : model output for the given analyzer input output_selection : index of the output used for the explanation analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP heatmap : symbol indicating a preset heatmapping style,   e.g.  :attribution ,  :sensitivity  or  :cam extras : optional named tuple that can be used by analyzers   to return additional information. source For heatmapping functionality, take a look at either  VisionHeatmaps.jl  or  TextHeatmaps.jl . Both provide  heatmap  methods for visualizing explanations,  either for images or text, respectively."},{"id":12,"pagetitle":"API Reference","title":"Analyzers","ref":"/XAIDocs/ExplainableAI/stable/api/#Analyzers","content":" Analyzers"},{"id":13,"pagetitle":"API Reference","title":"ExplainableAI.Gradient","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.Gradient","content":" ExplainableAI.Gradient  —  Type Gradient(model) Analyze model by calculating the gradient of a neuron activation with respect to the input. source"},{"id":14,"pagetitle":"API Reference","title":"ExplainableAI.InputTimesGradient","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.InputTimesGradient","content":" ExplainableAI.InputTimesGradient  —  Type InputTimesGradient(model) Analyze model by calculating the gradient of a neuron activation with respect to the input. This gradient is then multiplied element-wise with the input. source"},{"id":15,"pagetitle":"API Reference","title":"ExplainableAI.SmoothGrad","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.SmoothGrad","content":" ExplainableAI.SmoothGrad  —  Function SmoothGrad(analyzer, [n=50, std=1.0f0, rng=GLOBAL_RNG])\nSmoothGrad(analyzer, [n=50, distribution=Normal(0.0f0, 1.0f0), rng=GLOBAL_RNG]) Analyze model by calculating a smoothed sensitivity map. This is done by averaging sensitivity maps of a  Gradient  analyzer over random samples in a neighborhood of the input, typically by adding Gaussian noise with mean 0. References Smilkov et al.,  SmoothGrad: removing noise by adding noise source"},{"id":16,"pagetitle":"API Reference","title":"ExplainableAI.IntegratedGradients","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.IntegratedGradients","content":" ExplainableAI.IntegratedGradients  —  Function IntegratedGradients(analyzer, [n=50])\nIntegratedGradients(analyzer, [n=50]) Analyze model by using the Integrated Gradients method. References Sundararajan et al.,  Axiomatic Attribution for Deep Networks source"},{"id":17,"pagetitle":"API Reference","title":"ExplainableAI.GradCAM","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.GradCAM","content":" ExplainableAI.GradCAM  —  Type GradCAM(feature_layers, adaptation_layers) Calculates the Gradient-weighted Class Activation Map (GradCAM). GradCAM provides a visual explanation of the regions with significant neuron importance for the model's classification decision. Parameters feature_layers : The layers of a convolutional neural network (CNN) responsible for extracting feature maps. adaptation_layers : The layers of the CNN used for adaptation and classification. Note Flux is not required for GradCAM.  GradCAM is compatible with a wide variety of CNN model-families. References Selvaraju et al.,  Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization source"},{"id":18,"pagetitle":"API Reference","title":"Input augmentations","ref":"/XAIDocs/ExplainableAI/stable/api/#Input-augmentations","content":" Input augmentations SmoothGrad  and  IntegratedGradients  are special cases of the input augmentations   NoiseAugmentation  and  InterpolationAugmentation ,  which can be applied as a wrapper to any analyzer:"},{"id":19,"pagetitle":"API Reference","title":"ExplainableAI.NoiseAugmentation","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.NoiseAugmentation","content":" ExplainableAI.NoiseAugmentation  —  Type NoiseAugmentation(analyzer, n)\nNoiseAugmentation(analyzer, n, std::Real)\nNoiseAugmentation(analyzer, n, distribution::Sampleable) A wrapper around analyzers that augments the input with  n  samples of additive noise sampled from a scalar  distribution . This input augmentation is then averaged to return an  Explanation . Defaults to the normal distribution  Normal(0, std^2)  with  std=1.0f0 . For optimal results, Smilkov et al.,  SmoothGrad: removing noise by adding noise  recommends setting  std  between 10% and 20% of the input range of each sample, e.g.  std = 0.1 * (maximum(input) - minimum(input)) . Keyword arguments rng::AbstractRNG : Specify the random number generator that is used to sample noise from the  distribution .  Defaults to  GLOBAL_RNG .  source"},{"id":20,"pagetitle":"API Reference","title":"ExplainableAI.InterpolationAugmentation","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.InterpolationAugmentation","content":" ExplainableAI.InterpolationAugmentation  —  Type InterpolationAugmentation(model, [n=50]) A wrapper around analyzers that augments the input with  n  steps of linear interpolation between the input and a reference input (typically  zero(input) ). The gradients w.r.t. this augmented input are then averaged and multiplied with the difference between the input and the reference input. source"},{"id":21,"pagetitle":"API Reference","title":"Index","ref":"/XAIDocs/ExplainableAI/stable/api/#Index","content":" Index ExplainableAI.GradCAM ExplainableAI.Gradient ExplainableAI.InputTimesGradient ExplainableAI.InterpolationAugmentation ExplainableAI.NoiseAugmentation XAIBase.Explanation ExplainableAI.IntegratedGradients ExplainableAI.SmoothGrad XAIBase.analyze"},{"id":24,"pagetitle":"Input augmentations","title":"Analyzer augmentations","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#docs-augmentations","content":" Analyzer augmentations All analyzers implemented in ExplainableAI.jl can be augmented by two types of augmentations:  NoiseAugmentation s and  InterpolationAugmentation s. These augmentations are wrappers around analyzers that modify the input before passing it to the analyzer. We build on the basics shown in the  Getting started  section and start out by loading the same pre-trained LeNet5 model and MNIST input data: using ExplainableAI\nusing VisionHeatmaps\nusing Zygote\nusing Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)"},{"id":25,"pagetitle":"Input augmentations","title":"Noise augmentation","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#Noise-augmentation","content":" Noise augmentation The  NoiseAugmentation  wrapper computes explanations averaged over noisy inputs. Let's demonstrate this on the  Gradient  analyzer. First, we compute the heatmap of an explanation without augmentation: analyzer = Gradient(model)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.542044692993164, 0.5190736373901367, 0.18255157775878905) RGB(0.5405419067382813, 0.5186877868652343, 0.18299835205078127) … RGB(0.5501000518798829, 0.5211419052124023, 0.18022431030273436) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); RGB(0.5577551651000977, 0.5230979751586914, 0.1782020248413086) RGB(0.5673554992675781, 0.5255300598144531, 0.17589942016601562) … RGB(0.550458218383789, 0.5212338668823242, 0.18012750854492188) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); … ; RGB(0.5540689025878907, 0.5221609344482422, 0.17915164794921876) RGB(0.5669629669189453, 0.5254306182861328, 0.17598839416503906) … RGB(0.5511143676757813, 0.5214023376464844, 0.1799501708984375) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) … RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687);;;] Now we wrap the analyzer in a  NoiseAugmentation  with 10 samples of noise. By default, the noise is sampled from a Gaussian distribution with mean 0 and standard deviation 1. analyzer = NoiseAugmentation(Gradient(model), 50)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.40819922637939454, 0.4820197738647461, 0.24198040466308593) RGB(0.40804266738891604, 0.4819740104675293, 0.24206229705810545) … RGB(0.41625033569335934, 0.4842731750488281, 0.23776905517578126) RGB(0.409951774597168, 0.48252510681152344, 0.24106368713378906); RGB(0.40696287155151367, 0.4816583778381347, 0.24262711334228515) RGB(0.4001279335021973, 0.479660472869873, 0.2462464973449707) … RGB(0.4171973266601562, 0.48454998779296876, 0.2372737060546875) RGB(0.4097672958374023, 0.4824740203857422, 0.2411601837158203); … ; RGB(0.4076599235534668, 0.48186213150024415, 0.24226250152587891) RGB(0.4150740852355957, 0.48394359283447264, 0.23838432464599607) … RGB(0.40349184417724615, 0.48064376983642576, 0.2444427276611328) RGB(0.4079584121704102, 0.4819493820190429, 0.24210636901855467); RGB(0.411337100982666, 0.4829087356567383, 0.24033905487060545) RGB(0.4091468124389648, 0.482296760559082, 0.2414847442626953) … RGB(0.4060006561279297, 0.48137711486816404, 0.24313042602539062) RGB(0.40858340835571283, 0.4821320732116699, 0.24177944793701173);;;] Note that a higher sample size is desired, as it will lead to a smoother heatmap. However, this comes at the cost of a longer computation time. We can also set the standard deviation of the Gaussian distribution: analyzer = NoiseAugmentation(Gradient(model), 50, 0.1)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.5190726928710937, 0.5129775161743164, 0.1903570831298828) RGB(0.5176305877685546, 0.5125941009521484, 0.19089259338378906) … RGB(0.5242746093749999, 0.514350244140625, 0.18847861328125) RGB(0.5223930908203125, 0.5138537322998047, 0.18915805053710938); RGB(0.520618798828125, 0.5133855163574219, 0.18979876708984375) RGB(0.5207162475585938, 0.5134112319946289, 0.1897635772705078) … RGB(0.5246007934570313, 0.5144363204956055, 0.18836082458496092) RGB(0.5211674560546875, 0.5135303009033203, 0.18960064086914064); … ; RGB(0.5188783447265625, 0.5129262298583985, 0.19042726440429686) RGB(0.523073583984375, 0.5140333068847657, 0.18891231689453125) … RGB(0.5193462524414063, 0.5130497055053711, 0.19025829772949218) RGB(0.5198564575195312, 0.513184342956543, 0.19007405700683594); RGB(0.5219544067382812, 0.5137379684448242, 0.18931646423339843) RGB(0.5236450927734375, 0.5141841217041017, 0.18870593872070313) … RGB(0.5195867431640625, 0.513113168334961, 0.19017145385742187) RGB(0.5207046020507812, 0.5134081588745117, 0.18976778259277344);;;] When used with a  Gradient  analyzer, this is equivalent to  SmoothGrad : analyzer = SmoothGrad(model, 50)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.31847197265625, 0.455286572265625, 0.291114208984375) RGB(0.320031575012207, 0.4557645149230957, 0.29023378829956054) … RGB(0.325915170288086, 0.45756755218505857, 0.28699490661621097) RGB(0.3214790710449219, 0.45620810241699217, 0.2894276062011719); RGB(0.3185287826538086, 0.45530398178100584, 0.2910821388244629) RGB(0.32032011871337884, 0.4558529396057129, 0.29007090072631836) … RGB(0.32855203399658206, 0.45837562332153314, 0.28552385177612305) RGB(0.3211192901611328, 0.4560978469848633, 0.2896249053955078); … ; RGB(0.3206842514038086, 0.45596452865600584, 0.2898653419494629) RGB(0.32145016937255855, 0.4561992454528809, 0.2894434555053711) … RGB(0.3200398056030273, 0.4557670372009277, 0.29022914199829103) RGB(0.3178595977783203, 0.45509890899658206, 0.2914599044799805); RGB(0.3205546905517578, 0.4559248245239258, 0.2899384811401367) RGB(0.32228051300048827, 0.4564537055969239, 0.28898810577392575) … RGB(0.3118085258483887, 0.45323052444458006, 0.2948559036254883) RGB(0.3188186508178711, 0.45539281234741213, 0.29091850357055665);;;] We can also use any distribution from  Distributions.jl , for example Poisson noise with rate  $\\lambda=0.5$ : using Distributions\n\nanalyzer = NoiseAugmentation(Gradient(model), 50, Poisson(0.5))\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.35835003585815434, 0.46731509017944334, 0.26897220230102536) RGB(0.35938411026000977, 0.4676269538879394, 0.2683977165222168) … RGB(0.3696745300292969, 0.4707304138183594, 0.26276136474609374) RGB(0.3611967445373535, 0.46817362136840823, 0.26739069747924804); RGB(0.35522776947021484, 0.466373454284668, 0.2707067947387696) RGB(0.35666587982177733, 0.46680717010498046, 0.26990784454345707) … RGB(0.3668849708557129, 0.4698891181945801, 0.26426684112548826) RGB(0.36248561248779293, 0.4685623275756836, 0.26667465972900395); … ; RGB(0.3586190086364746, 0.4673962089538574, 0.26882277297973634) RGB(0.3524949752807617, 0.46554927825927733, 0.2722250137329102) … RGB(0.3514731575012207, 0.4652494735717773, 0.2727926902770996) RGB(0.35396480865478513, 0.465992561340332, 0.2714084396362305); RGB(0.3575334564208984, 0.4670688201904297, 0.2694258575439453) RGB(0.3546446426391602, 0.466197590637207, 0.2710307540893555) … RGB(0.35414769668579105, 0.46604771804809575, 0.27130683517456056) RGB(0.3553883071899414, 0.46642187042236327, 0.27061760711669924);;;] Is is also possible to define your own distributions or mixture distributions. NoiseAugmentation  can be combined with any analyzer type from the Julia-XAI ecosystem, for example  LRP  from  RelevancePropagation.jl ."},{"id":26,"pagetitle":"Input augmentations","title":"Integration augmentation","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#Integration-augmentation","content":" Integration augmentation The  InterpolationAugmentation  wrapper computes explanations averaged over  n  steps of linear interpolation between the input and a reference input, which is set to  zero(input)  by default: analyzer = InterpolationAugmentation(Gradient(model), 50)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); … ; RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156);;;] When used with a  Gradient  analyzer, this is equivalent to  IntegratedGradients : analyzer = IntegratedGradients(model, 50)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); … ; RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156); RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) … RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156) RGB(0.26731090087890624, 0.43895178527832035, 0.31948900451660156);;;] To select a different reference input, pass it to the  analyze  function using the keyword argument  input_ref . Note that this is an arbitrary example for the sake of demonstration. matrix_of_ones = ones(Float32, size(input))\n\nanalyzer = InterpolationAugmentation(Gradient(model), 50)\nexpl = analyzer(input; input_ref=matrix_of_ones)\nheatmap(expl) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.4490871696472168, 0.49369636154174806, 0.22110631942749023) RGB(0.44208317718505863, 0.49171015472412116, 0.22455604705810545) … RGB(0.41876847076416013, 0.485009245300293, 0.23645187683105467) RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185); RGB(0.44949763870239257, 0.4938127632141114, 0.22090414810180664) RGB(0.4431518791198731, 0.4920132194519043, 0.22402967147827146) … RGB(0.4489227264404297, 0.49364972839355475, 0.22118731384277343) RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185); … ; RGB(0.4539996612548828, 0.4950894561767578, 0.21868673400878905) RGB(0.47183586425781254, 0.5000806091308594, 0.21015371093750002) … RGB(0.4439007423400879, 0.49222558364868163, 0.2236608283996582) RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185); RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185) RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185) … RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185) RGB(0.44284696655273437, 0.49192675170898437, 0.22417985229492185);;;] Once again,  InterpolationAugmentation  can be combined with any analyzer type from the Julia-XAI ecosystem, for example  LRP  from  RelevancePropagation.jl . This page was generated using  Literate.jl ."},{"id":29,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#docs-getting-started","content":" Getting started Note This package is part of a wider  Julia XAI ecosystem . For an introduction to this ecosystem, please refer to the  Getting started guide . For this first example, we already have loaded a pre-trained LeNet5 model to look at explanations on the MNIST dataset. using Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB. Supported models ExplainableAI.jl can be used on any differentiable classifier."},{"id":30,"pagetitle":"Getting started","title":"Preparing the input data","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Preparing-the-input-data","content":" Preparing the input data We use MLDatasets to load a single image from the MNIST dataset: using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\n\nconvert2image(MNIST, x) By convention in Flux.jl, this input needs to be resized to WHCN format by adding a color channel and batch dimensions. input = reshape(x, 28, 28, 1, :); Input format For any explanation of a model, ExplainableAI.jl assumes the batch dimension to come last in the input. For the purpose of heatmapping, the input is assumed to be in WHCN order (width, height, channels, batch), which is Flux.jl's convention."},{"id":31,"pagetitle":"Getting started","title":"Explanations","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Explanations","content":" Explanations We can now select an analyzer of our choice and call  analyze  to get an  Explanation . Note that for gradient-based optimizers, a backend for automatic differentiation must be loaded, by default  Zygote.jl : using ExplainableAI\nusing Zygote\n\nanalyzer = InputTimesGradient(model)\nexpl = analyze(input, analyzer); The return value  expl  is of type  Explanation  and bundles the following data: expl.val : numerical output of the analyzer, e.g. an attribution or gradient expl.output : model output for the given analyzer input expl.output_selection : index of the output used for the explanation expl.analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP expl.heatmap : symbol indicating a preset heatmapping style,   e.g.  :attibution ,  :sensitivity  or  :cam expl.extras : optional named tuple that can be used by analyzers   to return additional information. We used  InputTimesGradient , so  expl.analyzer  is  :InputTimesGradient . expl.analyzer :InputTimesGradient By default, the explanation is computed for the maximally activated output neuron. Since our digit is a 9 and Julia's indexing is 1-based, the output neuron at index  10  of our trained model is maximally activated. Finally, we obtain the result of the analyzer in form of an array. expl.val 28×28×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0  …  -0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0     -0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0     -0.0       0.0         0.0\n  0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0  -0.0   0.0   0.0      0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0   0.0  -0.0  -0.0  …   0.0       0.0         0.0\n -0.0   0.0   0.0   0.0   0.0   0.0   0.0      0.0       0.0         0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.0       0.0        -0.0\n -0.0   0.0   0.0   0.0  -0.0  -0.0  -0.0      0.0       0.0        -0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.385115  0.0714216   0.0\n  ⋮                             ⋮          ⋱   ⋮                    \n  0.0  -0.0  -0.0   0.0   0.0   0.0  -0.0     -0.0       0.0         0.0\n -0.0  -0.0   0.0   0.0   0.0   0.0  -0.0  …  -0.0       0.0         0.0\n -0.0   0.0   0.0   0.0   0.0   0.0  -0.0      0.0       0.0         0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0   0.0  -0.0  -0.0   0.0   0.0  …   0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0   0.0   0.0   0.0      0.0       0.0         0.0\n  0.0   0.0   0.0   0.0   0.0   0.0  -0.0      0.0       0.0         0.0"},{"id":32,"pagetitle":"Getting started","title":"Heatmapping basics","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Heatmapping-basics","content":" Heatmapping basics Since the array  expl.val  is not very informative at first sight, we can visualize  Explanation s by computing a  heatmap  using either  VisionHeatmaps.jl  or  TextHeatmaps.jl . using VisionHeatmaps\n\nheatmap(expl) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;] If we are only interested in the heatmap, we can combine analysis and heatmapping into a single function call: heatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]"},{"id":33,"pagetitle":"Getting started","title":"Neuron selection","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Neuron-selection","content":" Neuron selection By passing an additional index to our call to  analyze , we can compute an explanation with respect to a specific output neuron. Let's see why the output wasn't interpreted as a 4 (output neuron at index 5) expl = analyze(input, analyzer, 5)\nheatmap(expl) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;] This heatmap shows us that the \"upper loop\" of the hand-drawn 9 has negative relevance with respect to the output neuron corresponding to digit 4! Note The output neuron can also be specified when calling  heatmap : heatmap(input, analyzer, 5)"},{"id":34,"pagetitle":"Getting started","title":"Analyzing batches","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Analyzing-batches","content":" Analyzing batches ExplainableAI also supports explanations of input batches: batchsize = 20\nxs, _ = MNIST(Float32, :test)[1:batchsize]\nbatch = reshape(xs, 28, 28, 1, :) # reshape to WHCN format\nexpl = analyze(batch, analyzer); This will return a single  Explanation expl  for the entire batch. Calling  heatmap  on  expl  will detect the batch dimension and return a vector of heatmaps. heatmap(expl)\n\n# Custom heatmaps 20-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;]\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;] The function  heatmap  automatically applies common presets for each method. Since  InputTimesGradient  computes attributions, heatmaps are shown in a blue-white-red color scheme. Gradient methods however are typically shown in grayscale: analyzer = Gradient(model)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.542044692993164, 0.5190736373901367, 0.18255157775878905) RGB(0.5405419067382813, 0.5186877868652343, 0.18299835205078127) … RGB(0.5501000518798829, 0.5211419052124023, 0.18022431030273436) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); RGB(0.5577551651000977, 0.5230979751586914, 0.1782020248413086) RGB(0.5673554992675781, 0.5255300598144531, 0.17589942016601562) … RGB(0.550458218383789, 0.5212338668823242, 0.18012750854492188) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); … ; RGB(0.5540689025878907, 0.5221609344482422, 0.17915164794921876) RGB(0.5669629669189453, 0.5254306182861328, 0.17598839416503906) … RGB(0.5511143676757813, 0.5214023376464844, 0.1799501708984375) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687); RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) … RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687) RGB(0.5480906158447265, 0.5206259689331054, 0.18076740112304687);;;] analyzer = InputTimesGradient(model)\nheatmap(input, analyzer) 1-element Vector{Array{ColorTypes.RGB{Float64}, 3}}:\n [RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); … ; RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005); RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005) … RGB(0.0963, 0.0479, 0.037250000000000005) RGB(0.0963, 0.0479, 0.037250000000000005);;;] Using  VisionHeatmaps.jl , heatmaps can be heavily customized. Check out the  heatmapping documentation  for more information. This page was generated using  Literate.jl ."},{"id":37,"pagetitle":"Home","title":"RelevancePropagation.jl","ref":"/XAIDocs/RelevancePropagation/stable/#RelevancePropagation.jl","content":" RelevancePropagation.jl Julia implementation of  Layerwise Relevance Propagation  (LRP)  and  Concept Relevance Propagation  (CRP)  for use with  Flux.jl  models. Note This package is part the  Julia-XAI ecosystem . For an introduction to the ecosystem, please refer to the   Getting started  guide ."},{"id":38,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/RelevancePropagation/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run  julia> ]add RelevancePropagation"},{"id":39,"pagetitle":"Home","title":"Manual","ref":"/XAIDocs/RelevancePropagation/stable/#Manual","content":" Manual"},{"id":40,"pagetitle":"Home","title":"Basic Usage","ref":"/XAIDocs/RelevancePropagation/stable/#Basic-Usage","content":" Basic Usage Creating an LRP Analyzer Model preparation Stripping the output softmax Model canonization Flattening the model LRP rules Computing layerwise relevances Performance tips Using LRP with a GPU Using LRP without a GPU Assigning LRP Rules to Layers Manually assigning rules Custom composites Composite primitives Assigning a rule to a specific layer Composite presets Concept Relevance Propagation Step 1: Create LRP analyzer Step 2: Define concepts Step 3: Use CRP analyzer Using CRP on input batches"},{"id":41,"pagetitle":"Home","title":"Advanced Usage","ref":"/XAIDocs/RelevancePropagation/stable/#Advanced-Usage","content":" Advanced Usage Supporting New Layers and Activation Functions Model checks Registering layers Registering activation functions Skipping model checks Custom LRP Rules Implementing a custom rule Step 1: Define rule struct Step 2: Implement rule behavior Step 3: Use rule in LRP analyzer Performance tips Advanced layer modification Advanced LRP rules Developer Documentation Generic LRP rule implementation Linear layers The automatic differentiation fallback LRP analyzer struct Forward and reverse pass Rule calls Specialized implementations"},{"id":42,"pagetitle":"Home","title":"LRP Rule Overview","ref":"/XAIDocs/RelevancePropagation/stable/#LRP-Rule-Overview","content":" LRP Rule Overview LRP Rule Overview Notation Basic rules Lower layer rules Input layer rules Specialized rules"},{"id":43,"pagetitle":"Home","title":"API Reference","ref":"/XAIDocs/RelevancePropagation/stable/#API-Reference","content":" API Reference API Reference Basic API LRP analyzer Model preparation LRP rules Composites Applying composites Composite primitives Composite presets Manual rule assignment Custom rules CRP Index"},{"id":46,"pagetitle":"API Reference","title":"API Reference","ref":"/XAIDocs/RelevancePropagation/stable/api/#API-Reference","content":" API Reference"},{"id":47,"pagetitle":"API Reference","title":"Basic API","ref":"/XAIDocs/RelevancePropagation/stable/api/#Basic-API","content":" Basic API All methods in RelevancePropagation.jl work by calling  analyze  on an input and an analyzer:"},{"id":48,"pagetitle":"API Reference","title":"XAIBase.analyze","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.analyze","content":" XAIBase.analyze  —  Function analyze(input, method)\nanalyze(input, method, output_selection) Apply the analyzer  method  for the given input, returning an  Explanation . If  output_selection  is specified, the explanation will be calculated for that output. Otherwise, the output with the highest activation is automatically chosen. See also  Explanation . source"},{"id":49,"pagetitle":"API Reference","title":"XAIBase.Explanation","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.Explanation","content":" XAIBase.Explanation  —  Type Explanation(val, output, output_selection, analyzer, heatmap, extras) Return type of analyzers when calling  analyze . Fields val : numerical output of the analyzer, e.g. an attribution or gradient output : model output for the given analyzer input output_selection : index of the output used for the explanation analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP heatmap : symbol indicating a preset heatmapping style,   e.g.  :attribution ,  :sensitivity  or  :cam extras : optional named tuple that can be used by analyzers   to return additional information. source For heatmapping functionality, take a look at either  VisionHeatmaps.jl  or  TextHeatmaps.jl . Both provide  heatmap  methods for visualizing explanations,  either for images or text, respectively."},{"id":50,"pagetitle":"API Reference","title":"LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/api/#LRP-analyzer","content":" LRP analyzer"},{"id":51,"pagetitle":"API Reference","title":"RelevancePropagation.LRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP","content":" RelevancePropagation.LRP  —  Type LRP(model, rules)\nLRP(model, composite) Analyze model by applying Layer-Wise Relevance Propagation. The analyzer can either be created by passing an array of LRP-rules or by passing a composite, see  Composite  for an example. Keyword arguments normalize_output_relevance : Selects whether output relevance should be set to 1 before applying LRP backward pass.   Defaults to  true  to match literature. If  false , values of output activations are used. skip_checks::Bool : Skip checks whether model is compatible with LRP and contains output softmax. Defaults to  false . verbose::Bool : Select whether the model checks should print a summary on failure. Defaults to  true . References [1] G. Montavon et al., Layer-Wise Relevance Propagation: An Overview [2] W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications source"},{"id":52,"pagetitle":"API Reference","title":"Model preparation","ref":"/XAIDocs/RelevancePropagation/stable/api/#Model-preparation","content":" Model preparation"},{"id":53,"pagetitle":"API Reference","title":"RelevancePropagation.strip_softmax","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.strip_softmax","content":" RelevancePropagation.strip_softmax  —  Function strip_softmax(model)\nstrip_softmax(layer) Remove softmax activation on layer or model if it exists. source"},{"id":54,"pagetitle":"API Reference","title":"RelevancePropagation.canonize","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.canonize","content":" RelevancePropagation.canonize  —  Function canonize(model) Canonize model by flattening it and fusing BatchNorm layers into preceding Dense and Conv layers with linear activation functions. source"},{"id":55,"pagetitle":"API Reference","title":"RelevancePropagation.flatten_model","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.flatten_model","content":" RelevancePropagation.flatten_model  —  Function flatten_model(model) Flatten a Flux  Chain  containing  Chain s. source"},{"id":56,"pagetitle":"API Reference","title":"LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#LRP-rules","content":" LRP rules Refer to the  LRP rule overview  for a detailed explanation  of the notation used for LRP rules."},{"id":57,"pagetitle":"API Reference","title":"RelevancePropagation.ZeroRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZeroRule","content":" RelevancePropagation.ZeroRule  —  Type ZeroRule() LRP- $0$  rule. Commonly used on upper layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i \\frac{W_{ij}a_j^k}{\\sum_l W_{il}a_l^k+b_i} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":58,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonRule","content":" RelevancePropagation.EpsilonRule  —  Type EpsilonRule([epsilon=1.0e-6]) LRP- $ϵ$  rule. Commonly used on middle layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}a_j^k}{\\epsilon +\\sum_{l}W_{il}a_l^k+b_i} R_i^{k+1}\\] Optional arguments epsilon : Optional stabilization parameter, defaults to  1.0e-6 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":59,"pagetitle":"API Reference","title":"RelevancePropagation.GammaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GammaRule","content":" RelevancePropagation.GammaRule  —  Type GammaRule([gamma=0.25]) LRP- $γ$  rule. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{(W_{ij}+\\gamma W_{ij}^+)a_j^k}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_l^k+(b_i+\\gamma b_i^+)} R_i^{k+1}\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":60,"pagetitle":"API Reference","title":"RelevancePropagation.WSquareRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.WSquareRule","content":" RelevancePropagation.WSquareRule  —  Type WSquareRule() LRP- $w²$  rule. Commonly used on the first layer when values are unbounded. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}^2}{\\sum_l W_{il}^2} R_i^{k+1}\\] References G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":61,"pagetitle":"API Reference","title":"RelevancePropagation.FlatRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FlatRule","content":" RelevancePropagation.FlatRule  —  Type FlatRule() LRP-Flat rule. Similar to the  WSquareRule , but with all weights set to one and all bias terms set to zero. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{1}{\\sum_l 1} R_i^{k+1} = \\sum_i\\frac{1}{n_i} R_i^{k+1}\\] where  $n_i$  is the number of input neurons connected to the output neuron at index  $i$ . References S. Lapuschkin et al.,  Unmasking Clever Hans predictors and assessing what machines really learn source"},{"id":62,"pagetitle":"API Reference","title":"RelevancePropagation.AlphaBetaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.AlphaBetaRule","content":" RelevancePropagation.AlphaBetaRule  —  Type AlphaBetaRule([alpha=2.0, beta=1.0]) LRP- $αβ$  rule. Weights positive and negative contributions according to the parameters  alpha  and  beta  respectively. The difference  $α-β$  must be equal to one. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\left(\n    \\alpha\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+}\n    -\\beta\\frac{\\left(W_{ij}a_j^k\\right)^-}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^-}\n\\right) R_i^{k+1}\\] Optional arguments alpha : Multiplier for the positive output term, defaults to  2.0 . beta : Multiplier for the negative output term, defaults to  1.0 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":63,"pagetitle":"API Reference","title":"RelevancePropagation.ZPlusRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZPlusRule","content":" RelevancePropagation.ZPlusRule  —  Type ZPlusRule() LRP- $z⁺$  rule. Commonly used on lower layers. Equivalent to  AlphaBetaRule(1.0f0, 0.0f0) , but slightly faster. See also  AlphaBetaRule . Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":64,"pagetitle":"API Reference","title":"RelevancePropagation.ZBoxRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZBoxRule","content":" RelevancePropagation.ZBoxRule  —  Type ZBoxRule(low, high) LRP- $zᴮ$ -rule. Commonly used on the first layer for pixel input. The parameters  low  and  high  should be set to the lower and upper bounds of the input features, e.g.  0.0  and  1.0  for raw image data. It is also possible to provide two arrays of that match the input size. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k=\\sum_i \\frac{W_{ij}a_j^k - W_{ij}^{+}l_j - W_{ij}^{-}h_j}\n    {\\sum_l W_{il}a_l^k+b_i - \\left(W_{il}^{+}l_l+b_i^{+}\\right) - \\left(W_{il}^{-}h_l+b_i^{-}\\right)} R_i^{k+1}\\] References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":65,"pagetitle":"API Reference","title":"RelevancePropagation.PassRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.PassRule","content":" RelevancePropagation.PassRule  —  Type PassRule() Pass-through rule. Passes relevance through to the lower layer. Supports layers with constant input and output shapes, e.g. reshaping layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = R_j^{k+1}\\] source"},{"id":66,"pagetitle":"API Reference","title":"RelevancePropagation.GeneralizedGammaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GeneralizedGammaRule","content":" RelevancePropagation.GeneralizedGammaRule  —  Type GeneralizedGammaRule([gamma=0.25]) Generalized LRP- $γ$  rule. Can be used on layers with  leakyrelu  activation functions. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^+)a_j^+ +(W_{ij}+\\gamma W_{ij}^-)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_j^+ +(W_{il}+\\gamma W_{il}^-)a_j^- +(b_i+\\gamma b_i^+)}\nI(z_k>0) \\cdot R^{k+1}_i\n+\\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^-)a_j^+ +(W_{ij}+\\gamma W_{ij}^+)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^-)a_j^+ +(W_{il}+\\gamma W_{il}^+)a_j^- +(b_i+\\gamma b_i^-)}\nI(z_k<0) \\cdot R^{k+1}_i\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References L. Andéol et al.,  Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization source"},{"id":67,"pagetitle":"API Reference","title":"RelevancePropagation.LayerNormRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LayerNormRule","content":" RelevancePropagation.LayerNormRule  —  Type LayerNormRule() LRP-LN rule. Used on  LayerNorm  layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_i^k = \\sum_j\\frac{a_i^k\\left(\\delta_{ij} - 1/N\\right)}{\\sum_l a_l^k\\left(\\delta_{lj}-1/N\\right)} R_j^{k+1}\\] Relevance through the affine transformation is by default propagated using the  ZeroRule . If you would like to assign a special rule to the affine transformation inside of the  LayerNorm  layer, call  canonize  on your model. This will split the  LayerNorm  layer into a  LayerNorm  layer without affine transformation a  Scale  layer implementing the affine transformation You can then assign separate rules to these two layers. References A. Ali et al.,  XAI for Transformers: Better Explanations through Conservative Propagation source"},{"id":68,"pagetitle":"API Reference","title":"Composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Composites","content":" Composites"},{"id":69,"pagetitle":"API Reference","title":"Applying composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Applying-composites","content":" Applying composites"},{"id":70,"pagetitle":"API Reference","title":"RelevancePropagation.Composite","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.Composite","content":" RelevancePropagation.Composite  —  Type Composite(primitives...)\nComposite(default_rule, primitives...) Automatically contructs a list of LRP-rules by sequentially applying composite primitives. Primitives To apply a single rule, use: LayerMap  to apply a rule to the  n -th layer of a model GlobalMap  to apply a rule to all layers RangeMap  to apply a rule to a positional range of layers FirstLayerMap  to apply a rule to the first layer LastLayerMap  to apply a rule to the last layer To apply a set of rules to layers based on their type, use: GlobalTypeMap  to apply a dictionary that maps layer types to LRP-rules RangeTypeMap  for a  TypeMap  on generalized ranges FirstLayerTypeMap  for a  TypeMap  on the first layer of a model LastLayerTypeMap  for a  TypeMap  on the last layer FirstNTypeMap  for a  TypeMap  on the first  n  layers Example Using a VGG11 model: julia> composite = Composite(\n           GlobalTypeMap(\n               ConvLayer => AlphaBetaRule(),\n               Dense => EpsilonRule(),\n               PoolingLayer => EpsilonRule(),\n               DropoutLayer => PassRule(),\n               ReshapingLayer => PassRule(),\n           ),\n           FirstNTypeMap(7, Conv => FlatRule()),\n       );\n\njulia> analyzer = LRP(model, composite)\nLRP(\n  Conv((3, 3), 3 => 64, relu, pad=1)    => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 64 => 128, relu, pad=1)  => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 128 => 256, relu, pad=1) => FlatRule(),\n  Conv((3, 3), 256 => 256, relu, pad=1) => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 256 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  MLUtils.flatten                       => PassRule(),\n  Dense(25088 => 4096, relu)            => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 4096, relu)             => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 1000)                   => EpsilonRule{Float32}(1.0f-6),\n) source"},{"id":71,"pagetitle":"API Reference","title":"RelevancePropagation.lrp_rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.lrp_rules","content":" RelevancePropagation.lrp_rules  —  Function lrp_rules(model, composite) Apply a composite to obtain LRP-rules for a given Flux model. source"},{"id":72,"pagetitle":"API Reference","title":"Composite primitives","ref":"/XAIDocs/RelevancePropagation/stable/api/#api-composite-primitives","content":" Composite primitives"},{"id":73,"pagetitle":"API Reference","title":"Mapping layers to rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#Mapping-layers-to-rules","content":" Mapping layers to rules Composite primitives that apply a single rule:"},{"id":74,"pagetitle":"API Reference","title":"RelevancePropagation.LayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LayerMap","content":" RelevancePropagation.LayerMap  —  Type LayerMap(index, rule) Composite primitive that maps an LRP-rule to all layers in the model at the given index. The index can either be an integer or a tuple of integers to map a rule to a specific layer in nested Flux  Chain s. See  show_layer_indices  to print layer indices and  Composite  for an example. source"},{"id":75,"pagetitle":"API Reference","title":"RelevancePropagation.GlobalMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GlobalMap","content":" RelevancePropagation.GlobalMap  —  Type GlobalMap(rule) Composite primitive that maps an LRP-rule to all layers in the model. See  Composite  for an example. source"},{"id":76,"pagetitle":"API Reference","title":"RelevancePropagation.RangeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.RangeMap","content":" RelevancePropagation.RangeMap  —  Type RangeMap(range, rule) Composite primitive that maps an LRP-rule to the specified positional  range  of layers in the model. See  Composite  for an example. source"},{"id":77,"pagetitle":"API Reference","title":"RelevancePropagation.FirstLayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstLayerMap","content":" RelevancePropagation.FirstLayerMap  —  Type FirstLayerMap(rule) Composite primitive that maps an LRP-rule to the first layer in the model. See  Composite  for an example. source"},{"id":78,"pagetitle":"API Reference","title":"RelevancePropagation.LastLayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LastLayerMap","content":" RelevancePropagation.LastLayerMap  —  Type LastLayerMap(rule) Composite primitive that maps an LRP-rule to the last layer in the model. See  Composite  for an example. source To apply  LayerMap  to nested Flux Chains or  Parallel  layers,  make use of  show_layer_indices :"},{"id":79,"pagetitle":"API Reference","title":"RelevancePropagation.show_layer_indices","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.show_layer_indices","content":" RelevancePropagation.show_layer_indices  —  Function show_layer_indices(model) Print layer indices of Flux models. This is primarily a utility to help define  LayerMap  primitives. source"},{"id":80,"pagetitle":"API Reference","title":"Mapping layers to rules based on type","ref":"/XAIDocs/RelevancePropagation/stable/api/#Mapping-layers-to-rules-based-on-type","content":" Mapping layers to rules based on type Composite primitives that apply rules based on the layer type:"},{"id":81,"pagetitle":"API Reference","title":"RelevancePropagation.GlobalTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GlobalTypeMap","content":" RelevancePropagation.GlobalTypeMap  —  Type GlobalTypeMap(map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":82,"pagetitle":"API Reference","title":"RelevancePropagation.RangeTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.RangeTypeMap","content":" RelevancePropagation.RangeTypeMap  —  Type RangeTypeMap(range, map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map  within the specified  range  of layers in the model. See  Composite  for an example. source"},{"id":83,"pagetitle":"API Reference","title":"RelevancePropagation.FirstLayerTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstLayerTypeMap","content":" RelevancePropagation.FirstLayerTypeMap  —  Type FirstLayerTypeMap(map) Composite primitive that maps the type of the first layer of the model to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":84,"pagetitle":"API Reference","title":"RelevancePropagation.LastLayerTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LastLayerTypeMap","content":" RelevancePropagation.LastLayerTypeMap  —  Type LastLayerTypeMap(map) Composite primitive that maps the type of the last layer of the model to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":85,"pagetitle":"API Reference","title":"RelevancePropagation.FirstNTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstNTypeMap","content":" RelevancePropagation.FirstNTypeMap  —  Type FirstNTypeMap(n, map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map  within the first  n  layers in the model. See  Composite  for an example. source"},{"id":86,"pagetitle":"API Reference","title":"Union types for composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Union-types-for-composites","content":" Union types for composites The following exported union types types can be used to define TypeMaps:"},{"id":87,"pagetitle":"API Reference","title":"RelevancePropagation.ConvLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ConvLayer","content":" RelevancePropagation.ConvLayer  —  Type Union type for convolutional layers. source"},{"id":88,"pagetitle":"API Reference","title":"RelevancePropagation.PoolingLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.PoolingLayer","content":" RelevancePropagation.PoolingLayer  —  Type Union type for pooling layers. source"},{"id":89,"pagetitle":"API Reference","title":"RelevancePropagation.DropoutLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.DropoutLayer","content":" RelevancePropagation.DropoutLayer  —  Type Union type for dropout layers. source"},{"id":90,"pagetitle":"API Reference","title":"RelevancePropagation.ReshapingLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ReshapingLayer","content":" RelevancePropagation.ReshapingLayer  —  Type Union type for reshaping layers such as  flatten . source"},{"id":91,"pagetitle":"API Reference","title":"RelevancePropagation.NormalizationLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.NormalizationLayer","content":" RelevancePropagation.NormalizationLayer  —  Type Union type for normalization layers. source"},{"id":92,"pagetitle":"API Reference","title":"Composite presets","ref":"/XAIDocs/RelevancePropagation/stable/api/#api-composite-presets","content":" Composite presets"},{"id":93,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonGammaBox","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonGammaBox","content":" RelevancePropagation.EpsilonGammaBox  —  Function EpsilonGammaBox(low, high; [epsilon=1.0f-6, gamma=0.25f0]) Composite using the following primitives: julia> EpsilonGammaBox(-3.0f0, 3.0f0)\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.ConvTranspose      => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.CrossCor           => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.ConvTranspose => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.CrossCor      => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n ),\n) source"},{"id":94,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonPlus","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonPlus","content":" RelevancePropagation.EpsilonPlus  —  Function EpsilonPlus(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonPlus()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n) source"},{"id":95,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonAlpha2Beta1","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonAlpha2Beta1","content":" RelevancePropagation.EpsilonAlpha2Beta1  —  Function EpsilonAlpha2Beta1(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonAlpha2Beta1()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n) source"},{"id":96,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonPlusFlat","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonPlusFlat","content":" RelevancePropagation.EpsilonPlusFlat  —  Function EpsilonPlusFlat(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonPlusFlat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n) source"},{"id":97,"pagetitle":"API Reference","title":"RelevancePropagation.EpsilonAlpha2Beta1Flat","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonAlpha2Beta1Flat","content":" RelevancePropagation.EpsilonAlpha2Beta1Flat  —  Function EpsilonAlpha2Beta1Flat(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonAlpha2Beta1Flat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n) source"},{"id":98,"pagetitle":"API Reference","title":"Manual rule assignment","ref":"/XAIDocs/RelevancePropagation/stable/api/#Manual-rule-assignment","content":" Manual rule assignment For  manual rule assignment , use  ChainTuple ,   ParallelTuple  and  SkipConnectionTuple , matching the model structure:"},{"id":99,"pagetitle":"API Reference","title":"RelevancePropagation.ChainTuple","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ChainTuple","content":" RelevancePropagation.ChainTuple  —  Type ChainTuple(xs) Thin wrapper around  Tuple  for use with Flux.jl models. Combining  ChainTuple ,  ParallelTuple  and  SkipConnectionTuple , data  xs  can be stored while preserving the structure of a Flux model without risking type piracy. source"},{"id":100,"pagetitle":"API Reference","title":"RelevancePropagation.ParallelTuple","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ParallelTuple","content":" RelevancePropagation.ParallelTuple  —  Type ParallelTuple(xs) Thin wrapper around  Tuple  for use with Flux.jl models. Combining  ChainTuple ,  ParallelTuple  and  SkipConnectionTuple , data  xs  can be stored while preserving the structure of a Flux model without risking type piracy. source"},{"id":101,"pagetitle":"API Reference","title":"RelevancePropagation.SkipConnectionTuple","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.SkipConnectionTuple","content":" RelevancePropagation.SkipConnectionTuple  —  Type SkipConnectionTuple(xs) Thin wrapper around  Tuple  for use with Flux.jl models. Combining  ChainTuple ,  ParallelTuple  and  SkipConnectionTuple , data  xs  can be stored while preserving the structure of a Flux model without risking type piracy. source"},{"id":102,"pagetitle":"API Reference","title":"Custom rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#Custom-rules","content":" Custom rules These utilities can be used to define custom rules without writing boilerplate code. To extend these functions, explicitly  import  them: "},{"id":103,"pagetitle":"API Reference","title":"RelevancePropagation.modify_input","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_input","content":" RelevancePropagation.modify_input  —  Function modify_input(rule, input) Modify input activation before computing relevance propagation. source"},{"id":104,"pagetitle":"API Reference","title":"RelevancePropagation.modify_denominator","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_denominator","content":" RelevancePropagation.modify_denominator  —  Function modify_denominator(rule, d) Modify denominator  $z$  for numerical stability on the forward pass. source"},{"id":105,"pagetitle":"API Reference","title":"RelevancePropagation.modify_parameters","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_parameters","content":" RelevancePropagation.modify_parameters  —  Function modify_parameters(rule, parameter) Modify parameters before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":106,"pagetitle":"API Reference","title":"RelevancePropagation.modify_weight","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_weight","content":" RelevancePropagation.modify_weight  —  Function modify_weight(rule, weight) Modify layer weights before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":107,"pagetitle":"API Reference","title":"RelevancePropagation.modify_bias","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_bias","content":" RelevancePropagation.modify_bias  —  Function modify_bias(rule, bias) Modify layer bias before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":108,"pagetitle":"API Reference","title":"RelevancePropagation.modify_layer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_layer","content":" RelevancePropagation.modify_layer  —  Function modify_layer(rule, layer) Modify layer before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":109,"pagetitle":"API Reference","title":"RelevancePropagation.is_compatible","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.is_compatible","content":" RelevancePropagation.is_compatible  —  Function is_compatible(rule, layer) Check compatibility of a LRP-Rule with layer type. source Compatibility settings:"},{"id":110,"pagetitle":"API Reference","title":"RelevancePropagation.LRP_CONFIG.supports_layer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP_CONFIG.supports_layer","content":" RelevancePropagation.LRP_CONFIG.supports_layer  —  Function LRP_CONFIG.supports_layer(layer) Check whether LRP can be used on a layer or a Chain. To extend LRP to your own layers, define: LRP_CONFIG.supports_layer(::MyLayer) = true          # for structs\nLRP_CONFIG.supports_layer(::typeof(mylayer)) = true  # for functions source"},{"id":111,"pagetitle":"API Reference","title":"RelevancePropagation.LRP_CONFIG.supports_activation","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP_CONFIG.supports_activation","content":" RelevancePropagation.LRP_CONFIG.supports_activation  —  Function LRP_CONFIG.supports_activation(σ) Check whether LRP can be used on a given activation function. To extend LRP to your own activation functions, define: LRP_CONFIG.supports_activation(::typeof(myactivation)) = true  # for functions\nLRP_CONFIG.supports_activation(::MyActivation) = true          # for structs source"},{"id":112,"pagetitle":"API Reference","title":"CRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#CRP","content":" CRP"},{"id":113,"pagetitle":"API Reference","title":"RelevancePropagation.CRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.CRP","content":" RelevancePropagation.CRP  —  Type CRP(lrp_analyzer, layer, features) Use Concept Relevance Propagation to explain the output of a neural network with respect to specific features in a given layer. Arguments lrp_analyzer::LRP : LRP analyzer layer::Int : Index of layer after which the concept is located features : Concept / feature to explain. See also  TopNFeatures  and  IndexedFeatures . References [1] R. Achtibat et al., From attribution maps to human-understandable explanations     through Concept Relevance Propagation source"},{"id":114,"pagetitle":"API Reference","title":"XAIBase.TopNFeatures","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.TopNFeatures","content":" XAIBase.TopNFeatures  —  Type TopNFeatures(n) Select top-n features. For outputs of convolutional layers, the relevance is summed across height and width channels for each feature. See also  IndexedFeatures . Note The XAIBase interface currently assumes that features have either 2 or 4 dimensions ( (features, batchsize)  or  (width, height, features, batchsize) ). It also assumes that the batch dimension is the last dimension of the feature. Example julia> feature_selector = TopNFeatures(2)\n TopNFeatures(2)\n\njulia> feature = rand(3, 2)\n3×2 Matrix{Float64}:\n 0.265312  0.953689\n 0.674377  0.172154\n 0.649722  0.570809\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}}}:\n [CartesianIndices((2:2, 1:1)), CartesianIndices((1:1, 2:2))]\n [CartesianIndices((3:3, 1:1)), CartesianIndices((3:3, 2:2))]\n\njulia> feature = rand(3, 3, 3, 2);\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{4, NTuple{4, UnitRange{Int64}}}}}:\n [CartesianIndices((1:3, 1:3, 2:2, 1:1)), CartesianIndices((1:3, 1:3, 1:1, 2:2))]\n [CartesianIndices((1:3, 1:3, 1:1, 1:1)), CartesianIndices((1:3, 1:3, 3:3, 2:2))] source"},{"id":115,"pagetitle":"API Reference","title":"XAIBase.IndexedFeatures","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.IndexedFeatures","content":" XAIBase.IndexedFeatures  —  Type IndexedFeatures(indices...) Select features by indices. For outputs of convolutional layers, the index refers to a feature dimension. See also See also  TopNFeatures . Note The XAIBase interface currently assumes that features have either 2 or 4 dimensions ( (features, batchsize)  or  (width, height, features, batchsize) ). It also assumes that the batch dimension is the last dimension of the feature. Example julia> feature_selector = IndexedFeatures(2, 3)\n IndexedFeatures(2, 3)\n\njulia> feature = rand(3, 3, 3, 2);\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{4, NTuple{4, UnitRange{Int64}}}}}:\n [CartesianIndices((1:3, 1:3, 2:2, 1:1)), CartesianIndices((1:3, 1:3, 2:2, 2:2))]\n [CartesianIndices((1:3, 1:3, 3:3, 1:1)), CartesianIndices((1:3, 1:3, 3:3, 2:2))]\n\njulia> feature = rand(3, 2);\n\njulia> feature_selector(feature)\n 1-element Vector{Vector{CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}}}:\n  [CartesianIndices((2:2, 1:1)), CartesianIndices((2:2, 2:2))] source"},{"id":116,"pagetitle":"API Reference","title":"Index","ref":"/XAIDocs/RelevancePropagation/stable/api/#Index","content":" Index RelevancePropagation.AlphaBetaRule RelevancePropagation.CRP RelevancePropagation.ChainTuple RelevancePropagation.Composite RelevancePropagation.ConvLayer RelevancePropagation.DropoutLayer RelevancePropagation.EpsilonRule RelevancePropagation.FirstLayerMap RelevancePropagation.FirstLayerTypeMap RelevancePropagation.FirstNTypeMap RelevancePropagation.FlatRule RelevancePropagation.GammaRule RelevancePropagation.GeneralizedGammaRule RelevancePropagation.GlobalMap RelevancePropagation.GlobalTypeMap RelevancePropagation.LRP RelevancePropagation.LastLayerMap RelevancePropagation.LastLayerTypeMap RelevancePropagation.LayerMap RelevancePropagation.LayerNormRule RelevancePropagation.NormalizationLayer RelevancePropagation.ParallelTuple RelevancePropagation.PassRule RelevancePropagation.PoolingLayer RelevancePropagation.RangeMap RelevancePropagation.RangeTypeMap RelevancePropagation.ReshapingLayer RelevancePropagation.SkipConnectionTuple RelevancePropagation.WSquareRule RelevancePropagation.ZBoxRule RelevancePropagation.ZPlusRule RelevancePropagation.ZeroRule XAIBase.Explanation XAIBase.IndexedFeatures XAIBase.TopNFeatures RelevancePropagation.EpsilonAlpha2Beta1 RelevancePropagation.EpsilonAlpha2Beta1Flat RelevancePropagation.EpsilonGammaBox RelevancePropagation.EpsilonPlus RelevancePropagation.EpsilonPlusFlat RelevancePropagation.LRP_CONFIG.supports_activation RelevancePropagation.LRP_CONFIG.supports_layer RelevancePropagation.canonize RelevancePropagation.flatten_model RelevancePropagation.is_compatible RelevancePropagation.lrp_rules RelevancePropagation.modify_bias RelevancePropagation.modify_denominator RelevancePropagation.modify_input RelevancePropagation.modify_layer RelevancePropagation.modify_parameters RelevancePropagation.modify_weight RelevancePropagation.show_layer_indices RelevancePropagation.strip_softmax XAIBase.analyze"},{"id":119,"pagetitle":"Developer Documentation","title":"Developer Documentation","ref":"/XAIDocs/RelevancePropagation/stable/developer/#developer","content":" Developer Documentation"},{"id":120,"pagetitle":"Developer Documentation","title":"Generic LRP rule implementation","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Generic-LRP-rule-implementation","content":" Generic LRP rule implementation Before we dive into package-specific implementation details  in later sections of this developer documentation,  we first need to cover some fundamentals of LRP, starting with our notation. The generic LRP rule, of which the  $0$ -,  $\\epsilon$ - and  $\\gamma$ -rules are special cases, reads [1] [2] \\[\\begin{equation}\nR_j^k = \\sum_i \\frac{\\rho(W_{ij}) \\; a_j^k}{\\epsilon + \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i)} R_i^{k+1}\n\\end{equation}\\] where  $W$  is the weight matrix of the layer $b$  is the bias vector of the layer $a^k$  is the activation vector at the input of layer  $k$ $a^{k+1}$  is the activation vector at the output of layer  $k$ $R^k$  is the relevance vector at the input of layer  $k$ $R^{k+1}$  is the relevance vector at the output of layer  $k$ $\\rho$  is a function that modifies parameters (what we call  modify_parameters ) $\\epsilon$  is a small positive constant to avoid division by zero Subscript characters are used to index vectors and matrices  (e.g.  $b_i$  is the  $i$ -th entry of the bias vector),  while the superscripts  $^k$  and  $^{k+1}$   indicate the relative positions of activations  $a$  and relevances  $R$  in the model. For any  $k$ ,  $a^k$  and  $R^k$  have the same shape.  Note that every term in this equation is a scalar value, which removes the need to differentiate between matrix and element-wise operations."},{"id":121,"pagetitle":"Developer Documentation","title":"Linear layers","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Linear-layers","content":" Linear layers LRP was developed for  deep rectifier networks , neural networks that are composed of linear layers with ReLU activation functions. Linear layers are layers that can be represented as affine transformations of the form  \\[\\begin{equation}\nf(x) = Wx + b \\quad .\n\\end{equation}\\] This includes most commonly used types of layers, such as fully connected layers,  convolutional layers, pooling layers, and normalization layers. We will now describe a generic implementation of equation (1)  that can be applied to any linear layer."},{"id":122,"pagetitle":"Developer Documentation","title":"The automatic differentiation fallback","ref":"/XAIDocs/RelevancePropagation/stable/developer/#fallback","content":" The automatic differentiation fallback The computation of the generic LRP rule can be decomposed into four steps [1] : \\[\\begin{array}{lr}\nz_{i} = \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i) & \\text{(Step 1)} \\\\[0.5em]\ns_{i} = R_{i}^{k+1} / (z_{i} + \\epsilon)           & \\text{(Step 2)} \\\\[0.5em]\nc_{j} = \\sum_i \\rho(W_{ij}) \\; s_{i}               & \\text{(Step 3)} \\\\[0.5em]\nR_{j}^{k} = a_{j}^{k} c_{j}                        & \\text{(Step 4)}\n\\end{array}\\] To compute step 1 , we first create a modified layer,  applying  $\\rho$  to the weights and biases  and replacing the activation function with the identity function. The vector  $z$  is then computed using a forward pass through the modified layer. It has the same dimensionality as  $R^{k+1}$  and  $a^{k+1}$ . Step 2  is an element-wise division of  $R^{k+1}$  by  $z$ . To avoid division by zero, a small constant  $\\epsilon$  is added to  $z$  when necessary. Step 3  is trivial for fully connected layers,  as  $\\rho(W)$  corresponds to the weight matrix of the modified layer. For other types of linear layers, however, the implementation is more involved: A naive approach would be to construct a large matrix  $W$  that corresponds to the affine transformation  $Wx+b$  implemented by the modified layer. This has multiple drawbacks: the implementation is error-prone a separate implementation is required for each type of linear layer for some layer types, e.g. pooling layers, the matrix  $W$  depends on the input for many layer types, e.g. convolutional layers,  the matrix  $W$  is very large and sparse, mostly consisting of zeros, leading to a large computational overhead A better approach can be found by observing that the matrix  $W$  is the Jacobian of the affine transformation  $f(x) = Wx + b$ . The vector  $c$  computed in step 3 corresponds to  $c = s^T W$ , a so-called  Vector-Jacobian-Product  (VJP) of the vector  $s$  with the Jacobian  $W$ .  VJPs are the fundamental building blocks of reverse-mode automatic differentiation (AD), and therefore implemented by most AD frameworks in a highly performant, matrix-free, GPU-accelerated manner. Note that computing the VJP is much more efficient than first computing the full Jacobian  $W$  and later multiplying it with  $s$ .  This is due to the fact that computing the full Jacobian of a function   $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$  requires computing  $m$  VJPs. Functions that compute VJP's are commonly called  pullbacks . Using the  Zygote.jl  AD system, we obtain the output  $z$  of a modified layer and its pullback  back  in a single function call: z, back = pullback(modified_layer, aᵏ) We then call the pullback with the vector  $s$  to obtain  $c$ : c = back(s) Finally, step 4  consists of an element-wise multiplication of the vector  $c$   with the input activation vector  $a^k$ , resulting in the relevance vector  $R^k$ . This AD-based implementation is used in RelevancePropagation.jl as the default method for all layer types that don't have a more optimized implementation (e.g. fully connected layers). We will refer to it as the  \"AD fallback\" . For more background information on automatic differentiation, refer to the   JuML lecture on AD ."},{"id":123,"pagetitle":"Developer Documentation","title":"LRP analyzer struct","ref":"/XAIDocs/RelevancePropagation/stable/developer/#LRP-analyzer-struct","content":" LRP analyzer struct The  LRP  analyzer struct holds three fields: the  model  to analyze, the LRP  rules  to use, and pre-allocated  modified_layers . As described in the section on  Composites , applying a composite to a model will return LRP rules in nested  ChainTuple ,  ParallelTuple  and  SkipConnectionTuple s. These wrapper types are used to match the structure of Flux models with  Chain ,   Parallel  and  SkipConnection  layers while avoiding type piracy. When creating an  LRP  analyzer with the default keyword argument  flatten=true ,   flatten_model  is called on the model and rules. This is done for performance reasons, as discussed in   Flattening the model . After passing the  Model checks , modified layers are pre-allocated, once again using the  ChainTuple ,  ParallelTuple   and  SkipConnectionTuple  wrapper types to match the structure of the model. If a rule doesn't modify a layer,  the corresponding entry in  modified_layers  is set to  nothing ,  avoiding unnecessary allocations.  If a rule requires multiple modified layers,  the corresponding entry in  modified_layers  is set to a named tuple of modified layers. Apart from these special cases,  the corresponding entry in  modified_layers  is simply set to the modified layer. For a detailed description of the layer modification mechanism, refer to the section on  Advanced layer modification ."},{"id":124,"pagetitle":"Developer Documentation","title":"Forward and reverse pass","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Forward-and-reverse-pass","content":" Forward and reverse pass When calling an  LRP  analyzer, a forward pass through the model is performed, saving the activations  $aᵏ$  for all layers  $k$  in a vector called  as . This vector of activations is then used to pre-allocate the relevances  $R^k$   for all layers in a vector called  Rs . This is possible since for any layer  $k$ ,  $a^k$  and  $R^k$  have the same shape. Finally, the last array of relevances  $R^N$  in  Rs  is set to zeros,  except for the specified output neuron, which is set to one. We can now run the reverse pass, iterating backwards over the layers in the model and writing relevances  $R^k$  into the pre-allocated array  Rs : for k in length(model):-1:1\n    #                  └─ loop over layers in reverse\n    lrp!(Rs[k], rules[k], layers[k], modified_layers[k], as[k], Rs[k+1])\n    #    └─ Rᵏ: modified in-place                        └─ aᵏ  └─ Rᵏ⁺¹\nend This is done by calling low-level functions function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend that implement individual LRP rules. The correct rule is applied via   multiple dispatch  on the types of the arguments  rule  and  modified_layer . The relevance  Rᵏ  is then computed based on the input activation  aᵏ  and the output relevance  Rᵏ⁺¹ . The exclamation point in the function name  lrp!  is a   naming convention  in Julia to denote functions that modify their arguments –  in this case the first argument  Rs[k] , which corresponds to  $R^k$ ."},{"id":125,"pagetitle":"Developer Documentation","title":"Rule calls","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Rule-calls","content":" Rule calls As discussed in  The AD fallback , the default LRP fallback for unknown layers uses AD via   Zygote . Now that you are familiar with both the API and the four-step computation of the generic LRP rules, the following implementation should be straightforward to understand: function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z, back = pullback(modified_layer, ãᵏ)\n   s = Rᵏ⁺¹ ./ modify_denominator(rule, z)\n   Rᵏ .= ãᵏ .* only(back(s))\nend Not only  lrp!  dispatches on the rule and layer type,  but also the internal functions  modify_input  and  modify_denominator . Unknown layers that are registered in the  LRP_CONFIG  use this exact function. All LRP rules are implemented in the file  /src/rules.jl ."},{"id":126,"pagetitle":"Developer Documentation","title":"Specialized implementations","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Specialized-implementations","content":" Specialized implementations In other programming languages, LRP is commonly implemented in an object-oriented manner, providing a single backward pass implementation per rule. This can be seen as a form of  single dispatch  on the rule type. Using multiple dispatch, we can implement specialized versions of  lrp!  that not only take into account the rule type, but also the layer type,  for example for fully connected layers or reshaping layers.  Reshaping layers don't affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back: function lrp!(Rᵏ, rule, layer::ReshapingLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= reshape(Rᵏ⁺¹, size(aᵏ))\nend We can even provide a specialized implementation of the generic LRP rule for  Dense  layers. Since we can access the weight matrix directly, we can skip the use of automatic differentiation and implement the following equation directly, using Einstein summation notation: \\[R_j^k = \\sum_i \\frac{\\rho(W_{ij}) \\; a_j^k}{\\epsilon + \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i)} R_i^{k+1}\\] function lrp!(Rᵏ, rule, layer::Dense, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z = modify_denominator(rule, layer(ãᵏ))\n\n   # Implement LRP using Einsum notation, where `b` is the batch index\n   @tullio Rᵏ[j, b] = layer.weight[i, j] * ãᵏ[j, b] / z[i, b] * Rᵏ⁺¹[i, b]\nend For maximum low-level control beyond  modify_input  and  modify_denominator , you can also implement your own  lrp!  function and dispatch on individual rule types  MyRule  and layer types  MyLayer : function lrp!(Rᵏ, rule::MyRule, layer::MyLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend 1 G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview 2 W. Samek et al.,  Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications"},{"id":129,"pagetitle":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Creating-an-LRP-Analyzer","content":" Creating an LRP Analyzer Note This package is part the  Julia-XAI ecosystem . For an introduction to the ecosystem, please refer to the  Getting started  guide . We start out by loading a small convolutional neural network: using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16; pad=1),\n        BatchNorm(16, relu),\n        Conv((3, 3), 16 => 8, relu; pad=1),\n        BatchNorm(8, relu),\n    ),\n    Chain(Flux.flatten, Dense(2048 => 512, relu), Dropout(0.5), Dense(512 => 100, softmax)),\n); This model contains two chains: the convolutional layers and the fully connected layers."},{"id":130,"pagetitle":"Creating an LRP Analyzer","title":"Model preparation","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Model-preparation","content":" Model preparation TLDR Use  strip_softmax  to strip the output softmax from your model. Otherwise  model checks  will fail. Use  canonize  to fuse linear layers. Don't just call  LRP(model) , instead use a  Composite  to apply LRP rules to your model. Read  Assigning rules to layers  for more information. By default,  LRP  will call  flatten_model  to flatten your model. This reduces computational overhead."},{"id":131,"pagetitle":"Creating an LRP Analyzer","title":"Stripping the output softmax","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Stripping-the-output-softmax","content":" Stripping the output softmax When using LRP, it is recommended to explain output logits instead of probabilities. This can be done by stripping the output softmax activation from the model using the  strip_softmax  function: model = strip_softmax(model) Chain(\n  Chain(\n    Conv((3, 3), 3 => 8, relu, pad=1),   # 224 parameters \n    Conv((3, 3), 8 => 8, relu, pad=1),   # 584 parameters \n    MaxPool((2, 2)),\n    Conv((3, 3), 8 => 16, pad=1),        # 1_168 parameters \n    BatchNorm(16, relu),                 # 32 parameters, plus 32 \n    Conv((3, 3), 16 => 8, relu, pad=1),   # 1_160 parameters \n    BatchNorm(8, relu),                  # 16 parameters, plus 16 \n  ),\n  Chain(\n    Flux.flatten,\n    Dense(2048 => 512, relu),            # 1_049_088 parameters \n    Dropout(0.5),\n    Dense(512 => 100),                   # 51_300 parameters \n  ),\n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.211 MiB. If you don't remove the output softmax,  model checks  will fail."},{"id":132,"pagetitle":"Creating an LRP Analyzer","title":"Model canonization","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#canonization","content":" Model canonization LRP is not invariant to a model's implementation. Applying the  GammaRule  to two linear layers in a row will yield different results than first fusing the two layers into one linear layer and then applying the rule. This fusing is called \"canonization\" and can be done using the  canonize  function: model_canonized = canonize(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),    # 1_168 parameters \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 14 trainable arrays,  1_103_540 parameters,\n           # plus 2 non-trainable, 16 parameters, summarysize  4.211 MiB. After canonization, the first  BatchNorm  layer has been fused into the preceding  Conv  layer. The second  BatchNorm  layer wasn't fused since its preceding  Conv  layer has a ReLU activation function."},{"id":133,"pagetitle":"Creating an LRP Analyzer","title":"Flattening the model","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#flatten-model","content":" Flattening the model RelevancePropagation.jl's LRP implementation supports nested Flux Chains and Parallel layers. However, it is recommended to flatten the model before analyzing it. LRP is implemented by first running a forward pass through the model, keeping track of the intermediate activations, followed by a backward pass that computes the relevances. To keep the LRP implementation simple and maintainable, RelevancePropagation.jl does not pre-compute \"nested\" activations. Instead, for every internal chain, a new forward pass is run to compute activations. By \"flattening\" a model, this overhead can be avoided. For this purpose, RelevancePropagation.jl provides the function  flatten_model : model_flat = flatten_model(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),          # 1_168 parameters \n  BatchNorm(16, relu),                   # 32 parameters, plus 32 \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.211 MiB. This function is called by default when creating an LRP analyzer. Note that we pass the unflattened model to the analyzer, but  analyzer.model  is flattened: analyzer = LRP(model)\nanalyzer.model Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),          # 1_168 parameters \n  BatchNorm(16, relu),                   # 32 parameters, plus 32 \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.211 MiB. If this flattening is not desired, it can be disabled by passing the keyword argument  flatten=false  to the  LRP  constructor."},{"id":134,"pagetitle":"Creating an LRP Analyzer","title":"LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#LRP-rules","content":" LRP rules The following examples will be run on a pre-trained LeNet-5 model: using BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB. We also load the MNIST dataset: using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x) By default, the  LRP  constructor will assign the  ZeroRule  to all layers. analyzer = LRP(model) LRP(\n  Conv((5, 5), 1 => 6, relu)   =>  ZeroRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Conv((5, 5), 6 => 16, relu)  =>  ZeroRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Flux.flatten                 =>  ZeroRule() ,\n  Dense(256 => 120, relu)      =>  ZeroRule() ,\n  Dense(120 => 84, relu)       =>  ZeroRule() ,\n  Dense(84 => 10)              =>  ZeroRule() ,\n) This ana lyzer will return heatmaps that look identical to the  InputTimesGradient  analyzer from  ExplainableAI.jl . We can visualize  Explanation s by computing a  heatmap  using either  VisionHeatmaps.jl  or  TextHeatmaps.jl , either for images or text, respectively. using VisionHeatmaps\n\nheatmap(input, analyzer) LRP's strength lies in assigning different rules to different layers, based on their functionality in the neural network [1] . RelevancePropagation.jl  implements many LRP rules out of the box , but it is also possible to  implement custom rules . To assign different rules to different layers, use one of the  composites presets , or create your own composite, as described in  Assigning rules to layers . composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat Composite(\n  GlobalTypeMap(   # all layers \n     Flux.Conv                =>  ZPlusRule() ,\n     Flux.ConvTranspose       =>  ZPlusRule() ,\n     Flux.CrossCor            =>  ZPlusRule() ,\n     Flux.Dense               =>  EpsilonRule{Float32}(1.0f-6) ,\n     Flux.Scale               =>  EpsilonRule{Float32}(1.0f-6) ,\n     Flux.LayerNorm           =>  LayerNormRule() ,\n     typeof(NNlib.dropout)    =>  PassRule() ,\n     Flux.AlphaDropout        =>  PassRule() ,\n     Flux.Dropout             =>  PassRule() ,\n     Flux.BatchNorm           =>  PassRule() ,\n     typeof(Flux.flatten)     =>  PassRule() ,\n     typeof(MLUtils.flatten)  =>  PassRule() ,\n     typeof(identity)         =>  PassRule() ,\n ),\n  FirstLayerTypeMap(   # first layer \n     Flux.Conv           =>  FlatRule() ,\n     Flux.ConvTranspose  =>  FlatRule() ,\n     Flux.CrossCor       =>  FlatRule() ,\n ),\n) analyzer = LRP(model, composite) LRP(\n  Conv((5, 5), 1 => 6, relu)   =>  FlatRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Conv((5, 5), 6 => 16, relu)  =>  ZPlusRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Flux.flatten                 =>  PassRule() ,\n  Dense(256 => 120, relu)      =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(120 => 84, relu)       =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(84 => 10)              =>  EpsilonRule{Float32}(1.0f-6) ,\n) heatmap(input, analyzer)"},{"id":135,"pagetitle":"Creating an LRP Analyzer","title":"Computing layerwise relevances","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Computing-layerwise-relevances","content":" Computing layerwise relevances If you are interested in computing layerwise relevances, call  analyze  with an LRP analyzer and the keyword argument  layerwise_relevances=true . The layerwise relevances can be accessed in the  extras  field of the returned  Explanation : expl = analyze(input, analyzer; layerwise_relevances=true)\nexpl.extras.layerwise_relevances (Float32[-1.5046089f-6 -1.5046089f-6 … 4.4148962f-8 0.0; -1.5046089f-6 -1.5046089f-6 … 4.4148962f-8 0.0; … ; 6.1168203f-6 6.1168203f-6 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], Float32[-3.7615224f-5 0.0 … 1.103724f-6 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0001529205 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], Float32[-3.7615224f-5 0.00021888175 … 0.000114222385 1.103724f-6; 0.00018855373 0.00027439542 … 0.00020195934 -3.4702516f-5; … ; -2.6229336f-5 7.008412f-5 … -2.8691686f-6 0.0; 0.0001529205 0.00029437395 … 0.0 0.0;;; 0.0 0.0 … -0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 0.0;;; -0.0 0.0 … -0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -0.0 0.0 … 0.0 0.0;;; -0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; -0.0 0.0 … 0.0 0.0; 0.0 -0.0 … 0.0 0.0;;; 0.0 -0.0 … -0.0 0.0; -0.0 0.0 … 0.0 0.0; … ; 0.0 -0.0 … -0.0 0.0; 0.0 0.0 … -0.0 0.0;;; 0.0 -0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0; 0.0 0.0 … 0.0 0.0;;;;], Float32[0.0 0.0 … 0.0 0.0; -0.0027488603 0.0 … 0.02671153 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.03954778 0.0 … 0.0 -0.0014172087;;; -0.0014038438 0.0 … 0.0 -0.00047513167; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.004709776 … 0.0 0.0;;; 0.0 0.0 … 0.0 -0.002364168; 0.0 0.0 … 0.0 0.0; … ; 0.00063266495 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 -0.014013007; 0.0 0.014055459 … 0.0 0.0; … ; 0.0 0.0 … -0.00013713303 0.0; 0.037856653 0.0 … 0.0 0.0;;; -0.0033661663 0.0 … 0.008260983 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.01444692 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.038602687; … ; 0.0 0.0014011612 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], Float32[-0.0027488603 -0.0 0.0006903657 0.02671153; -0.0043666936 -0.01234732 0.0 -0.013395859; -0.010977116 0.04276983 -0.0 -9.9327546f-5; 0.03954778 0.0 -0.0 -0.0014172087;;; -0.0014038438 0.017887121 0.0 -0.0004751317; -0.0 0.016042996 -0.0 -0.0; 0.011004027 0.0 -0.0 -0.0; 0.004709776 -0.009777178 -0.0 0.0;;; -0.0 -0.025314394 -0.011204087 -0.002364168; 0.027246127 -0.016887693 0.0 -0.010472963; 0.00095062767 -0.00031839436 -0.022801049 0.0036320935; 0.00063266495 -0.0028789637 0.011017141 0.0;;; … ;;; 0.014055459 0.011828159 -0.0 -0.014013007; 0.01990776 0.017337693 -0.0001569362 -0.0; -0.011918854 -0.00459134 0.0 -0.004262696; 0.037856653 0.023232974 0.0 -0.00013713304;;; -0.0033661663 -0.0 0.01047569 0.008260983; -0.0 0.010837908 0.06946186 0.0038659107; 0.0 -0.0063336077 0.029801883 0.02022359; -0.0 -0.001508267 -0.0047802655 0.01444692;;; -0.0 0.060284954 0.0021190182 -0.038602687; -0.0 0.0 0.015209901 0.0; -0.037314404 0.0054231985 0.0 0.0; 0.0014011612 0.0 0.0 0.0;;;;], Float32[-0.0027488603; -0.0043666936; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; -0.0; 0.0;;], Float32[0.028250216; 0.026568355; … ; -0.0; 0.032229893;;], Float32[0.0; 0.0; … ; 0.0; 1.0;;]) Note that the layerwise relevances are only kept for layers in the outermost  Chain  of the model. Since we used a flattened model, we obtained all relevances."},{"id":136,"pagetitle":"Creating an LRP Analyzer","title":"Performance tips","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Performance-tips","content":" Performance tips"},{"id":137,"pagetitle":"Creating an LRP Analyzer","title":"Using LRP with a GPU","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Using-LRP-with-a-GPU","content":" Using LRP with a GPU All LRP analyzers support GPU backends, building on top of  Flux.jl's GPU support . Using a GPU only requires moving the input array and model weights to the GPU. For example, using  CUDA.jl : using CUDA, cuDNN\nusing Flux\nusing RelevancePropagation\n\n# move input array and model weights to GPU\ninput = input |> gpu # or gpu(input)\nmodel = model |> gpu # or gpu(model)\n\n# analyzers don't require calling `gpu`\nanalyzer = LRP(model)\n\n# explanations are computed on the GPU\nexpl = analyze(input, analyzer) Some operations, like saving, require moving explanations back to the CPU. This can be done using Flux's  cpu  function: val = expl.val |> cpu # or cpu(expl.val)\n\nusing BSON\nBSON.@save \"explanation.bson\" val"},{"id":138,"pagetitle":"Creating an LRP Analyzer","title":"Using LRP without a GPU","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Using-LRP-without-a-GPU","content":" Using LRP without a GPU Using Julia's package extension mechanism, RelevancePropagation.jl's LRP implementation can optionally make use of  Tullio.jl  and  LoopVectorization.jl  for faster LRP rules on dense layers. This only requires loading the packages before loading RelevancePropagation.jl: using LoopVectorization, Tullio\nusing RelevancePropagation This page was generated using  Literate.jl . 1 G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview"},{"id":141,"pagetitle":"Assigning Rules to Layers","title":"Assigning LRP Rules to Layers","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#composites","content":" Assigning LRP Rules to Layers In this example, we will show how to assign LRP rules to specific layers. For this purpose, we first define a small VGG-like convolutional neural network: using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16, relu; pad=1),\n        Conv((3, 3), 16 => 16, relu; pad=1),\n        MaxPool((2, 2)),\n    ),\n    Chain(Flux.flatten, Dense(1024 => 512, relu), Dropout(0.5), Dense(512 => 100, relu)),\n);"},{"id":142,"pagetitle":"Assigning Rules to Layers","title":"Manually assigning rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#composites-manual","content":" Manually assigning rules When creating an LRP-analyzer, we can assign individual rules to each layer. As we can see above, our model is a  Chain  of two Flux  Chain s. Using  flatten_model , we can flatten the model into a single  Chain : model_flat = flatten_model(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),    # 1_168 parameters \n  Conv((3, 3), 16 => 16, relu, pad=1),   # 2_320 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(1024 => 512, relu),              # 524_800 parameters \n  Dropout(0.5),\n  Dense(512 => 100, relu),               # 51_300 parameters \n)                    # Total: 12 arrays,  580_396 parameters, 2.215 MiB. This allows us to define an LRP analyzer using an array of rules matching the length of the Flux chain: rules = [\n    FlatRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    ZPlusRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    PassRule(),\n    EpsilonRule(),\n    PassRule(),\n    EpsilonRule(),\n]; The  LRP  analyzer will show a summary of how layers and rules got matched: LRP(model_flat, rules) LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n  Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n  MaxPool((2, 2))                      =>  ZeroRule() ,\n  Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n  Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n  MaxPool((2, 2))                      =>  ZeroRule() ,\n  Flux.flatten                         =>  PassRule() ,\n  Dense(1024 => 512, relu)             =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dropout(0.5)                         =>  PassRule() ,\n  Dense(512 => 100, relu)              =>  EpsilonRule{Float32}(1.0f-6) ,\n) However, this approach only works for models that can be fully flattened. For unflattened models and models containing  Parallel  and  SkipConnection  layers, we can compose rules using  ChainTuple ,  ParallelTuple  and  SkipConnectionTuple s which match the model structure: rules = ChainTuple(\n    ChainTuple(FlatRule(), ZPlusRule(), ZeroRule(), ZPlusRule(), ZPlusRule(), ZeroRule()),\n    ChainTuple(PassRule(), EpsilonRule(), PassRule(), EpsilonRule()),\n)\n\nanalyzer = LRP(model, rules; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) Keyword argument `flatten` We used the  LRP  keyword argument  flatten=false  to showcase that the structure of the model can be preserved. For performance reasons, the default  flatten=true  is recommended."},{"id":143,"pagetitle":"Assigning Rules to Layers","title":"Custom composites","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#Custom-composites","content":" Custom composites Instead of manually defining a list of rules, we can also define a  Composite . A composite constructs a list of LRP-rules by sequentially applying the  composite primitives  it contains. To obtain the same set of rules as in the previous example, we can define composite = Composite(\n    GlobalTypeMap( # the following maps of layer types to LRP rules are applied globally\n        Conv                 => ZPlusRule(),   # apply ZPlusRule on all Conv layers\n        Dense                => EpsilonRule(), # apply EpsilonRule on all Dense layers\n        Dropout              => PassRule(),    # apply PassRule on all Dropout layers\n        MaxPool              => ZeroRule(),    # apply ZeroRule on all MaxPool layers\n        typeof(Flux.flatten) => PassRule(),    # apply PassRule on all flatten layers\n    ),\n    FirstLayerMap( # the following rule is applied to the first layer\n        FlatRule(),\n    ),\n); We now construct an LRP analyzer from  composite analyzer = LRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) As you can see, this analyzer contains the same rules as our previous one. To compute rules for a model without creating an analyzer, use  lrp_rules : lrp_rules(model, composite) ChainTuple(\n  ChainTuple(\n    FlatRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    ZPlusRule(),\n    ZPlusRule(),\n    ZeroRule(),\n  ),\n  ChainTuple(\n    PassRule(),\n    EpsilonRule{Float32}(1.0f-6),\n    PassRule(),\n    EpsilonRule{Float32}(1.0f-6),\n  ),\n)\n"},{"id":144,"pagetitle":"Assigning Rules to Layers","title":"Composite primitives","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#Composite-primitives","content":" Composite primitives The following  Composite primitives  can used to construct a  Composite . To apply a single rule, use: LayerMap  to apply a rule to a layer at a given index GlobalMap  to apply a rule to all layers RangeMap  to apply a rule to a positional range of layers FirstLayerMap  to apply a rule to the first layer LastLayerMap  to apply a rule to the last layer To apply a set of rules to layers based on their type, use: GlobalTypeMap  to apply a dictionary that maps layer types to LRP-rules RangeTypeMap  for a  TypeMap  on generalized ranges FirstLayerTypeMap  for a  TypeMap  on the first layer of a model LastLayerTypeMap  for a  TypeMap  on the last layer FirstNTypeMap  for a  TypeMap  on the first  n  layers Primitives are called sequentially in the order the  Composite  was created with and overwrite rules specified by previous primitives."},{"id":145,"pagetitle":"Assigning Rules to Layers","title":"Assigning a rule to a specific layer","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#Assigning-a-rule-to-a-specific-layer","content":" Assigning a rule to a specific layer To assign a rule to a specific layer, we can use  LayerMap , which maps an LRP-rule to all layers in the model at the given index. To display indices, use the  show_layer_indices  helper function: show_layer_indices(model) ChainTuple(\n  ChainTuple(\n    (1, 1),\n    (1, 2),\n    (1, 3),\n    (1, 4),\n    (1, 5),\n    (1, 6),\n  ),\n  ChainTuple(\n    (2, 1),\n    (2, 2),\n    (2, 3),\n    (2, 4),\n  ),\n)\n Let's demonstrate  LayerMap  by assigning a specific rule to the last  Conv  layer at index  (1, 5) : composite = Composite(LayerMap((1, 5), EpsilonRule()))\n\nLRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  ZeroRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZeroRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZeroRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  ZeroRule() ,\n    Dense(1024 => 512, relu)  =>  ZeroRule() ,\n    Dropout(0.5)              =>  ZeroRule() ,\n    Dense(512 => 100, relu)   =>  ZeroRule() ,\n  ),\n) This approach also works with  Parallel  layers."},{"id":146,"pagetitle":"Assigning Rules to Layers","title":"Composite presets","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#composites-presets","content":" Composite presets RelevancePropagation.jl provides a set of default composites. A list of all implemented default composites can be found  in the API reference , e.g. the  EpsilonPlusFlat  composite: composite = EpsilonPlusFlat() Composite(\n  GlobalTypeMap(   # all layers \n     Flux.Conv                =>  ZPlusRule() ,\n     Flux.ConvTranspose       =>  ZPlusRule() ,\n     Flux.CrossCor            =>  ZPlusRule() ,\n     Flux.Dense               =>  EpsilonRule{Float32}(1.0f-6) ,\n     Flux.Scale               =>  EpsilonRule{Float32}(1.0f-6) ,\n     Flux.LayerNorm           =>  LayerNormRule() ,\n     typeof(NNlib.dropout)    =>  PassRule() ,\n     Flux.AlphaDropout        =>  PassRule() ,\n     Flux.Dropout             =>  PassRule() ,\n     Flux.BatchNorm           =>  PassRule() ,\n     typeof(Flux.flatten)     =>  PassRule() ,\n     typeof(MLUtils.flatten)  =>  PassRule() ,\n     typeof(identity)         =>  PassRule() ,\n ),\n  FirstLayerTypeMap(   # first layer \n     Flux.Conv           =>  FlatRule() ,\n     Flux.ConvTranspose  =>  FlatRule() ,\n     Flux.CrossCor       =>  FlatRule() ,\n ),\n) analyzer = LRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) This page was generated using  Literate.jl ."},{"id":149,"pagetitle":"Concept Relevance Propagation","title":"Concept Relevance Propagation","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Concept-Relevance-Propagation","content":" Concept Relevance Propagation In  From attribution maps to human-understandable explanations through Concept Relevance Propagation  (CRP), Achtibat et al. propose the conditioning of LRP relevances on individual features of a model. Note This package is part the  Julia-XAI ecosystem  and builds on the basics shown in the  Getting started  guide . We start out by loading the same pre-trained LeNet5 model and MNIST input data: using RelevancePropagation\nusing Flux\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)"},{"id":150,"pagetitle":"Concept Relevance Propagation","title":"Step 1: Create LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-1:-Create-LRP-analyzer","content":" Step 1: Create LRP analyzer To create a CRP analyzer, first define an LRP analyzer with your desired rules: composite = EpsilonPlusFlat()\nlrp_analyzer = LRP(model, composite) LRP(\n  Conv((5, 5), 1 => 6, relu)   =>  FlatRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Conv((5, 5), 6 => 16, relu)  =>  ZPlusRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Flux.flatten                 =>  PassRule() ,\n  Dense(256 => 120, relu)      =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(120 => 84, relu)       =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(84 => 10)              =>  EpsilonRule{Float32}(1.0f-6) ,\n)"},{"id":151,"pagetitle":"Concept Relevance Propagation","title":"Step 2: Define concepts","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-2:-Define-concepts","content":" Step 2: Define concepts Then, specify the index of the layer on the outputs of which you want to condition the explanation. In this example, we are interested in the outputs of the last convolutional layer, layer 3: feature_layer = 3    # index of relevant layer in model\nmodel[feature_layer] # show layer Conv((5, 5), 6 => 16, relu)   # 2_416 parameters Then, specify the concepts / features you are interested in. To automatically select the  $n$  most relevant features, use  TopNFeatures . Note that for convolutional layers, a feature corresponds to an entire output channel of the layer. features = TopNFeatures(5) TopNFeatures(5) To manually specify features, use  IndexedFeatures . features = IndexedFeatures(1, 2, 10) IndexedFeatures(1, 2, 10)"},{"id":152,"pagetitle":"Concept Relevance Propagation","title":"Step 3: Use CRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-3:-Use-CRP-analyzer","content":" Step 3: Use CRP analyzer We can now create a  CRP  analyzer and use it like any other analyzer from RelevancePropagation.jl: using VisionHeatmaps\n\nanalyzer = CRP(lrp_analyzer, feature_layer, features)\nheatmap(input, analyzer) (a vector displayed as a row to save space)"},{"id":153,"pagetitle":"Concept Relevance Propagation","title":"Using CRP on input batches","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Using-CRP-on-input-batches","content":" Using CRP on input batches Note that  CRP  uses the batch dimension to return explanations. When using CRP on batches, the explanations are first sorted by features, then inputs, e.g.  [c1_i1, c1_i2, c2_i1, c2_i2, c3_i1, c3_i2]  in the following example: x, y = MNIST(Float32, :test)[10:11]\nbatch = reshape(x, 28, 28, 1, :)\n\nheatmap(batch, analyzer) (a vector displayed as a row to save space) This page was generated using  Literate.jl ."},{"id":156,"pagetitle":"Supporting New Layer Types","title":"Supporting New Layers and Activation Functions","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#custom-layers","content":" Supporting New Layers and Activation Functions One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research. This example will show you how to extent LRP to new layer types and activation functions. using Flux\nusing RelevancePropagation"},{"id":157,"pagetitle":"Supporting New Layer Types","title":"Model checks","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#model-checks","content":" Model checks To assure that novice users use LRP according to best practices, RelevancePropagation.jl runs strict model checks when creating an  LRP  analyzer. Let's demonstrate this by defining a new layer type that doubles its input struct MyDoublingLayer end\n(::MyDoublingLayer)(x) = 2 * x\n\nmylayer = MyDoublingLayer()\nmylayer([1, 2, 3]) 3-element Vector{Int64}:\n 2\n 4\n 6 and by defining a model that uses this layer: model = Chain(Dense(100, 20), MyDoublingLayer()); Creating an LRP analyzer, e.g.  LRP(model) , will throw an  ArgumentError  and print a summary of the model check in the REPL: julia> LRP(model)\n  ChainTuple(\n    Dense(100 => 20)  => supported,\n    MyDoublingLayer() => unknown layer type,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model LRP should only be used on deep rectifier networks and RelevancePropagation doesn't recognize  MyDoublingLayer  as a compatible layer by default. It will therefore return an error and a model check summary instead of returning an incorrect explanation. However, if we know  MyDoublingLayer  is compatible with deep rectifier networks, we can register it to tell RelevancePropagation that it is ok to use. This will be shown in the following section."},{"id":158,"pagetitle":"Supporting New Layer Types","title":"Registering layers","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Registering-layers","content":" Registering layers The error in the model check will stop after registering our custom layer type  MyDoublingLayer  as \"supported\" by RelevancePropagation. This is done using the function  LRP_CONFIG.supports_layer , which should be set to return  true  for the type  MyDoublingLayer : LRP_CONFIG.supports_layer(::MyDoublingLayer) = true Now we can create and run an analyzer without getting an error: analyzer = LRP(model) LRP(\n  Dense(100 => 20)                                         =>  ZeroRule() ,\n  Main.MyDoublingLayer()  =>  ZeroRule() ,\n) Registering functions Flux's  Chains  can also contain functions, e.g.  flatten . This kind of layer can be registered as LRP_CONFIG.supports_layer(::typeof(flatten)) = true"},{"id":159,"pagetitle":"Supporting New Layer Types","title":"Registering activation functions","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Registering-activation-functions","content":" Registering activation functions The mechanism for registering custom activation functions is analogous to that of custom layers: myrelu(x) = max.(0, x)\n\nmodel = Chain(Dense(784, 100, myrelu), Dense(100, 10)); Once again, creating an LRP analyzer for this model will throw an  ArgumentError  and display the following model check summary: julia> LRP(model)\n  ChainTuple(\n    Dense(784 => 100, myrelu) => unsupported or unknown activation function myrelu,\n    Dense(100 => 10)          => supported,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model Registation works by defining the function  LRP_CONFIG.supports_activation  as  true : LRP_CONFIG.supports_activation(::typeof(myrelu)) = true now the analyzer can be created without error: analyzer = LRP(model) LRP(\n  Dense(784 => 100, myrelu)  =>  ZeroRule() ,\n  Dense(100 => 10)           =>  ZeroRule() ,\n)"},{"id":160,"pagetitle":"Supporting New Layer Types","title":"Skipping model checks","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Skipping-model-checks","content":" Skipping model checks All model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument  skip_checks=true . struct UnknownLayer end\n(::UnknownLayer)(x) = x\n\nunknown_activation(x) = max.(0, x)\n\nmodel = Chain(Dense(100, 20, unknown_activation), MyDoublingLayer())\n\nLRP(model; skip_checks=true) LRP(\n  Dense(100 => 20, unknown_activation)                     =>  ZeroRule() ,\n  Main.MyDoublingLayer()  =>  ZeroRule() ,\n) Instead of throwing the usual  ERROR: Unknown layer or activation function found in model , the LRP analyzer was created without having to register either the layer  UnknownLayer  or the activation function  unknown_activation . This page was generated using  Literate.jl ."},{"id":163,"pagetitle":"Custom LRP Rules","title":"Custom LRP Rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#custom-rules","content":" Custom LRP Rules One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research. This example will show you how to implement custom LRP rules. Note This package is part the  Julia-XAI ecosystem  and builds on the basics shown in the  Getting started  guide . We start out by loading the same pre-trained LeNet-5 model and MNIST input data: using RelevancePropagation\nusing VisionHeatmaps\nusing Flux\nusing MLDatasets\nusing ImageCore\nusing BSON\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB."},{"id":164,"pagetitle":"Custom LRP Rules","title":"Implementing a custom rule","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Implementing-a-custom-rule","content":" Implementing a custom rule"},{"id":165,"pagetitle":"Custom LRP Rules","title":"Step 1: Define rule struct","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Step-1:-Define-rule-struct","content":" Step 1: Define rule struct Let's define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of supertype  AbstractLRPRule . struct MyGammaRule <: AbstractLRPRule end"},{"id":166,"pagetitle":"Custom LRP Rules","title":"Step 2: Implement rule behavior","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Step-2:-Implement-rule-behavior","content":" Step 2: Implement rule behavior It is then possible to dispatch on the following four utility functions with the rule type  MyCustomLRPRule  to define custom rules without writing boilerplate code. modify_input(rule::MyGammaRule, input) modify_parameters(rule::MyGammaRule, parameter) modify_denominator(rule::MyGammaRule, denominator) is_compatible(rule::MyGammaRule, layer) By default: modify_input  doesn't change the input modify_parameters  doesn't change the parameters modify_denominator  avoids division by zero by adding a small epsilon-term ( 1.0f-9 ) is_compatible  returns  true  if a layer has fields  weight  and  bias To extend internal functions, import them explicitly: import RelevancePropagation: modify_parameters\n\nmodify_parameters(::MyGammaRule, param) = param + 0.25f0 * relu.(param) modify_parameters (generic function with 7 methods) Note that we didn't implement three of the four functions. This is because the defaults are sufficient to implement the  GammaRule ."},{"id":167,"pagetitle":"Custom LRP Rules","title":"Step 3: Use rule in LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Step-3:-Use-rule-in-LRP-analyzer","content":" Step 3: Use rule in LRP analyzer We can directly use our rule to make an analyzer! rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    MyGammaRule(), # our custom GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\n\nheatmap(input, analyzer) # using VisionHeatmaps.jl We just implemented our own version of the  $γ$ -rule in 2 lines of code. The heatmap perfectly matches the pre-implemented  GammaRule : rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    GammaRule(), # XAI.jl's GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)"},{"id":168,"pagetitle":"Custom LRP Rules","title":"Performance tips","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Performance-tips","content":" Performance tips Make sure functions like  modify_parameters  don't promote the type of weights (e.g. from  Float32  to  Float64 ). If your rule  MyRule  doesn't modify weights or biases, defining  modify_layer(::MyRule, layer) = nothing  can provide reduce memory allocations and improve performance."},{"id":169,"pagetitle":"Custom LRP Rules","title":"Advanced layer modification","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#custom-rules-advanced","content":" Advanced layer modification For more granular control over weights and biases,  modify_weight  and  modify_bias  can be used. If the layer doesn't use weights ( layer.weight ) and biases ( layer.bias ), RelevancePropagation provides a lower-level variant of  modify_parameters  called  modify_layer . This function is expected to take a layer and return a new, modified layer. To add compatibility checks between rule and layer types, extend  is_compatible . Extending modify_layer Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ Therefore  modify_layer  should only be extended for a specific rule and a specific layer type."},{"id":170,"pagetitle":"Custom LRP Rules","title":"Advanced LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Advanced-LRP-rules","content":" Advanced LRP rules To implement custom LRP rules that require more than  modify_layer ,  modify_input  and  modify_denominator , take a look at the  LRP developer documentation . This page was generated using  Literate.jl ."},{"id":173,"pagetitle":"LRP Rule Overview","title":"LRP Rule Overview","ref":"/XAIDocs/RelevancePropagation/stable/rules/#rules","content":" LRP Rule Overview"},{"id":174,"pagetitle":"LRP Rule Overview","title":"Notation","ref":"/XAIDocs/RelevancePropagation/stable/rules/#Notation","content":" Notation We use the following notation for LRP rules:  $W$  is the weight matrix of the layer $b$  is the bias vector of the layer $a^k$  is the activation vector at the input of layer  $k$ $a^{k+1}$  is the activation vector at the output of layer  $k$ $R^k$  is the relevance vector at the input of layer  $k$ $R^{k+1}$  is the relevance vector at the output of layer  $k$ $\\rho$  is a function that modifies parameters (what we call  modify_parameters ) $\\epsilon$  is a small positive constant to avoid division by zero Subscript characters are used to index vectors and matrices  (e.g.  $b_i$  is the  $i$ -th entry of the bias vector),  while the superscripts  $^k$  and  $^{k+1}$  indicate the relative positions  of activations  $a$  and relevances  $R$  in the model. For any  $k$ ,  $a^k$  and  $R^k$  have the same shape.  Note that all terms in the following equations are scalar value, which removes the need to differentiate between matrix and element-wise operations. For more information, refer to the  developer documentation ."},{"id":175,"pagetitle":"LRP Rule Overview","title":"Basic rules","ref":"/XAIDocs/RelevancePropagation/stable/rules/#Basic-rules","content":" Basic rules"},{"id":176,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.ZeroRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.ZeroRule-rules","content":" RelevancePropagation.ZeroRule  —  Type ZeroRule() LRP- $0$  rule. Commonly used on upper layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i \\frac{W_{ij}a_j^k}{\\sum_l W_{il}a_l^k+b_i} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":177,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.EpsilonRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.EpsilonRule-rules","content":" RelevancePropagation.EpsilonRule  —  Type EpsilonRule([epsilon=1.0e-6]) LRP- $ϵ$  rule. Commonly used on middle layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}a_j^k}{\\epsilon +\\sum_{l}W_{il}a_l^k+b_i} R_i^{k+1}\\] Optional arguments epsilon : Optional stabilization parameter, defaults to  1.0e-6 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":178,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.PassRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.PassRule-rules","content":" RelevancePropagation.PassRule  —  Type PassRule() Pass-through rule. Passes relevance through to the lower layer. Supports layers with constant input and output shapes, e.g. reshaping layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = R_j^{k+1}\\] source"},{"id":179,"pagetitle":"LRP Rule Overview","title":"Lower layer rules","ref":"/XAIDocs/RelevancePropagation/stable/rules/#Lower-layer-rules","content":" Lower layer rules"},{"id":180,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.GammaRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.GammaRule-rules","content":" RelevancePropagation.GammaRule  —  Type GammaRule([gamma=0.25]) LRP- $γ$  rule. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{(W_{ij}+\\gamma W_{ij}^+)a_j^k}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_l^k+(b_i+\\gamma b_i^+)} R_i^{k+1}\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":181,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.AlphaBetaRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.AlphaBetaRule-rules","content":" RelevancePropagation.AlphaBetaRule  —  Type AlphaBetaRule([alpha=2.0, beta=1.0]) LRP- $αβ$  rule. Weights positive and negative contributions according to the parameters  alpha  and  beta  respectively. The difference  $α-β$  must be equal to one. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\left(\n    \\alpha\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+}\n    -\\beta\\frac{\\left(W_{ij}a_j^k\\right)^-}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^-}\n\\right) R_i^{k+1}\\] Optional arguments alpha : Multiplier for the positive output term, defaults to  2.0 . beta : Multiplier for the negative output term, defaults to  1.0 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":182,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.ZPlusRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.ZPlusRule-rules","content":" RelevancePropagation.ZPlusRule  —  Type ZPlusRule() LRP- $z⁺$  rule. Commonly used on lower layers. Equivalent to  AlphaBetaRule(1.0f0, 0.0f0) , but slightly faster. See also  AlphaBetaRule . Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":183,"pagetitle":"LRP Rule Overview","title":"Input layer rules","ref":"/XAIDocs/RelevancePropagation/stable/rules/#Input-layer-rules","content":" Input layer rules"},{"id":184,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.FlatRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.FlatRule-rules","content":" RelevancePropagation.FlatRule  —  Type FlatRule() LRP-Flat rule. Similar to the  WSquareRule , but with all weights set to one and all bias terms set to zero. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{1}{\\sum_l 1} R_i^{k+1} = \\sum_i\\frac{1}{n_i} R_i^{k+1}\\] where  $n_i$  is the number of input neurons connected to the output neuron at index  $i$ . References S. Lapuschkin et al.,  Unmasking Clever Hans predictors and assessing what machines really learn source"},{"id":185,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.WSquareRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.WSquareRule-rules","content":" RelevancePropagation.WSquareRule  —  Type WSquareRule() LRP- $w²$  rule. Commonly used on the first layer when values are unbounded. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}^2}{\\sum_l W_{il}^2} R_i^{k+1}\\] References G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":186,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.ZBoxRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.ZBoxRule-rules","content":" RelevancePropagation.ZBoxRule  —  Type ZBoxRule(low, high) LRP- $zᴮ$ -rule. Commonly used on the first layer for pixel input. The parameters  low  and  high  should be set to the lower and upper bounds of the input features, e.g.  0.0  and  1.0  for raw image data. It is also possible to provide two arrays of that match the input size. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k=\\sum_i \\frac{W_{ij}a_j^k - W_{ij}^{+}l_j - W_{ij}^{-}h_j}\n    {\\sum_l W_{il}a_l^k+b_i - \\left(W_{il}^{+}l_l+b_i^{+}\\right) - \\left(W_{il}^{-}h_l+b_i^{-}\\right)} R_i^{k+1}\\] References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":187,"pagetitle":"LRP Rule Overview","title":"Specialized rules","ref":"/XAIDocs/RelevancePropagation/stable/rules/#Specialized-rules","content":" Specialized rules"},{"id":188,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.LayerNormRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.LayerNormRule-rules","content":" RelevancePropagation.LayerNormRule  —  Type LayerNormRule() LRP-LN rule. Used on  LayerNorm  layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_i^k = \\sum_j\\frac{a_i^k\\left(\\delta_{ij} - 1/N\\right)}{\\sum_l a_l^k\\left(\\delta_{lj}-1/N\\right)} R_j^{k+1}\\] Relevance through the affine transformation is by default propagated using the  ZeroRule . If you would like to assign a special rule to the affine transformation inside of the  LayerNorm  layer, call  canonize  on your model. This will split the  LayerNorm  layer into a  LayerNorm  layer without affine transformation a  Scale  layer implementing the affine transformation You can then assign separate rules to these two layers. References A. Ali et al.,  XAI for Transformers: Better Explanations through Conservative Propagation source"},{"id":189,"pagetitle":"LRP Rule Overview","title":"RelevancePropagation.GeneralizedGammaRule","ref":"/XAIDocs/RelevancePropagation/stable/rules/#RelevancePropagation.GeneralizedGammaRule-rules","content":" RelevancePropagation.GeneralizedGammaRule  —  Type GeneralizedGammaRule([gamma=0.25]) Generalized LRP- $γ$  rule. Can be used on layers with  leakyrelu  activation functions. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^+)a_j^+ +(W_{ij}+\\gamma W_{ij}^-)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_j^+ +(W_{il}+\\gamma W_{il}^-)a_j^- +(b_i+\\gamma b_i^+)}\nI(z_k>0) \\cdot R^{k+1}_i\n+\\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^-)a_j^+ +(W_{ij}+\\gamma W_{ij}^+)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^-)a_j^+ +(W_{il}+\\gamma W_{il}^+)a_j^- +(b_i+\\gamma b_i^-)}\nI(z_k<0) \\cdot R^{k+1}_i\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References L. Andéol et al.,  Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization source"},{"id":192,"pagetitle":"Home","title":"VisionHeatmaps.jl","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.jl","content":" VisionHeatmaps.jl Documentation for  VisionHeatmaps.jl ."},{"id":193,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/VisionHeatmaps/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run ]add VisionHeatmaps"},{"id":194,"pagetitle":"Home","title":"API","ref":"/XAIDocs/VisionHeatmaps/stable/#API","content":" API"},{"id":195,"pagetitle":"Home","title":"VisionHeatmaps.heatmap","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.heatmap","content":" VisionHeatmaps.heatmap  —  Function heatmap(x::AbstractArray)\nheatmap(x::AbstractArray, pipeline)\nheatmap(x::AbstractArray, image)\nheatmap(x::AbstractArray, image, pipeline) Visualize 4D arrays as heatmaps, assuming the WHCN convention for input array dimensions (width, height, color channels, batch dimension). source heatmap(expl::Explanation)\nheatmap(expl::Explanation, pipeline)\nheatmap(expl::Explanation, image)   \nheatmap(expl::Explanation, image, pipeline) Visualize  Explanation  from XAIBase as a vision heatmap. Assumes WHCN convention (width, height, channels, batch dimension) for  explanation.val . This will use the default heatmapping style for the given type of explanation. source heatmap(input::AbstractArray, analyzer::AbstractXAIMethod)\nheatmap(input::AbstractArray, analyzer::AbstractXAIMethod, image) Compute an  Explanation  for a given  input  using the XAI method  analyzer  and visualize it as a vision heatmap. This will use the default heatmapping style for the given type of explanation. source"},{"id":196,"pagetitle":"Home","title":"VisionHeatmaps.Pipeline","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.Pipeline","content":" VisionHeatmaps.Pipeline  —  Type Pipeline(transforms...) Heatmapping pipelines sequentially apply transforms. source"},{"id":197,"pagetitle":"Home","title":"VisionHeatmaps.AbstractTransform","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.AbstractTransform","content":" VisionHeatmaps.AbstractTransform  —  Type AbstractTransform Abstract supertype for all heatmapping transforms.  Custom transforms must implement an  apply  method. source"},{"id":198,"pagetitle":"Home","title":"Color-channel reduction","ref":"/XAIDocs/VisionHeatmaps/stable/#api-reduction","content":" Color-channel reduction"},{"id":199,"pagetitle":"Home","title":"VisionHeatmaps.NormReduction","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.NormReduction","content":" VisionHeatmaps.NormReduction  —  Type NormReduction() Computes 2-norm over color channels source"},{"id":200,"pagetitle":"Home","title":"VisionHeatmaps.SumReduction","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.SumReduction","content":" VisionHeatmaps.SumReduction  —  Type SumReduction() Sums up color channels. source"},{"id":201,"pagetitle":"Home","title":"VisionHeatmaps.MaxAbsReduction","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.MaxAbsReduction","content":" VisionHeatmaps.MaxAbsReduction  —  Type MaxAbsReduction() Computes  maximum(abs, x)  over color channels source"},{"id":202,"pagetitle":"Home","title":"VisionHeatmaps.SumAbsReduction","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.SumAbsReduction","content":" VisionHeatmaps.SumAbsReduction  —  Type SumAbsReduction() Computes  sum(abs, x)  over color channels source"},{"id":203,"pagetitle":"Home","title":"VisionHeatmaps.AbsSumReduction","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.AbsSumReduction","content":" VisionHeatmaps.AbsSumReduction  —  Type AbsSumReduction() Computes  abs(sum(x))  the color channels source"},{"id":204,"pagetitle":"Home","title":"Manipulating array dimensions","ref":"/XAIDocs/VisionHeatmaps/stable/#Manipulating-array-dimensions","content":" Manipulating array dimensions"},{"id":205,"pagetitle":"Home","title":"VisionHeatmaps.FlipImage","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.FlipImage","content":" VisionHeatmaps.FlipImage  —  Type FlipImage() Permutes the width and height dimensions of an array. Assumes width and height are the leading directions in the array. source"},{"id":206,"pagetitle":"Home","title":"VisionHeatmaps.PermuteDims","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.PermuteDims","content":" VisionHeatmaps.PermuteDims  —  Type PermuteDims(dims...) Permutes arrays according to the specified dimensions. source"},{"id":207,"pagetitle":"Home","title":"VisionHeatmaps.DropDims","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.DropDims","content":" VisionHeatmaps.DropDims  —  Type DropDims(dims...) Drops specified singleton array dimensions. source"},{"id":208,"pagetitle":"Home","title":"Outlier removal","ref":"/XAIDocs/VisionHeatmaps/stable/#Outlier-removal","content":" Outlier removal"},{"id":209,"pagetitle":"Home","title":"VisionHeatmaps.PercentileClip","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.PercentileClip","content":" VisionHeatmaps.PercentileClip  —  Type PercentileClip()\nPercentileClip(lower, upper) Clip values  outside of the specified percentiles of values. Bounds default to  0.001  and  0.999  (99.9% percentiles). source"},{"id":210,"pagetitle":"Home","title":"Colormaps","ref":"/XAIDocs/VisionHeatmaps/stable/#Colormaps","content":" Colormaps Turn numerical arrays to images by applying color schemes:"},{"id":211,"pagetitle":"Home","title":"VisionHeatmaps.ExtremaColormap","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.ExtremaColormap","content":" VisionHeatmaps.ExtremaColormap  —  Type ExtremaColormap(name::Symbol)\nExtremaColormap(name::Symbol, colorscheme) Apply a sequential  colorscheme  from ColorSchemes.jl, turning an array of values into an image. Defaults to  :batlow . source"},{"id":212,"pagetitle":"Home","title":"VisionHeatmaps.CenteredColormap","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.CenteredColormap","content":" VisionHeatmaps.CenteredColormap  —  Type CenteredColormap(name::Symbol)\nCenteredColormap(name::Symbol, colorscheme) Apply a divergent  colorscheme  from ColorSchemes.jl, turning an array of values into an image. Defaults to  :berlin . source"},{"id":213,"pagetitle":"Home","title":"Resizing","ref":"/XAIDocs/VisionHeatmaps/stable/#Resizing","content":" Resizing"},{"id":214,"pagetitle":"Home","title":"VisionHeatmaps.ResizeToImage","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.ResizeToImage","content":" VisionHeatmaps.ResizeToImage  —  Type ResizeToImage()\nResizeToImage(method) Resize the heatmap to match the image dimensions. In some cases, this is needed for overlays. Defaults to  Lanczos(1)  from Interpolations.jl. source"},{"id":215,"pagetitle":"Home","title":"Image overlays","ref":"/XAIDocs/VisionHeatmaps/stable/#Image-overlays","content":" Image overlays"},{"id":216,"pagetitle":"Home","title":"VisionHeatmaps.AlphaOverlay","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.AlphaOverlay","content":" VisionHeatmaps.AlphaOverlay  —  Type AlphaOverlay()\nAlphaOverlay(alpha) Overlays a heatmap on top of an image. The opacity  alpha  of the heatmap defaults to  0.6 . source"},{"id":219,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Getting-started","content":" Getting started Let's assume you took the following image  img , reshaped it to WHCN format  (width, height, color channels, batch dimension)  and ran it through a vision model: using Images\nimg = load(joinpath(asset_dir, \"img1.png\")) # load image file You might use an input space attribution method  (for example from  ExplainableAI.jl ) to determine which parts of the input contributed most to the \"saxophone\" class. Let's load such an attribution  x  in WHCN format: x = load(data_heatmap, \"x\") # load precomputed array from file\ntypeof(x) Array{Float32, 4} size(x) (224, 224, 3, 1) To make this attribution more interpretable, we can visualize it as a heatmap: using VisionHeatmaps\nheatmap(x) (a vector displayed as a row to save space) By default, to support batched explanations, a vector of heatmaps is returned. Since our following examples don't use batches, we will use the  only  function to unpack singleton heatmaps: using VisionHeatmaps\nheatmap(x) |> only"},{"id":220,"pagetitle":"Getting started","title":"Custom heatmapping pipelines","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Custom-heatmapping-pipelines","content":" Custom heatmapping pipelines VisionHeatmaps internally applies a sequence of image transformations in what we call a  Pipeline . The default pipeline corresponds to:  pipe = NormReduction() |> ExtremaColormap() |> FlipImage() Pipeline(\n  NormReduction(),\n  ExtremaColormap(:batlow),\n  FlipImage(),\n) We can apply this pipeline by passing it to  heatmap : heatmap(x, pipe) |> only In the following subsection, we will explain and modify this pipeline step by step."},{"id":221,"pagetitle":"Getting started","title":"Color channel reduction","ref":"/XAIDocs/VisionHeatmaps/stable/example/#docs-heatmap-reduce","content":" Color channel reduction For arrays with multiple color channels, the channels need to be reduced to a single scalar value for each pixel, which is later mapped onto a color scheme. Several transformats are available  for this purpose. Let's compare the two most commonly used ones.  NormReduction  reduces color channels in the array by taking their norm, whereas  SumReduction  takes the sum: pipe = NormReduction() |> ExtremaColormap() |> FlipImage()\nheatmap(x, pipe) |> only pipe = SumReduction() |> ExtremaColormap() |> FlipImage()\nheatmap(x, pipe) |> only"},{"id":222,"pagetitle":"Getting started","title":"Colormaps","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Colormaps","content":" Colormaps To map the now  color-channel-reduced  array onto a color scheme, we first need to normalize all values to the range  $[0, 1]$ . For this purpose, two colormapping transforms are available: ExtremaColormap : normalizes colorscheme to the minimum and maximum value in the array. CenteredColormap : normalizes colorscheme to the maximum absolute value of the array. Values of zero will be mapped to the center of the color scheme. Since  NormReduction  only yields positive values, it is well suited for  ExtremaColormap .  SumReduction  on the other hand can yield positive and negative values. If zero-values are meaningful, using a divergent color scheme with  CenteredColormap  can be the right choice: pipe = NormReduction() |> ExtremaColormap() |> FlipImage()\nheatmap(x, pipe) |> only pipe = SumReduction() |> CenteredColormap() |> FlipImage()\nheatmap(x, pipe) |> only"},{"id":223,"pagetitle":"Getting started","title":"Outlier removal","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Outlier-removal","content":" Outlier removal While this isn't part of the default heatmapping pipelines, previous heatmaps visibly emphasized three \"dots\" on the neck of the saxophone. Very high values in explanations tend to desaturate colors. For this purpose, we provide the adaptive  PercentileClip . By default, it clips the 0.1-th and 99.9-th percentiles of values. pipe = SumReduction() |> PercentileClip() |> CenteredColormap() |> FlipImage()\nheatmap(x, pipe) |> only"},{"id":224,"pagetitle":"Getting started","title":"Custom color schemes","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Custom-color-schemes","content":" Custom color schemes We can use a custom color scheme from  ColorSchemes.jl  in our colormap: using ColorSchemes\npipe = NormReduction() |> ExtremaColormap(:jet) |> FlipImage()\nheatmap(x, pipe) |> only pipe = NormReduction() |> ExtremaColormap(:viridis) |> FlipImage()\nheatmap(x, pipe) |> only We strongly suggest to only use sequential color schemes with  ExtremaColormap   and divergent color schemes with  CenteredColormap . ColorSchemes.jl catalogue Refer to the  ColorSchemes.jl catalogue  for a gallery of available color schemes."},{"id":225,"pagetitle":"Getting started","title":"Overlays","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Overlays","content":" Overlays Singleton heatmaps can be overlaid on top of the original image. This can be used to recreate CAM-like heatmaps (usually in combination with  ResizeToImage ): pipe = NormReduction() |> PercentileClip() |> ExtremaColormap(:jet) |> FlipImage() |> AlphaOverlay()\nheatmap(x, img, pipe) |> only"},{"id":226,"pagetitle":"Getting started","title":"Heatmapping batches","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Heatmapping-batches","content":" Heatmapping batches heatmap  can also be used to visualize input batches. Let's assume we computed an input space attribution  batch  for the following images. imgs = [load(joinpath(asset_dir, f)) for f in (\"img1.png\", \"img2.png\", \"img3.png\", \"img4.png\", \"img5.png\")] # load image files (a vector displayed as a row to save space) Once again, we assume that  batch  is in WHCN format: batch = load(data_heatmaps, \"x\") # load precomputed array from file\ntypeof(batch) Array{Float32, 4} size(batch) (224, 224, 3, 5) Calling  heatmap  will automatically return an vector of images: heatmap(batch) (a vector displayed as a row to save space) These heatmaps can be customized as usual: pipe = SumReduction() |> CenteredColormap() |> FlipImage()\nheatmap(batch, pipe) (a vector displayed as a row to save space)"},{"id":229,"pagetitle":"Home","title":"TextHeatmaps.jl","ref":"/XAIDocs/TextHeatmaps/stable/#TextHeatmaps.jl","content":" TextHeatmaps.jl Documentation for  TextHeatmaps.jl ."},{"id":230,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/TextHeatmaps/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run ]add TextHeatmaps"},{"id":231,"pagetitle":"Home","title":"API","ref":"/XAIDocs/TextHeatmaps/stable/#API","content":" API"},{"id":232,"pagetitle":"Home","title":"TextHeatmaps.heatmap","ref":"/XAIDocs/TextHeatmaps/stable/#TextHeatmaps.heatmap","content":" TextHeatmaps.heatmap  —  Function heatmap(values::AbstractArray, words) Create a heatmap of words where the background color of each word is determined by its corresponding value. Arguments  values  and  words  (and optionally  colors ) must have the same size. Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  ColorSchemes.seismic . rangescale::Symbol : selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered  for use with the default color scheme  seismic . source heatmap(expl::Explanation, text) Visualize  Explanation  from XAIBase as text heatmap. Text should be a vector containing vectors of strings, one for each input in the batched explanation. This will use the default heatmapping style for the given type of explanation. Defaults can be overridden via keyword arguments. source"},{"id":235,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/TextHeatmaps/stable/example/#Getting-started","content":" Getting started Let's assume you put the following text into a sentiment analysis model: text = \"I loved the concert but not the opening act\"\nwords = split(text) 9-element Vector{SubString{String}}:\n \"I\"\n \"loved\"\n \"the\"\n \"concert\"\n \"but\"\n \"not\"\n \"the\"\n \"opening\"\n \"act\" The model returns a vector of sentiment scores for each word,  where positive values indicate positive sentiment  and negative values indicate negative sentiment: val = [0.1, 2.5, 0.0, 0.3, -0.6, -1.4, 0.0, 0.1, -0.1] To visualize the sentiment scores, we can use the  heatmap  function: using TextHeatmaps\n\nheatmap(val, words) I loved the concert but not the opening act"},{"id":236,"pagetitle":"Getting started","title":"Color schemes","ref":"/XAIDocs/TextHeatmaps/stable/example/#Color-schemes","content":" Color schemes We can use a custom color scheme from ColorSchemes.jl using the keyword argument cs: using ColorSchemes\nheatmap(val, words; colorscheme=ColorSchemes.jet) I loved the concert but not the opening act heatmap(val, words; colorscheme=ColorSchemes.inferno) I loved the concert but not the opening act"},{"id":237,"pagetitle":"Getting started","title":"Mapping values onto the color scheme","ref":"/XAIDocs/TextHeatmaps/stable/example/#Mapping-values-onto-the-color-scheme","content":" Mapping values onto the color scheme To map values onto a color scheme, we first need to normalize all values to the range  $[0,1]$ . For this purpose, two presets are available through the  rangescale  keyword argument: :extrema : normalize to the minimum and maximum value of the explanation :centered : normalize to the maximum absolute value of the explanation.    Values of zero will be mapped to the center of the color scheme. Depending on the color scheme, one of these presets may be more suitable than the other. The default color scheme,  seismic , is centered around zero, making  :centered  a good choice: heatmap(val, words; rangescale=:centered) I loved the concert but not the opening act With the  seismic  colorscheme, the  :extrema  rangescale should be avoided: Even though the word \"concert\" has a positive sentiment score of  0.3 , it is colored in blue: heatmap(val, words; rangescale=:extrema) I loved the concert but not the opening act However, for the  inferno  color scheme, which is not centered around zero,  :extrema  leads to a heatmap with higher contrast. heatmap(val, words; colorscheme=ColorSchemes.inferno, rangescale=:centered) I loved the concert but not the opening act heatmap(val, words; colorscheme=ColorSchemes.inferno, rangescale=:extrema) I loved the concert but not the opening act"},{"id":238,"pagetitle":"Getting started","title":"Terminal support","ref":"/XAIDocs/TextHeatmaps/stable/example/#Terminal-support","content":" Terminal support In the context of this documentation page and notebooks, heatmaps are rendered using HTML. TextHeatmaps.jl also supports rendering heatmaps in the terminal. Here we use the  print  function to force Documenter.jl to render the heatmap as raw text: heatmap(val, words) |> print I   loved   the   concert   but   not   the   opening   act"},{"id":241,"pagetitle":"XAIBase Interface","title":"Interface description","ref":"/XAIDocs/XAIBase/stable/#docs-interface","content":" Interface description XAIBase.jl is a light-weight dependency that defines the interface of XAI methods  in the  Julia-XAI ecosystem . Building on top of XAIBase  (or providing an interface via  package extensions ) makes your package compatible with the Julia-XAI ecosystem, allowing you to automatically compute heatmaps for vision and language models using  VisionHeatmaps.jl  and  TextHeatmaps.jl . This only requires you to fulfill the following two requirements: An XAI method has to be a subtype of  AbstractXAIMethod An XAI algorithm has to implement a  call_analyzer  method:  import XAIBase: call_analyzer\n\ncall_analyzer(input, method::MyMethod, output_selector::AbstractOutputSelector; kwargs...) call_analyzer  has to return an  Explanation The input is expected to have a batch dimensions as its last dimension When applied to a batch, the method returns a single  Explanation ,  which contains the batched output in the  val  field. AbstractOutputSelector s are predefined callable structs  that select a single scalar value from a model's output,  e.g. the maximally activated output of a classifier using  MaxActivationSelector  or a specific output using  IndexSelector . Refer to the  Explanation  documentation for a description of the expected fields. For more information, take a look at  src/XAIBase.jl ."},{"id":242,"pagetitle":"XAIBase Interface","title":"Implementation template","ref":"/XAIDocs/XAIBase/stable/#Implementation-template","content":" Implementation template Julia-XAI methods will usually follow the following template: using XAIBase\nimport XAIBase: call_analyzer\n\nstruct MyMethod{M} <: AbstractXAIMethod \n    model::M    \nend\n\nfunction call_analyzer(input, method::MyMethod, output_selector::AbstractOutputSelector; kwargs...)\n    output = method.model(input)\n    output_selection = output_selector(output)\n\n    val = ...         # your method's implementation\n    extras = nothing  # optionally add additional information using a named tuple\n    return Explanation(val, input, output, output_selection, :MyMethod, :attribution, extras)\nend Refer to the  example implementations  for more information."},{"id":245,"pagetitle":"API Reference","title":"API Reference","ref":"/XAIDocs/XAIBase/stable/api/#API-Reference","content":" API Reference"},{"id":246,"pagetitle":"API Reference","title":"XAIBase.AbstractXAIMethod","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.AbstractXAIMethod","content":" XAIBase.AbstractXAIMethod  —  Type Abstract super type of all XAI methods. It is expected that all XAI methods are callable types that return an  Explanation : (method::AbstractXAIMethod)(input, output_selector::AbstractOutputSelector) If this function is implemented, XAIBase will provide the  analyze  functionality and  heatmap  functionality by loading either VisionHeatmaps.jl or TextHeatmaps.jl. source"},{"id":247,"pagetitle":"API Reference","title":"Computing explanations","ref":"/XAIDocs/XAIBase/stable/api/#Computing-explanations","content":" Computing explanations Most methods in the Julia-XAI ecosystem work by calling  analyze  on an input and an analyzer:"},{"id":248,"pagetitle":"API Reference","title":"XAIBase.analyze","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.analyze","content":" XAIBase.analyze  —  Function analyze(input, method)\nanalyze(input, method, output_selection) Apply the analyzer  method  for the given input, returning an  Explanation . If  output_selection  is specified, the explanation will be calculated for that output. Otherwise, the output with the highest activation is automatically chosen. See also  Explanation . source The return type of  analyze  is an  Explanation :"},{"id":249,"pagetitle":"API Reference","title":"XAIBase.Explanation","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.Explanation","content":" XAIBase.Explanation  —  Type Explanation(val, output, output_selection, analyzer, heatmap, extras) Return type of analyzers when calling  analyze . Fields val : numerical output of the analyzer, e.g. an attribution or gradient output : model output for the given analyzer input output_selection : index of the output used for the explanation analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP heatmap : symbol indicating a preset heatmapping style,   e.g.  :attribution ,  :sensitivity  or  :cam extras : optional named tuple that can be used by analyzers   to return additional information. source"},{"id":250,"pagetitle":"API Reference","title":"Feature selection","ref":"/XAIDocs/XAIBase/stable/api/#Feature-selection","content":" Feature selection"},{"id":251,"pagetitle":"API Reference","title":"XAIBase.AbstractFeatureSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.AbstractFeatureSelector","content":" XAIBase.AbstractFeatureSelector  —  Type Abstract super type of all feature selectors in XAIBase. Feature selectors are expected to be callable and to return a vector of  CartesianIndices  for each selected feature. Note The XAIBase interface currently assumes that features have either 2 or 4 dimensions ( (features, batchsize)  or  (width, height, features, batchsize) ). It also assumes that the batch dimension is the last dimension of the feature. source"},{"id":252,"pagetitle":"API Reference","title":"XAIBase.IndexedFeatures","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.IndexedFeatures","content":" XAIBase.IndexedFeatures  —  Type IndexedFeatures(indices...) Select features by indices. For outputs of convolutional layers, the index refers to a feature dimension. See also See also  TopNFeatures . Note The XAIBase interface currently assumes that features have either 2 or 4 dimensions ( (features, batchsize)  or  (width, height, features, batchsize) ). It also assumes that the batch dimension is the last dimension of the feature. Example julia> feature_selector = IndexedFeatures(2, 3)\n IndexedFeatures(2, 3)\n\njulia> feature = rand(3, 3, 3, 2);\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{4, NTuple{4, UnitRange{Int64}}}}}:\n [CartesianIndices((1:3, 1:3, 2:2, 1:1)), CartesianIndices((1:3, 1:3, 2:2, 2:2))]\n [CartesianIndices((1:3, 1:3, 3:3, 1:1)), CartesianIndices((1:3, 1:3, 3:3, 2:2))]\n\njulia> feature = rand(3, 2);\n\njulia> feature_selector(feature)\n 1-element Vector{Vector{CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}}}:\n  [CartesianIndices((2:2, 1:1)), CartesianIndices((2:2, 2:2))] source"},{"id":253,"pagetitle":"API Reference","title":"XAIBase.TopNFeatures","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.TopNFeatures","content":" XAIBase.TopNFeatures  —  Type TopNFeatures(n) Select top-n features. For outputs of convolutional layers, the relevance is summed across height and width channels for each feature. See also  IndexedFeatures . Note The XAIBase interface currently assumes that features have either 2 or 4 dimensions ( (features, batchsize)  or  (width, height, features, batchsize) ). It also assumes that the batch dimension is the last dimension of the feature. Example julia> feature_selector = TopNFeatures(2)\n TopNFeatures(2)\n\njulia> feature = rand(3, 2)\n3×2 Matrix{Float64}:\n 0.265312  0.953689\n 0.674377  0.172154\n 0.649722  0.570809\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}}}:\n [CartesianIndices((2:2, 1:1)), CartesianIndices((1:1, 2:2))]\n [CartesianIndices((3:3, 1:1)), CartesianIndices((3:3, 2:2))]\n\njulia> feature = rand(3, 3, 3, 2);\n\njulia> feature_selector(feature)\n2-element Vector{Vector{CartesianIndices{4, NTuple{4, UnitRange{Int64}}}}}:\n [CartesianIndices((1:3, 1:3, 2:2, 1:1)), CartesianIndices((1:3, 1:3, 1:1, 2:2))]\n [CartesianIndices((1:3, 1:3, 1:1, 1:1)), CartesianIndices((1:3, 1:3, 3:3, 2:2))] source"},{"id":254,"pagetitle":"API Reference","title":"Output selection","ref":"/XAIDocs/XAIBase/stable/api/#Output-selection","content":" Output selection"},{"id":255,"pagetitle":"API Reference","title":"XAIBase.AbstractOutputSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.AbstractOutputSelector","content":" XAIBase.AbstractOutputSelector  —  Type Abstract super type of all output selectors in XAIBase. Output selectors are expected to be callable and to return a vector of  CartesianIndex  of the selected outputs. Note XAIBase assumes that the batch dimension is the last dimension of the output. source"},{"id":256,"pagetitle":"API Reference","title":"XAIBase.MaxActivationSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.MaxActivationSelector","content":" XAIBase.MaxActivationSelector  —  Type MaxActivationSelector() Output selector that picks the output with the highest activation. Note XAIBase assumes that the batch dimension is the last dimension of the output. Example julia> output = rand(3, 3)\n3×3 Matrix{Float64}:\n 0.411871  0.313366  0.13402\n 0.885562  0.136938  0.465622\n 0.498235  0.627209  0.298911\n\njulia> output_selector = MaxActivationSelector()\n MaxActivationSelector()\n\njulia> output_selector(output)\n 3-element Vector{CartesianIndex{2}}:\n  CartesianIndex(2, 1)\n  CartesianIndex(3, 2)\n  CartesianIndex(2, 3) source"},{"id":257,"pagetitle":"API Reference","title":"XAIBase.IndexSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.IndexSelector","content":" XAIBase.IndexSelector  —  Type IndexSelector(index) Output selector that picks the output at the given index. Note XAIBase assumes that the batch dimension is the last dimension of the output. Example julia> output = rand(3, 3)\n3×3 Matrix{Float64}:\n 0.411871  0.313366  0.13402\n 0.885562  0.136938  0.465622\n 0.498235  0.627209  0.298911\n\njulia> output_selector = IndexSelector(1)\n IndexSelector{Int64}(1)\n\njulia> output_selector(output)\n 3-element Vector{CartesianIndex{2}}:\n  CartesianIndex(1, 1)\n  CartesianIndex(1, 2)\n  CartesianIndex(1, 3) source"},{"id":258,"pagetitle":"API Reference","title":"Index","ref":"/XAIDocs/XAIBase/stable/api/#Index","content":" Index XAIBase.AbstractFeatureSelector XAIBase.AbstractOutputSelector XAIBase.AbstractXAIMethod XAIBase.Explanation XAIBase.IndexSelector XAIBase.IndexedFeatures XAIBase.MaxActivationSelector XAIBase.TopNFeatures XAIBase.analyze"},{"id":261,"pagetitle":"Example Implementations","title":"Example Implementations","ref":"/XAIDocs/XAIBase/stable/examples/#examples","content":" Example Implementations The following examples demonstrate the implementation of XAI methods using the XAIBase.jl interface. To evaluate our methods, we load a small, pre-trained LeNet5 model and the MNIST dataset: using Flux\nusing BSON\n\nmodel = BSON.load(\"model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.344 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\n\n# By convention in Flux.jl, the input needs to be resized to WHCN format\n# by adding a color channel and batch dimensions.\ninput = reshape(x, 28, 28, 1, :);\n\nconvert2image(MNIST, x)"},{"id":262,"pagetitle":"Example Implementations","title":"Example 1: Random explanation","ref":"/XAIDocs/XAIBase/stable/examples/#Example-1:-Random-explanation","content":" Example 1: Random explanation To get started, we implement a nonsensical method that returns a random explanation in the shape of the input. using XAIBase\nimport XAIBase: call_analyzer\n\nstruct RandomAnalyzer{M} <: AbstractXAIMethod\n    model::M\nend\n\nfunction call_analyzer(input, method::RandomAnalyzer, output_selector::AbstractOutputSelector; kwargs...)\n    output = method.model(input)\n    output_selection = output_selector(output)\n\n    val = rand(size(input)...)\n    return Explanation(val, input, output, output_selection, :RandomAnalyzer, :sensitivity, nothing)\nend call_analyzer (generic function with 2 methods) We can directly use XAIBase's  analyze  function  to compute the random explanation: analyzer = RandomAnalyzer(model)\nexpl = analyze(input, analyzer) Explanation{Array{Float64, 4}, Array{Float32, 4}, Matrix{Float32}, Vector{CartesianIndex{2}}, Nothing}([0.7273925748723502 0.690652989164023 … 0.06244825968041412 0.7485865198891324; 0.39566743212040767 0.63849859266577 … 0.4814987142217383 0.42002066847604447; … ; 0.7394244019133709 0.9682783414945164 … 0.18283448357566745 0.8207014650011124; 0.86085989925645 0.5320034672864322 … 0.35427691037156517 0.17150778975221426;;;;], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], Float32[-17.578014; -14.809775; … ; -0.22507292; 21.765005;;], CartesianIndex{2}[CartesianIndex(10, 1)], :RandomAnalyzer, :sensitivity, nothing) Using either  VisionHeatmaps.jl  or  TextHeatmaps.jl , which provide package extensions on XAIBase's  Explanation  type, we can visualize the explanations: using VisionHeatmaps # load heatmapping functionality\n\nheatmap(expl.val) As expected, the explanation is just noise."},{"id":263,"pagetitle":"Example Implementations","title":"Example 2: Input sensitivity","ref":"/XAIDocs/XAIBase/stable/examples/#Example-2:-Input-sensitivity","content":" Example 2: Input sensitivity In this second example, we naively reimplement the  Gradient  analyzer from  ExplainableAI.jl . using XAIBase\nimport XAIBase: call_analyzer\n\nusing Zygote: gradient\n\nstruct MyGradient{M} <: AbstractXAIMethod\n    model::M\nend\n\nfunction call_analyzer(input, method::MyGradient, output_selector::AbstractOutputSelector; kwargs...)\n    output = method.model(input)\n    output_selection = output_selector(output)\n\n    grad = gradient((x) -> only(method.model(x)[output_selection]), input)\n    val = only(grad)\n    return Explanation(val, input, output, output_selection, :MyGradient, :sensitivity, nothing)\nend call_analyzer (generic function with 3 methods) Note ExplainableAI.jl  implements the  Gradient  analyzer in a more efficient way  that works with batched inputs and only requires a single forward  and backward pass through the model. Once again, we can directly use XAIBase's  analyze  and VisionHeatmaps'  heatmap  functions using VisionHeatmaps\n\nanalyzer = MyGradient(model)\nexpl = analyze(input, analyzer)\nheatmap(expl.val) heatmap(expl.val, colorscheme=:twilight, reduce=:norm, rangescale=:centered) and make use of all the features provided by the Julia-XAI ecosystem. Note For an introduction to the  Julia-XAI ecosystem ,  please refer to the  Getting started  guide ."}]
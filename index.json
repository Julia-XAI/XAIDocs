[{"id":3,"pagetitle":"Home","title":"ExplainableAI.jl","ref":"/XAIDocs/ExplainableAI/stable/#ExplainableAI.jl","content":" ExplainableAI.jl Explainable AI methods in Julia. Note This package is part of a wider  Julia XAI ecosystem . For an introduction to this ecosystem, please refer to the   Getting started guide ."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/ExplainableAI/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run  julia> ]add ExplainableAI"},{"id":5,"pagetitle":"Home","title":"Manual","ref":"/XAIDocs/ExplainableAI/stable/#Manual","content":" Manual Getting started Preparing the input data Explanations Heatmapping basics List of analyzers Neuron selection Analyzing batches Heatmapping Automatic heatmap presets Custom heatmap settings Color schemes Color channel reduction Mapping explanations onto the color scheme Heatmapping batches Processing heatmaps Saving heatmaps Analyzer augmentations Noise augmentation Integration augmentation"},{"id":6,"pagetitle":"Home","title":"API reference","ref":"/XAIDocs/ExplainableAI/stable/#API-reference","content":" API reference Basic API Analyzers Input augmentations Index"},{"id":9,"pagetitle":"API Reference","title":"Basic API","ref":"/XAIDocs/ExplainableAI/stable/api/#Basic-API","content":" Basic API All methods in ExplainableAI.jl work by calling  analyze  on an input and an analyzer:"},{"id":10,"pagetitle":"API Reference","title":"XAIBase.analyze","ref":"/XAIDocs/ExplainableAI/stable/api/#XAIBase.analyze","content":" XAIBase.analyze  —  Function analyze(input, method)\nanalyze(input, method, neuron_selection) Apply the analyzer  method  for the given input, returning an  Explanation . If  neuron_selection  is specified, e.g. the index of a specific output neuron, the explanation will be calculated for that neuron. Otherwise, the output neuron with the highest activation is automatically chosen. See also  Explanation  and  heatmap . Keyword arguments add_batch_dim : add batch dimension to the input without allocating. Default is  false . source"},{"id":11,"pagetitle":"API Reference","title":"XAIBase.Explanation","ref":"/XAIDocs/ExplainableAI/stable/api/#XAIBase.Explanation","content":" XAIBase.Explanation  —  Type Return type of analyzers when calling  analyze . Fields val : numerical output of the analyzer, e.g. an attribution or gradient output : model output for the given analyzer input output_selection : index of the output used for the explanation analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP heatmap : symbol indicating a preset heatmapping style,   e.g.  :attibution ,  :sensitivity  or  :cam extras : optional named tuple that can be used by analyzers   to return additional information. source"},{"id":12,"pagetitle":"API Reference","title":"XAIBase.heatmap","ref":"/XAIDocs/ExplainableAI/stable/api/#XAIBase.heatmap","content":" XAIBase.heatmap  —  Function heatmap(explanation) Visualize  Explanation  from XAIBase as a vision heatmap. Assumes WHCN convention (width, height, channels, batchsize) for  explanation.val . Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  :seismic . reduce::Symbol : Selects how color channels are reduced to a single number to apply a color scheme. The following methods can be selected, which are then applied over the color channels for each \"pixel\" in the array: :sum : sum up color channels :norm : compute 2-norm over the color channels :maxabs : compute  maximum(abs, x)  over the color channels Defaults to  :sum . rangescale::Symbol : Selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered . process_batch::Bool : When heatmapping a batch, setting  process_batch=true  will apply the  rangescale  normalization to the entire batch instead of computing it individually for each sample in the batch. Defaults to  false . permute::Bool : Whether to flip W&H input channels. Default is  true . unpack_singleton::Bool : If false,  heatmap  will always return a vector of images. When heatmapping a batch with a single sample, setting  unpack_singleton=true  will unpack the singleton vector and directly return the image. Defaults to  true . source heatmap(input, analyzer) Compute an  Explanation  for a given  input  using the method  analyzer  and visualize it as a vision heatmap. Any additional arguments and keyword arguments are passed to the analyzer. Refer to  analyze  for more information on available keyword arguments. To customize the heatmapping style, first compute an explanation using  analyze  and then call  heatmap  on the explanation. source heatmap(explanation, text) Visualize  Explanation  from XAIBase as text heatmap. Text should be a vector containing vectors of strings, one for each input in the batched explanation. Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  :seismic . rangescale::Symbol : selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered  for use with the default color scheme  :seismic . source"},{"id":13,"pagetitle":"API Reference","title":"Analyzers","ref":"/XAIDocs/ExplainableAI/stable/api/#Analyzers","content":" Analyzers"},{"id":14,"pagetitle":"API Reference","title":"ExplainableAI.Gradient","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.Gradient","content":" ExplainableAI.Gradient  —  Type Gradient(model) Analyze model by calculating the gradient of a neuron activation with respect to the input. source"},{"id":15,"pagetitle":"API Reference","title":"ExplainableAI.InputTimesGradient","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.InputTimesGradient","content":" ExplainableAI.InputTimesGradient  —  Type InputTimesGradient(model) Analyze model by calculating the gradient of a neuron activation with respect to the input. This gradient is then multiplied element-wise with the input. source"},{"id":16,"pagetitle":"API Reference","title":"ExplainableAI.SmoothGrad","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.SmoothGrad","content":" ExplainableAI.SmoothGrad  —  Function SmoothGrad(analyzer, [n=50, std=0.1, rng=GLOBAL_RNG])\nSmoothGrad(analyzer, [n=50, distribution=Normal(0, σ²=0.01), rng=GLOBAL_RNG]) Analyze model by calculating a smoothed sensitivity map. This is done by averaging sensitivity maps of a  Gradient  analyzer over random samples in a neighborhood of the input, typically by adding Gaussian noise with mean 0. References Smilkov et al.,  SmoothGrad: removing noise by adding noise source"},{"id":17,"pagetitle":"API Reference","title":"ExplainableAI.IntegratedGradients","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.IntegratedGradients","content":" ExplainableAI.IntegratedGradients  —  Function IntegratedGradients(analyzer, [n=50])\nIntegratedGradients(analyzer, [n=50]) Analyze model by using the Integrated Gradients method. References Sundararajan et al.,  Axiomatic Attribution for Deep Networks source"},{"id":18,"pagetitle":"API Reference","title":"Input augmentations","ref":"/XAIDocs/ExplainableAI/stable/api/#Input-augmentations","content":" Input augmentations SmoothGrad  and  IntegratedGradients  are special cases of the input augmentations   NoiseAugmentation  and  InterpolationAugmentation ,  which can be applied as a wrapper to any analyzer:"},{"id":19,"pagetitle":"API Reference","title":"ExplainableAI.NoiseAugmentation","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.NoiseAugmentation","content":" ExplainableAI.NoiseAugmentation  —  Type NoiseAugmentation(analyzer, n, [std=1, rng=GLOBAL_RNG])\nNoiseAugmentation(analyzer, n, distribution, [rng=GLOBAL_RNG]) A wrapper around analyzers that augments the input with  n  samples of additive noise sampled from  distribution . This input augmentation is then averaged to return an  Explanation . source"},{"id":20,"pagetitle":"API Reference","title":"ExplainableAI.InterpolationAugmentation","ref":"/XAIDocs/ExplainableAI/stable/api/#ExplainableAI.InterpolationAugmentation","content":" ExplainableAI.InterpolationAugmentation  —  Type InterpolationAugmentation(model, [n=50]) A wrapper around analyzers that augments the input with  n  steps of linear interpolation between the input and a reference input (typically  zero(input) ). The gradients w.r.t. this augmented input are then averaged and multiplied with the difference between the input and the reference input. source"},{"id":21,"pagetitle":"API Reference","title":"Index","ref":"/XAIDocs/ExplainableAI/stable/api/#Index","content":" Index ExplainableAI.Gradient ExplainableAI.InputTimesGradient ExplainableAI.InterpolationAugmentation ExplainableAI.NoiseAugmentation XAIBase.Explanation ExplainableAI.IntegratedGradients ExplainableAI.SmoothGrad XAIBase.analyze XAIBase.heatmap"},{"id":24,"pagetitle":"Input augmentations","title":"Analyzer augmentations","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#docs-augmentations","content":" Analyzer augmentations All analyzers implemented in ExplainableAI.jl can be augmented by two types of augmentations:  NoiseAugmentation s and  InterpolationAugmentation s. These augmentations are wrappers around analyzers that modify the input before passing it to the analyzer. We build on the basics shown in the  Getting started  section and start out by loading the same pre-trained LeNet5 model and MNIST input data: using ExplainableAI\nusing Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.867 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)"},{"id":25,"pagetitle":"Input augmentations","title":"Noise augmentation","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#Noise-augmentation","content":" Noise augmentation The  NoiseAugmentation  wrapper computes explanations averaged over noisy inputs. Let's demonstrate this on the  Gradient  analyzer. First, we compute the heatmap of an explanation without augmentation: analyzer = Gradient(model)\nheatmap(input, analyzer) Now we wrap the analyzer in a  NoiseAugmentation  with 10 samples of noise. By default, the noise is sampled from a Gaussian distribution with mean 0 and standard deviation 1. analyzer = NoiseAugmentation(Gradient(model), 50)\nheatmap(input, analyzer) Note that a higher sample size is desired, as it will lead to a smoother heatmap. However, this comes at the cost of a longer computation time. We can also set the standard deviation of the Gaussian distribution: analyzer = NoiseAugmentation(Gradient(model), 50, 0.1)\nheatmap(input, analyzer) When used with a  Gradient  analyzer, this is equivalent to  SmoothGrad : analyzer = SmoothGrad(model, 50)\nheatmap(input, analyzer) We can also use any distribution from  Distributions.jl , for example Poisson noise with rate  $\\lambda=0.5$ : using Distributions\n\nanalyzer = NoiseAugmentation(Gradient(model), 50, Poisson(0.5))\nheatmap(input, analyzer) Is is also possible to define your own distributions or mixture distributions. NoiseAugmentation  can be combined with any analyzer type from the Julia-XAI ecosystem, for example  LRP  from  RelevancePropagation.jl ."},{"id":26,"pagetitle":"Input augmentations","title":"Integration augmentation","ref":"/XAIDocs/ExplainableAI/stable/generated/augmentations/#Integration-augmentation","content":" Integration augmentation The  InterpolationAugmentation  wrapper computes explanations averaged over  n  steps of linear interpolation between the input and a reference input, which is set to  zero(input)  by default: analyzer = InterpolationAugmentation(Gradient(model), 50)\nheatmap(input, analyzer) When used with a  Gradient  analyzer, this is equivalent to  IntegratedGradients : analyzer = IntegratedGradients(model, 50)\nheatmap(input, analyzer) To select a different reference input, pass it to the  analyze  function using the keyword argument  input_ref . Note that this is an arbitrary example for the sake of demonstration. matrix_of_ones = ones(Float32, size(input))\n\nanalyzer = InterpolationAugmentation(Gradient(model), 50)\nexpl = analyzer(input; input_ref=matrix_of_ones)\nheatmap(expl) Once again,  InterpolationAugmentation  can be combined with any analyzer type from the Julia-XAI ecosystem, for example  LRP  from  RelevancePropagation.jl . This page was generated using  Literate.jl ."},{"id":29,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#docs-getting-started","content":" Getting started Note This package is part of a wider  Julia XAI ecosystem . For an introduction to this ecosystem, please refer to the  Getting started guide . For this first example, we already have loaded a pre-trained LeNet5 model to look at explanations on the MNIST dataset. using ExplainableAI\nusing Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.867 KiB. Supported models ExplainableAI.jl can be used on any differentiable classifier."},{"id":30,"pagetitle":"Getting started","title":"Preparing the input data","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Preparing-the-input-data","content":" Preparing the input data We use MLDatasets to load a single image from the MNIST dataset: using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\n\nconvert2image(MNIST, x) By convention in Flux.jl, this input needs to be resized to WHCN format by adding a color channel and batch dimensions. input = reshape(x, 28, 28, 1, :); Input format For any explanation of a model, ExplainableAI.jl assumes the batch dimension to come last in the input. For the purpose of heatmapping, the input is assumed to be in WHCN order (width, height, channels, batch), which is Flux.jl's convention."},{"id":31,"pagetitle":"Getting started","title":"Explanations","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Explanations","content":" Explanations We can now select an analyzer of our choice and call  analyze  to get an  Explanation : analyzer = InputTimesGradient(model)\nexpl = analyze(input, analyzer); The return value  expl  is of type  Explanation  and bundles the following data: expl.val : numerical output of the analyzer, e.g. an attribution or gradient expl.output : model output for the given analyzer input expl.output_selection : index of the output used for the explanation expl.analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP expl.heatmap : symbol indicating a preset heatmapping style,   e.g.  :attibution ,  :sensitivity  or  :cam expl.extras : optional named tuple that can be used by analyzers   to return additional information. We used  InputTimesGradient , so  expl.analyzer  is  :InputTimesGradient . expl.analyzer :InputTimesGradient By default, the explanation is computed for the maximally activated output neuron. Since our digit is a 9 and Julia's indexing is 1-based, the output neuron at index  10  of our trained model is maximally activated. Finally, we obtain the result of the analyzer in form of an array. expl.val 28×28×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0  …  -0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0     -0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0     -0.0       0.0         0.0\n  0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0  -0.0   0.0   0.0      0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0   0.0  -0.0  -0.0  …   0.0       0.0         0.0\n -0.0   0.0   0.0   0.0   0.0   0.0   0.0      0.0       0.0         0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.0       0.0        -0.0\n -0.0   0.0   0.0   0.0  -0.0  -0.0  -0.0      0.0       0.0        -0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.385115  0.0714216   0.0\n  ⋮                             ⋮          ⋱   ⋮                    \n  0.0  -0.0  -0.0   0.0   0.0   0.0  -0.0     -0.0       0.0         0.0\n -0.0  -0.0   0.0   0.0   0.0   0.0  -0.0  …  -0.0       0.0         0.0\n -0.0   0.0   0.0   0.0   0.0   0.0  -0.0      0.0       0.0         0.0\n -0.0   0.0   0.0   0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n -0.0   0.0   0.0  -0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0  -0.0   0.0  -0.0      0.0       0.0         0.0\n  0.0   0.0   0.0  -0.0  -0.0   0.0   0.0  …   0.0       0.0         0.0\n  0.0   0.0  -0.0  -0.0   0.0   0.0   0.0      0.0       0.0         0.0\n  0.0   0.0   0.0   0.0   0.0   0.0  -0.0      0.0       0.0         0.0"},{"id":32,"pagetitle":"Getting started","title":"Heatmapping basics","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Heatmapping-basics","content":" Heatmapping basics Since the array  expl.val  is not very informative at first sight, we can visualize  Explanation s by computing a  heatmap : heatmap(expl) If we are only interested in the heatmap, we can combine analysis and heatmapping into a single function call: heatmap(input, analyzer) For a more detailed explanation of the  heatmap  function, refer to the  heatmapping section ."},{"id":33,"pagetitle":"Getting started","title":"List of analyzers","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#docs-analyzers-list","content":" List of analyzers"},{"id":34,"pagetitle":"Getting started","title":"Neuron selection","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Neuron-selection","content":" Neuron selection By passing an additional index to our call to  analyze , we can compute an explanation with respect to a specific output neuron. Let's see why the output wasn't interpreted as a 4 (output neuron at index 5) expl = analyze(input, analyzer, 5)\nheatmap(expl) This heatmap shows us that the \"upper loop\" of the hand-drawn 9 has negative relevance with respect to the output neuron corresponding to digit 4! Note The output neuron can also be specified when calling  heatmap : heatmap(input, analyzer, 5)"},{"id":35,"pagetitle":"Getting started","title":"Analyzing batches","ref":"/XAIDocs/ExplainableAI/stable/generated/example/#Analyzing-batches","content":" Analyzing batches ExplainableAI also supports explanations of input batches: batchsize = 20\nxs, _ = MNIST(Float32, :test)[1:batchsize]\nbatch = reshape(xs, 28, 28, 1, :) # reshape to WHCN format\nexpl = analyze(batch, analyzer); This will return a single  Explanation expl  for the entire batch. Calling  heatmap  on  expl  will detect the batch dimension and return a vector of heatmaps. heatmap(expl) (a vector displayed as a row to save space) For more information on heatmapping batches, refer to the  heatmapping documentation . This page was generated using  Literate.jl ."},{"id":38,"pagetitle":"Heatmapping","title":"Heatmapping","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#docs-heatmapping","content":" Heatmapping Since numerical explanations are not very informative at first sight, we can visualize them by computing a  heatmap . This page showcases different options and preset for heatmapping, building on the basics shown in the  Getting started  section. We start out by loading the same pre-trained LeNet5 model and MNIST input data: using ExplainableAI\nusing Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.867 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)"},{"id":39,"pagetitle":"Heatmapping","title":"Automatic heatmap presets","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#Automatic-heatmap-presets","content":" Automatic heatmap presets The function  heatmap  automatically applies common presets for each method. Since  InputTimesGradient  computes attributions, heatmaps are shown in a blue-white-red color scheme. Gradient methods however are typically shown in grayscale: analyzer = Gradient(model)\nheatmap(input, analyzer) analyzer = InputTimesGradient(model)\nheatmap(input, analyzer)"},{"id":40,"pagetitle":"Heatmapping","title":"Custom heatmap settings","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#Custom-heatmap-settings","content":" Custom heatmap settings"},{"id":41,"pagetitle":"Heatmapping","title":"Color schemes","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#Color-schemes","content":" Color schemes We can partially or fully override presets by passing keyword arguments to  heatmap . For example, we can use a custom color scheme from ColorSchemes.jl using the keyword argument  colorscheme : using ColorSchemes\n\nexpl = analyze(input, analyzer)\nheatmap(expl; colorscheme=:jet) heatmap(expl; colorscheme=:inferno) Refer to the  ColorSchemes.jl catalogue  for a gallery of available color schemes."},{"id":42,"pagetitle":"Heatmapping","title":"Color channel reduction","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#docs-heatmap-reduce","content":" Color channel reduction Explanations have the same dimensionality as the inputs to the classifier. For images with multiple color channels, this means that the explanation also has a \"color channel\" dimension. The keyword argument  reduce  can be used to reduce this dimension to a single scalar value for each pixel. The following presets are available: :sum : sum up color channels (default setting) :norm : compute 2-norm over the color channels :maxabs : compute  maximum(abs, x)  over the color channels heatmap(expl; reduce=:sum) heatmap(expl; reduce=:norm) heatmap(expl; reduce=:maxabs) In this example, the heatmaps look identical. Since MNIST only has a single color channel, there is no need for color channel reduction."},{"id":43,"pagetitle":"Heatmapping","title":"Mapping explanations onto the color scheme","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#docs-heatmap-rangescale","content":" Mapping explanations onto the color scheme To map a  color-channel-reduced  explanation onto a color scheme, we first need to normalize all values to the range  $[0, 1]$ . For this purpose, two presets are available through the  rangescale  keyword argument: :extrema : normalize to the minimum and maximum value of the explanation :centered : normalize to the maximum absolute value of the explanation. Values of zero will be mapped to the center of the color scheme. Depending on the color scheme, one of these presets may be more suitable than the other. The default color scheme for  InputTimesGradient ,  seismic , is centered around zero, making  :centered  a good choice: heatmap(expl; rangescale=:centered) heatmap(expl; rangescale=:extrema) However, for the  inferno  color scheme, which is not centered around zero,  :extrema  leads to a heatmap with higher contrast. heatmap(expl; rangescale=:centered, colorscheme=:inferno) heatmap(expl; rangescale=:extrema, colorscheme=:inferno) For the full list of  heatmap  keyword arguments, refer to the  heatmap  documentation."},{"id":44,"pagetitle":"Heatmapping","title":"Heatmapping batches","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#docs-heatmapping-batches","content":" Heatmapping batches Heatmapping also works with input batches. Let's demonstrate this by using a batch of 100 images from the MNIST dataset: xs, ys = MNIST(Float32, :test)[1:100]\nbatch = reshape(xs, 28, 28, 1, :); # reshape to WHCN format The  heatmap  function automatically recognizes that the explanation is batched and returns a  Vector  of images: heatmaps = heatmap(batch, analyzer) (a vector displayed as a row to save space) Image.jl's  mosaic  function can used to display them in a grid: mosaic(heatmaps; nrow=10) When heatmapping batches, the mapping to the color scheme is applied per sample. For example,  rangescale=:extrema  will normalize each heatmap to the minimum and maximum value of each sample in the batch. This ensures that heatmaps don't depend on other samples in the batch. If this bevahior is not desired,  heatmap  can be called with the keyword-argument  process_batch=true : expl = analyze(batch, analyzer)\nheatmaps = heatmap(expl; process_batch=true)\nmosaic(heatmaps; nrow=10) This can be useful when comparing heatmaps for fixed output neurons: expl = analyze(batch, analyzer, 7) # explain digit \"6\"\nheatmaps = heatmap(expl; process_batch=true)\nmosaic(heatmaps; nrow=10) Output type consistency To obtain a singleton  Vector  containing a single heatmap for non-batched inputs, use the  heatmap  keyword argument  unpack_singleton=false ."},{"id":45,"pagetitle":"Heatmapping","title":"Processing heatmaps","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#Processing-heatmaps","content":" Processing heatmaps Heatmapping makes use of the  Julia-based image processing ecosystem Images.jl . If you want to further process heatmaps, you may benefit from reading about some  fundamental conventions  that the ecosystem utilizes that are different from how images are typically represented in OpenCV, MATLAB, ImageJ or Python."},{"id":46,"pagetitle":"Heatmapping","title":"Saving heatmaps","ref":"/XAIDocs/ExplainableAI/stable/generated/heatmapping/#Saving-heatmaps","content":" Saving heatmaps Since heatmaps are regular Images.jl images, they can be saved as such: using FileIO\n\nimg = heatmap(input, analyzer)\nsave(\"heatmap.png\", img) This page was generated using  Literate.jl ."},{"id":49,"pagetitle":"Home","title":"RelevancePropagation.jl","ref":"/XAIDocs/RelevancePropagation/stable/#RelevancePropagation.jl","content":" RelevancePropagation.jl Julia implementation of  Layerwise Relevance Propagation  (LRP)  and  Concept Relevance Propagation  (CRP)  for use with  Flux.jl  models. Note This package is part the  Julia-XAI ecosystem . For an introduction to the ecosystem, please refer to the   Getting started  guide ."},{"id":50,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/RelevancePropagation/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run  julia> ]add RelevancePropagation"},{"id":51,"pagetitle":"Home","title":"Index","ref":"/XAIDocs/RelevancePropagation/stable/#Index","content":" Index"},{"id":52,"pagetitle":"Home","title":"Manual","ref":"/XAIDocs/RelevancePropagation/stable/#Manual","content":" Manual Basic usage of LRP Model preparation Stripping the output softmax Canonizing the model Flattening the model LRP rules Computing layerwise relevances Performance tips Using LRP with a GPU Using LRP without a GPU Assigning LRP rules to layers Manually assigning rules Custom composites Composite primitives Assigning a rule to a specific layer Composite presets Supporting new layers and activation functions Model checks Registering layers Registering activation functions Skipping model checks Custom LRP rules Implementing a custom rule Step 1: Define rule struct Step 2: Implement rule behavior Step 3: Use rule in LRP analyzer Performance tips Advanced layer modification Advanced LRP rules LRP developer documentation Generic LRP rule implementation Linear layers The automatic differentiation fallback LRP analyzer struct Forward and reverse pass Rule calls Specialized implementations"},{"id":53,"pagetitle":"Home","title":"API reference","ref":"/XAIDocs/RelevancePropagation/stable/#API-reference","content":" API reference LRP analyzer LRP rules Model preparation Composites Applying composites Composite primitives Composite presets Custom rules CRP Index"},{"id":56,"pagetitle":"API reference","title":"LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/api/#LRP-analyzer","content":" LRP analyzer"},{"id":57,"pagetitle":"API reference","title":"RelevancePropagation.LRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP","content":" RelevancePropagation.LRP  —  Type LRP(model, rules)\nLRP(model, composite) Analyze model by applying Layer-Wise Relevance Propagation. The analyzer can either be created by passing an array of LRP-rules or by passing a composite, see  Composite  for an example. Keyword arguments skip_checks::Bool : Skip checks whether model is compatible with LRP and contains output softmax. Default is  false . verbose::Bool : Select whether the model checks should print a summary on failure. Default is  true . References [1] G. Montavon et al., Layer-Wise Relevance Propagation: An Overview [2] W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications source"},{"id":58,"pagetitle":"API reference","title":"LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#api-lrp-rules","content":" LRP rules"},{"id":59,"pagetitle":"API reference","title":"RelevancePropagation.ZeroRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZeroRule","content":" RelevancePropagation.ZeroRule  —  Type ZeroRule() LRP- $0$  rule. Commonly used on upper layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i \\frac{W_{ij}a_j^k}{\\sum_l W_{il}a_l^k+b_i} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":60,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonRule","content":" RelevancePropagation.EpsilonRule  —  Type EpsilonRule([epsilon=1.0e-6]) LRP- $ϵ$  rule. Commonly used on middle layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}a_j^k}{\\epsilon +\\sum_{l}W_{il}a_l^k+b_i} R_i^{k+1}\\] Optional arguments epsilon : Optional stabilization parameter, defaults to  1.0e-6 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation source"},{"id":61,"pagetitle":"API reference","title":"RelevancePropagation.GammaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GammaRule","content":" RelevancePropagation.GammaRule  —  Type GammaRule([gamma=0.25]) LRP- $γ$  rule. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{(W_{ij}+\\gamma W_{ij}^+)a_j^k}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_l^k+(b_i+\\gamma b_i^+)} R_i^{k+1}\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":62,"pagetitle":"API reference","title":"RelevancePropagation.WSquareRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.WSquareRule","content":" RelevancePropagation.WSquareRule  —  Type WSquareRule() LRP- $w²$  rule. Commonly used on the first layer when values are unbounded. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{W_{ij}^2}{\\sum_l W_{il}^2} R_i^{k+1}\\] References G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":63,"pagetitle":"API reference","title":"RelevancePropagation.FlatRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FlatRule","content":" RelevancePropagation.FlatRule  —  Type FlatRule() LRP-Flat rule. Similar to the  WSquareRule , but with all weights set to one and all bias terms set to zero. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{1}{\\sum_l 1} R_i^{k+1} = \\sum_i\\frac{1}{n_i} R_i^{k+1}\\] where  $n_i$  is the number of input neurons connected to the output neuron at index  $i$ . References S. Lapuschkin et al.,  Unmasking Clever Hans predictors and assessing what machines really learn source"},{"id":64,"pagetitle":"API reference","title":"RelevancePropagation.AlphaBetaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.AlphaBetaRule","content":" RelevancePropagation.AlphaBetaRule  —  Type AlphaBetaRule([alpha=2.0, beta=1.0]) LRP- $αβ$  rule. Weights positive and negative contributions according to the parameters  alpha  and  beta  respectively. The difference  $α-β$  must be equal to one. Commonly used on lower layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\left(\n    \\alpha\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+}\n    -\\beta\\frac{\\left(W_{ij}a_j^k\\right)^-}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^-}\n\\right) R_i^{k+1}\\] Optional arguments alpha : Multiplier for the positive output term, defaults to  2.0 . beta : Multiplier for the negative output term, defaults to  1.0 . References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":65,"pagetitle":"API reference","title":"RelevancePropagation.ZPlusRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZPlusRule","content":" RelevancePropagation.ZPlusRule  —  Type ZPlusRule() LRP- $z⁺$  rule. Commonly used on lower layers. Equivalent to  AlphaBetaRule(1.0f0, 0.0f0) , but slightly faster. See also  AlphaBetaRule . Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac{\\left(W_{ij}a_j^k\\right)^+}{\\sum_l\\left(W_{il}a_l^k+b_i\\right)^+} R_i^{k+1}\\] References S. Bach et al.,  On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation G. Montavon et al.,  Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition source"},{"id":66,"pagetitle":"API reference","title":"RelevancePropagation.ZBoxRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ZBoxRule","content":" RelevancePropagation.ZBoxRule  —  Type ZBoxRule(low, high) LRP- $zᴮ$ -rule. Commonly used on the first layer for pixel input. The parameters  low  and  high  should be set to the lower and upper bounds of the input features, e.g.  0.0  and  1.0  for raw image data. It is also possible to provide two arrays of that match the input size. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k=\\sum_i \\frac{W_{ij}a_j^k - W_{ij}^{+}l_j - W_{ij}^{-}h_j}\n    {\\sum_l W_{il}a_l^k+b_i - \\left(W_{il}^{+}l_l+b_i^{+}\\right) - \\left(W_{il}^{-}h_l+b_i^{-}\\right)} R_i^{k+1}\\] References G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview source"},{"id":67,"pagetitle":"API reference","title":"RelevancePropagation.PassRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.PassRule","content":" RelevancePropagation.PassRule  —  Type PassRule() Pass-through rule. Passes relevance through to the lower layer. Supports layers with constant input and output shapes, e.g. reshaping layers. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = R_j^{k+1}\\] source"},{"id":68,"pagetitle":"API reference","title":"RelevancePropagation.GeneralizedGammaRule","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GeneralizedGammaRule","content":" RelevancePropagation.GeneralizedGammaRule  —  Type GeneralizedGammaRule([gamma=0.25]) Generalized LRP- $γ$  rule. Can be used on layers with  leakyrelu  activation functions. Definition Propagates relevance  $R^{k+1}$  at layer output to  $R^k$  at layer input according to \\[R_j^k = \\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^+)a_j^+ +(W_{ij}+\\gamma W_{ij}^-)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^+)a_j^+ +(W_{il}+\\gamma W_{il}^-)a_j^- +(b_i+\\gamma b_i^+)}\nI(z_k>0) \\cdot R^{k+1}_i\n+\\sum_i\\frac\n    {(W_{ij}+\\gamma W_{ij}^-)a_j^+ +(W_{ij}+\\gamma W_{ij}^+)a_j^-}\n    {\\sum_l(W_{il}+\\gamma W_{il}^-)a_j^+ +(W_{il}+\\gamma W_{il}^+)a_j^- +(b_i+\\gamma b_i^-)}\nI(z_k<0) \\cdot R^{k+1}_i\\] Optional arguments gamma : Optional multiplier for added positive weights, defaults to  0.25 . References L. Andéol et al.,  Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization source For  manual rule assignment ,  use  ChainTuple  and  ParallelTuple , matching the model structure:"},{"id":69,"pagetitle":"API reference","title":"RelevancePropagation.ChainTuple","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ChainTuple","content":" RelevancePropagation.ChainTuple  —  Type ChainTuple(xs) Thin wrapper around  Tuple  for use with Flux.jl models. Combining  ChainTuple  and  ParallelTuple , data  xs  can be stored while preserving the structure of a Flux model without risking type piracy. source"},{"id":70,"pagetitle":"API reference","title":"RelevancePropagation.ParallelTuple","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ParallelTuple","content":" RelevancePropagation.ParallelTuple  —  Type ParallelTuple(xs) Thin wrapper around  Tuple  for use with Flux.jl models. Combining  ChainTuple  and  ParallelTuple , data  xs  can be stored while preserving the structure of a Flux model without risking type piracy. source"},{"id":71,"pagetitle":"API reference","title":"Model preparation","ref":"/XAIDocs/RelevancePropagation/stable/api/#Model-preparation","content":" Model preparation"},{"id":72,"pagetitle":"API reference","title":"RelevancePropagation.strip_softmax","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.strip_softmax","content":" RelevancePropagation.strip_softmax  —  Function strip_softmax(model)\nstrip_softmax(layer) Remove softmax activation on layer or model if it exists. source"},{"id":73,"pagetitle":"API reference","title":"RelevancePropagation.canonize","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.canonize","content":" RelevancePropagation.canonize  —  Function canonize(model) Canonize model by flattening it and fusing BatchNorm layers into preceding Dense and Conv layers with linear activation functions. source"},{"id":74,"pagetitle":"API reference","title":"RelevancePropagation.flatten_model","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.flatten_model","content":" RelevancePropagation.flatten_model  —  Function flatten_model(model) Flatten a Flux  Chain  containing  Chain s. source"},{"id":75,"pagetitle":"API reference","title":"Composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Composites","content":" Composites"},{"id":76,"pagetitle":"API reference","title":"Applying composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Applying-composites","content":" Applying composites"},{"id":77,"pagetitle":"API reference","title":"RelevancePropagation.Composite","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.Composite","content":" RelevancePropagation.Composite  —  Type Composite(primitives...)\nComposite(default_rule, primitives...) Automatically contructs a list of LRP-rules by sequentially applying composite primitives. Primitives To apply a single rule, use: LayerMap  to apply a rule to the  n -th layer of a model GlobalMap  to apply a rule to all layers RangeMap  to apply a rule to a positional range of layers FirstLayerMap  to apply a rule to the first layer LastLayerMap  to apply a rule to the last layer To apply a set of rules to layers based on their type, use: GlobalTypeMap  to apply a dictionary that maps layer types to LRP-rules RangeTypeMap  for a  TypeMap  on generalized ranges FirstLayerTypeMap  for a  TypeMap  on the first layer of a model LastLayerTypeMap  for a  TypeMap  on the last layer FirstNTypeMap  for a  TypeMap  on the first  n  layers Example Using a VGG11 model: julia> composite = Composite(\n           GlobalTypeMap(\n               ConvLayer => AlphaBetaRule(),\n               Dense => EpsilonRule(),\n               PoolingLayer => EpsilonRule(),\n               DropoutLayer => PassRule(),\n               ReshapingLayer => PassRule(),\n           ),\n           FirstNTypeMap(7, Conv => FlatRule()),\n       );\n\njulia> analyzer = LRP(model, composite)\nLRP(\n  Conv((3, 3), 3 => 64, relu, pad=1)    => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 64 => 128, relu, pad=1)  => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 128 => 256, relu, pad=1) => FlatRule(),\n  Conv((3, 3), 256 => 256, relu, pad=1) => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 256 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  MLUtils.flatten                       => PassRule(),\n  Dense(25088 => 4096, relu)            => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 4096, relu)             => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 1000)                   => EpsilonRule{Float32}(1.0f-6),\n) source"},{"id":78,"pagetitle":"API reference","title":"RelevancePropagation.lrp_rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.lrp_rules","content":" RelevancePropagation.lrp_rules  —  Function lrp_rules(model, composite) Apply a composite to obtain LRP-rules for a given Flux model. source"},{"id":79,"pagetitle":"API reference","title":"Composite primitives","ref":"/XAIDocs/RelevancePropagation/stable/api/#api-composite-primitives","content":" Composite primitives"},{"id":80,"pagetitle":"API reference","title":"Mapping layers to rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#Mapping-layers-to-rules","content":" Mapping layers to rules Composite primitives that apply a single rule:"},{"id":81,"pagetitle":"API reference","title":"RelevancePropagation.LayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LayerMap","content":" RelevancePropagation.LayerMap  —  Type LayerMap(index, rule) Composite primitive that maps an LRP-rule to all layers in the model at the given index. The index can either be an integer or a tuple of integers to map a rule to a specific layer in nested Flux  Chain s. See  show_layer_indices  to print layer indices and  Composite  for an example. source"},{"id":82,"pagetitle":"API reference","title":"RelevancePropagation.GlobalMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GlobalMap","content":" RelevancePropagation.GlobalMap  —  Type GlobalMap(rule) Composite primitive that maps an LRP-rule to all layers in the model. See  Composite  for an example. source"},{"id":83,"pagetitle":"API reference","title":"RelevancePropagation.RangeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.RangeMap","content":" RelevancePropagation.RangeMap  —  Type RangeMap(range, rule) Composite primitive that maps an LRP-rule to the specified positional  range  of layers in the model. See  Composite  for an example. source"},{"id":84,"pagetitle":"API reference","title":"RelevancePropagation.FirstLayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstLayerMap","content":" RelevancePropagation.FirstLayerMap  —  Type FirstLayerMap(rule) Composite primitive that maps an LRP-rule to the first layer in the model. See  Composite  for an example. source"},{"id":85,"pagetitle":"API reference","title":"RelevancePropagation.LastLayerMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LastLayerMap","content":" RelevancePropagation.LastLayerMap  —  Type LastLayerMap(rule) Composite primitive that maps an LRP-rule to the last layer in the model. See  Composite  for an example. source To apply  LayerMap  to nested Flux Chains or  Parallel  layers,  make use of  show_layer_indices :"},{"id":86,"pagetitle":"API reference","title":"RelevancePropagation.show_layer_indices","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.show_layer_indices","content":" RelevancePropagation.show_layer_indices  —  Function show_layer_indices(model) Print layer indices of Flux models. This is primarily a utility to help define  LayerMap  primitives. source"},{"id":87,"pagetitle":"API reference","title":"Mapping layers to rules based on type","ref":"/XAIDocs/RelevancePropagation/stable/api/#Mapping-layers-to-rules-based-on-type","content":" Mapping layers to rules based on type Composite primitives that apply rules based on the layer type:"},{"id":88,"pagetitle":"API reference","title":"RelevancePropagation.GlobalTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.GlobalTypeMap","content":" RelevancePropagation.GlobalTypeMap  —  Type GlobalTypeMap(map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":89,"pagetitle":"API reference","title":"RelevancePropagation.RangeTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.RangeTypeMap","content":" RelevancePropagation.RangeTypeMap  —  Type RangeTypeMap(range, map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map  within the specified  range  of layers in the model. See  Composite  for an example. source"},{"id":90,"pagetitle":"API reference","title":"RelevancePropagation.FirstLayerTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstLayerTypeMap","content":" RelevancePropagation.FirstLayerTypeMap  —  Type FirstLayerTypeMap(map) Composite primitive that maps the type of the first layer of the model to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":91,"pagetitle":"API reference","title":"RelevancePropagation.LastLayerTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LastLayerTypeMap","content":" RelevancePropagation.LastLayerTypeMap  —  Type LastLayerTypeMap(map) Composite primitive that maps the type of the last layer of the model to LRP rules based on a list of type-rule-pairs  map . See  Composite  for an example. source"},{"id":92,"pagetitle":"API reference","title":"RelevancePropagation.FirstNTypeMap","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.FirstNTypeMap","content":" RelevancePropagation.FirstNTypeMap  —  Type FirstNTypeMap(n, map) Composite primitive that maps layer types to LRP rules based on a list of type-rule-pairs  map  within the first  n  layers in the model. See  Composite  for an example. source"},{"id":93,"pagetitle":"API reference","title":"Union types for composites","ref":"/XAIDocs/RelevancePropagation/stable/api/#Union-types-for-composites","content":" Union types for composites The following exported union types types can be used to define TypeMaps:"},{"id":94,"pagetitle":"API reference","title":"RelevancePropagation.ConvLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ConvLayer","content":" RelevancePropagation.ConvLayer  —  Type Union type for convolutional layers. source"},{"id":95,"pagetitle":"API reference","title":"RelevancePropagation.PoolingLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.PoolingLayer","content":" RelevancePropagation.PoolingLayer  —  Type Union type for pooling layers. source"},{"id":96,"pagetitle":"API reference","title":"RelevancePropagation.DropoutLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.DropoutLayer","content":" RelevancePropagation.DropoutLayer  —  Type Union type for dropout layers. source"},{"id":97,"pagetitle":"API reference","title":"RelevancePropagation.ReshapingLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.ReshapingLayer","content":" RelevancePropagation.ReshapingLayer  —  Type Union type for reshaping layers such as  flatten . source"},{"id":98,"pagetitle":"API reference","title":"RelevancePropagation.NormalizationLayer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.NormalizationLayer","content":" RelevancePropagation.NormalizationLayer  —  Type Union type for normalization layers. source"},{"id":99,"pagetitle":"API reference","title":"Composite presets","ref":"/XAIDocs/RelevancePropagation/stable/api/#api-composite-presets","content":" Composite presets"},{"id":100,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonGammaBox","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonGammaBox","content":" RelevancePropagation.EpsilonGammaBox  —  Function EpsilonGammaBox(low, high; [epsilon=1.0f-6, gamma=0.25f0]) Composite using the following primitives: julia> EpsilonGammaBox(-3.0f0, 3.0f0)\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.ConvTranspose      => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.CrossCor           => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.ConvTranspose => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.CrossCor      => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n ),\n) source"},{"id":101,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonPlus","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonPlus","content":" RelevancePropagation.EpsilonPlus  —  Function EpsilonPlus(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonPlus()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n) source"},{"id":102,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonAlpha2Beta1","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonAlpha2Beta1","content":" RelevancePropagation.EpsilonAlpha2Beta1  —  Function EpsilonAlpha2Beta1(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonAlpha2Beta1()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n) source"},{"id":103,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonPlusFlat","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonPlusFlat","content":" RelevancePropagation.EpsilonPlusFlat  —  Function EpsilonPlusFlat(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonPlusFlat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n) source"},{"id":104,"pagetitle":"API reference","title":"RelevancePropagation.EpsilonAlpha2Beta1Flat","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.EpsilonAlpha2Beta1Flat","content":" RelevancePropagation.EpsilonAlpha2Beta1Flat  —  Function EpsilonAlpha2Beta1Flat(; [epsilon=1.0f-6]) Composite using the following primitives: julia> EpsilonAlpha2Beta1Flat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n) source"},{"id":105,"pagetitle":"API reference","title":"Custom rules","ref":"/XAIDocs/RelevancePropagation/stable/api/#Custom-rules","content":" Custom rules These utilities can be used to define custom rules without writing boilerplate code. To extend these functions, explicitly  import  them: "},{"id":106,"pagetitle":"API reference","title":"RelevancePropagation.modify_input","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_input","content":" RelevancePropagation.modify_input  —  Function modify_input(rule, input) Modify input activation before computing relevance propagation. source"},{"id":107,"pagetitle":"API reference","title":"RelevancePropagation.modify_denominator","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_denominator","content":" RelevancePropagation.modify_denominator  —  Function modify_denominator(rule, d) Modify denominator  $z$  for numerical stability on the forward pass. source"},{"id":108,"pagetitle":"API reference","title":"RelevancePropagation.modify_parameters","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_parameters","content":" RelevancePropagation.modify_parameters  —  Function modify_parameters(rule, parameter) Modify parameters before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":109,"pagetitle":"API reference","title":"RelevancePropagation.modify_weight","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_weight","content":" RelevancePropagation.modify_weight  —  Function modify_weight(rule, weight) Modify layer weights before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":110,"pagetitle":"API reference","title":"RelevancePropagation.modify_bias","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_bias","content":" RelevancePropagation.modify_bias  —  Function modify_bias(rule, bias) Modify layer bias before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":111,"pagetitle":"API reference","title":"RelevancePropagation.modify_layer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.modify_layer","content":" RelevancePropagation.modify_layer  —  Function modify_layer(rule, layer) Modify layer before computing the relevance. Note Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ source"},{"id":112,"pagetitle":"API reference","title":"RelevancePropagation.is_compatible","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.is_compatible","content":" RelevancePropagation.is_compatible  —  Function is_compatible(rule, layer) Check compatibility of a LRP-Rule with layer type. source Compatibility settings:"},{"id":113,"pagetitle":"API reference","title":"RelevancePropagation.LRP_CONFIG.supports_layer","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP_CONFIG.supports_layer","content":" RelevancePropagation.LRP_CONFIG.supports_layer  —  Function LRP_CONFIG.supports_layer(layer) Check whether LRP can be used on a layer or a Chain. To extend LRP to your own layers, define: LRP_CONFIG.supports_layer(::MyLayer) = true          # for structs\nLRP_CONFIG.supports_layer(::typeof(mylayer)) = true  # for functions source"},{"id":114,"pagetitle":"API reference","title":"RelevancePropagation.LRP_CONFIG.supports_activation","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.LRP_CONFIG.supports_activation","content":" RelevancePropagation.LRP_CONFIG.supports_activation  —  Function LRP_CONFIG.supports_activation(σ) Check whether LRP can be used on a given activation function. To extend LRP to your own activation functions, define: LRP_CONFIG.supports_activation(::typeof(myactivation)) = true  # for functions\nLRP_CONFIG.supports_activation(::MyActivation) = true          # for structs source"},{"id":115,"pagetitle":"API reference","title":"CRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#CRP","content":" CRP"},{"id":116,"pagetitle":"API reference","title":"RelevancePropagation.CRP","ref":"/XAIDocs/RelevancePropagation/stable/api/#RelevancePropagation.CRP","content":" RelevancePropagation.CRP  —  Type CRP(lrp_analyzer, layer, features) Use Concept Relevance Propagation to explain the output of a neural network with respect to specific features in a given layer. Arguments lrp_analyzer::LRP : LRP analyzer layer::Int : Index of layer after which the concept is located features : Concept / feature to explain. See also  TopNFeatures  and  IndexedFeatures . References [1] R. Achtibat et al., From attribution maps to human-understandable explanations     through Concept Relevance Propagation source"},{"id":117,"pagetitle":"API reference","title":"XAIBase.TopNFeatures","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.TopNFeatures","content":" XAIBase.TopNFeatures  —  Type TopNFeatures(n) Select top-n features. For outputs of convolutional layers, the relevance is summed across height and width channels for each feature. See also  IndexedFeatures . source"},{"id":118,"pagetitle":"API reference","title":"XAIBase.IndexedFeatures","ref":"/XAIDocs/RelevancePropagation/stable/api/#XAIBase.IndexedFeatures","content":" XAIBase.IndexedFeatures  —  Type IndexedFeatures(indices...) Select features by indices. For outputs of convolutional layers, the index refers to a feature dimension. See also See also  TopNFeatures . source"},{"id":119,"pagetitle":"API reference","title":"Index","ref":"/XAIDocs/RelevancePropagation/stable/api/#Index","content":" Index RelevancePropagation.AlphaBetaRule RelevancePropagation.CRP RelevancePropagation.ChainTuple RelevancePropagation.Composite RelevancePropagation.ConvLayer RelevancePropagation.DropoutLayer RelevancePropagation.EpsilonRule RelevancePropagation.FirstLayerMap RelevancePropagation.FirstLayerTypeMap RelevancePropagation.FirstNTypeMap RelevancePropagation.FlatRule RelevancePropagation.GammaRule RelevancePropagation.GeneralizedGammaRule RelevancePropagation.GlobalMap RelevancePropagation.GlobalTypeMap RelevancePropagation.LRP RelevancePropagation.LastLayerMap RelevancePropagation.LastLayerTypeMap RelevancePropagation.LayerMap RelevancePropagation.NormalizationLayer RelevancePropagation.ParallelTuple RelevancePropagation.PassRule RelevancePropagation.PoolingLayer RelevancePropagation.RangeMap RelevancePropagation.RangeTypeMap RelevancePropagation.ReshapingLayer RelevancePropagation.WSquareRule RelevancePropagation.ZBoxRule RelevancePropagation.ZPlusRule RelevancePropagation.ZeroRule XAIBase.IndexedFeatures XAIBase.TopNFeatures RelevancePropagation.EpsilonAlpha2Beta1 RelevancePropagation.EpsilonAlpha2Beta1Flat RelevancePropagation.EpsilonGammaBox RelevancePropagation.EpsilonPlus RelevancePropagation.EpsilonPlusFlat RelevancePropagation.LRP_CONFIG.supports_activation RelevancePropagation.LRP_CONFIG.supports_layer RelevancePropagation.canonize RelevancePropagation.flatten_model RelevancePropagation.is_compatible RelevancePropagation.lrp_rules RelevancePropagation.modify_bias RelevancePropagation.modify_denominator RelevancePropagation.modify_input RelevancePropagation.modify_layer RelevancePropagation.modify_parameters RelevancePropagation.modify_weight RelevancePropagation.show_layer_indices RelevancePropagation.strip_softmax"},{"id":122,"pagetitle":"Developer documentation","title":"LRP developer documentation","ref":"/XAIDocs/RelevancePropagation/stable/developer/#lrp-dev-docs","content":" LRP developer documentation"},{"id":123,"pagetitle":"Developer documentation","title":"Generic LRP rule implementation","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Generic-LRP-rule-implementation","content":" Generic LRP rule implementation Before we dive into package-specific implementation details  in later sections of this developer documentation,  we first need to cover some fundamentals of LRP, starting with our notation. The generic LRP rule, of which the  $0$ -,  $\\epsilon$ - and  $\\gamma$ -rules are special cases, reads [1] [2] \\[\\begin{equation}\nR_j^k = \\sum_i \\frac{\\rho(W_{ij}) \\; a_j^k}{\\epsilon + \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i)} R_i^{k+1}\n\\end{equation}\\] where  $W$  is the weight matrix of the layer $b$  is the bias vector of the layer $a^k$  is the activation vector at the input of layer  $k$ $a^{k+1}$  is the activation vector at the output of layer  $k$ $R^k$  is the relevance vector at the input of layer  $k$ $R^{k+1}$  is the relevance vector at the output of layer  $k$ $\\rho$  is a function that modifies parameters (what we call  modify_parameters ) $\\epsilon$  is a small positive constant to avoid division by zero Subscript characters are used to index vectors and matrices  (e.g.  $b_i$  is the  $i$ -th entry of the bias vector),  while the superscripts  $^k$  and  $^{k+1}$   indicate the relative positions of activations  $a$  and relevances  $R$  in the model. For any  $k$ ,  $a^k$  and  $R^k$  have the same shape.  Note that every term in this equation is a scalar value, which removes the need to differentiate between matrix and element-wise operations."},{"id":124,"pagetitle":"Developer documentation","title":"Linear layers","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Linear-layers","content":" Linear layers LRP was developed for  deep rectifier networks , neural networks that are composed of linear layers with ReLU activation functions. Linear layers are layers that can be represented as affine transformations of the form  \\[\\begin{equation}\nf(x) = Wx + b \\quad .\n\\end{equation}\\] This includes most commonly used types of layers, such as fully connected layers,  convolutional layers, pooling layers, and normalization layers. We will now describe a generic implementation of equation (1)  that can be applied to any linear layer."},{"id":125,"pagetitle":"Developer documentation","title":"The automatic differentiation fallback","ref":"/XAIDocs/RelevancePropagation/stable/developer/#lrp-dev-ad-fallback","content":" The automatic differentiation fallback The computation of the generic LRP rule can be decomposed into four steps [1] : \\[\\begin{array}{lr}\nz_{i} = \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i) & \\text{(Step 1)} \\\\[0.5em]\ns_{i} = R_{i}^{k+1} / (z_{i} + \\epsilon)           & \\text{(Step 2)} \\\\[0.5em]\nc_{j} = \\sum_i \\rho(W_{ij}) \\; s_{i}               & \\text{(Step 3)} \\\\[0.5em]\nR_{j}^{k} = a_{j}^{k} c_{j}                        & \\text{(Step 4)}\n\\end{array}\\] To compute step 1 , we first create a modified layer,  applying  $\\rho$  to the weights and biases  and replacing the activation function with the identity function. The vector  $z$  is then computed using a forward pass through the modified layer. It has the same dimensionality as  $R^{k+1}$  and  $a^{k+1}$ . Step 2  is an element-wise division of  $R^{k+1}$  by  $z$ . To avoid division by zero, a small constant  $\\epsilon$  is added to  $z$  when necessary. Step 3  is trivial for fully connected layers,  as  $\\rho(W)$  corresponds to the weight matrix of the modified layer. For other types of linear layers, however, the implementation is more involved: A naive approach would be to construct a large matrix  $W$  that corresponds to the affine transformation  $Wx+b$  implemented by the modified layer. This has multiple drawbacks: the implementation is error-prone a separate implementation is required for each type of linear layer for some layer types, e.g. pooling layers, the matrix  $W$  depends on the input for many layer types, e.g. convolutional layers,  the matrix  $W$  is very large and sparse, mostly consisting of zeros, leading to a large computational overhead A better approach can be found by observing that the matrix  $W$  is the Jacobian of the affine transformation  $f(x) = Wx + b$ . The vector  $c$  computed in step 3 corresponds to  $c = s^T W$ , a so-called  Vector-Jacobian-Product  (VJP) of the vector  $s$  with the Jacobian  $W$ .  VJPs are the fundamental building blocks of reverse-mode automatic differentiation (AD), and therefore implemented by most AD frameworks in a highly performant, matrix-free, GPU-accelerated manner. Note that computing the VJP is much more efficient than first computing the full Jacobian  $W$  and later multiplying it with  $s$ .  This is due to the fact that computing the full Jacobian of a function   $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$  requires computing  $m$  VJPs. Functions that compute VJP's are commonly called  pullbacks . Using the  Zygote.jl  AD system, we obtain the output  $z$  of a modified layer and its pullback  back  in a single function call: z, back = Zygote.pullback(modified_layer, aᵏ) We then call the pullback with the vector  $s$  to obtain  $c$ : c = back(s) Finally, step 4  consists of an element-wise multiplication of the vector  $c$   with the input activation vector  $a^k$ , resulting in the relevance vector  $R^k$ . This AD-based implementation is used in RelevancePropagation.jl as the default method for all layer types that don't have a more optimized implementation (e.g. fully connected layers). We will refer to it as the  \"AD fallback\" . For more background information on automatic differentiation, refer to the   JuML lecture on AD ."},{"id":126,"pagetitle":"Developer documentation","title":"LRP analyzer struct","ref":"/XAIDocs/RelevancePropagation/stable/developer/#LRP-analyzer-struct","content":" LRP analyzer struct The  LRP  analyzer struct holds three fields: the  model  to analyze, the LRP  rules  to use, and pre-allocated  modified_layers . As described in the section on  Composites , applying a composite to a model will return LRP rules in nested  ChainTuple  and  ParallelTuple s. These wrapper types are used to match the structure of Flux models with  Chain  and  Parallel  layers while avoiding type piracy. When creating an  LRP  analyzer with the default keyword argument  flatten=true ,   flatten_model  is called on the model and rules. This is done for performance reasons, as discussed in   Flattening the model . After passing the  Model checks , modified layers are pre-allocated, once again using the  ChainTuple  and  ParallelTuple  wrapper types to match the structure of the model. If a rule doesn't modify a layer,  the corresponding entry in  modified_layers  is set to  nothing ,  avoiding unnecessary allocations.  If a rule requires multiple modified layers,  the corresponding entry in  modified_layers  is set to a named tuple of modified layers. Apart from these special cases,  the corresponding entry in  modified_layers  is simply set to the modified layer. For a detailed description of the layer modification mechanism, refer to the section on  Advanced layer modification ."},{"id":127,"pagetitle":"Developer documentation","title":"Forward and reverse pass","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Forward-and-reverse-pass","content":" Forward and reverse pass When calling an  LRP  analyzer, a forward pass through the model is performed, saving the activations  $aᵏ$  for all layers  $k$  in a vector called  as . This vector of activations is then used to pre-allocate the relevances  $R^k$   for all layers in a vector called  Rs . This is possible since for any layer  $k$ ,  $a^k$  and  $R^k$  have the same shape. Finally, the last array of relevances  $R^N$  in  Rs  is set to zeros,  except for the specified output neuron, which is set to one. We can now run the reverse pass, iterating backwards over the layers in the model and writing relevances  $R^k$  into the pre-allocated array  Rs : for k in length(model):-1:1\n    #                  └─ loop over layers in reverse\n    lrp!(Rs[k], rules[k], layers[k], modified_layers[k], as[k], Rs[k+1])\n    #    └─ Rᵏ: modified in-place                        └─ aᵏ  └─ Rᵏ⁺¹\nend This is done by calling low-level functions function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend that implement individual LRP rules. The correct rule is applied via   multiple dispatch  on the types of the arguments  rule  and  modified_layer . The relevance  Rᵏ  is then computed based on the input activation  aᵏ  and the output relevance  Rᵏ⁺¹ . The exclamation point in the function name  lrp!  is a   naming convention  in Julia to denote functions that modify their arguments –  in this case the first argument  Rs[k] , which corresponds to  $R^k$ ."},{"id":128,"pagetitle":"Developer documentation","title":"Rule calls","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Rule-calls","content":" Rule calls As discussed in  The AD fallback , the default LRP fallback for unknown layers uses AD via   Zygote . Now that you are familiar with both the API and the four-step computation of the generic LRP rules, the following implementation should be straightforward to understand: function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z, back = Zygote.pullback(modified_layer, ãᵏ)\n   s = Rᵏ⁺¹ ./ modify_denominator(rule, z)\n   Rᵏ .= ãᵏ .* only(back(s))\nend Not only  lrp!  dispatches on the rule and layer type,  but also the internal functions  modify_input  and  modify_denominator . Unknown layers that are registered in the  LRP_CONFIG  use this exact function. All LRP rules are implemented in the file  /src/rules.jl ."},{"id":129,"pagetitle":"Developer documentation","title":"Specialized implementations","ref":"/XAIDocs/RelevancePropagation/stable/developer/#Specialized-implementations","content":" Specialized implementations In other programming languages, LRP is commonly implemented in an object-oriented manner, providing a single backward pass implementation per rule. This can be seen as a form of  single dispatch  on the rule type. Using multiple dispatch, we can implement specialized versions of  lrp!  that not only take into account the rule type, but also the layer type,  for example for fully connected layers or reshaping layers.  Reshaping layers don't affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back: function lrp!(Rᵏ, rule, layer::ReshapingLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= reshape(Rᵏ⁺¹, size(aᵏ))\nend We can even provide a specialized implementation of the generic LRP rule for  Dense  layers. Since we can access the weight matrix directly, we can skip the use of automatic differentiation and implement the following equation directly, using Einstein summation notation: \\[R_j^k = \\sum_i \\frac{\\rho(W_{ij}) \\; a_j^k}{\\epsilon + \\sum_{l} \\rho(W_{il}) \\; a_l^k + \\rho(b_i)} R_i^{k+1}\\] function lrp!(Rᵏ, rule, layer::Dense, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z = modify_denominator(rule, layer(ãᵏ))\n\n   # Implement LRP using Einsum notation, where `b` is the batch index\n   @tullio Rᵏ[j, b] = layer.weight[i, j] * ãᵏ[j, b] / z[i, b] * Rᵏ⁺¹[i, b]\nend For maximum low-level control beyond  modify_input  and  modify_denominator , you can also implement your own  lrp!  function and dispatch on individual rule types  MyRule  and layer types  MyLayer : function lrp!(Rᵏ, rule::MyRule, layer::MyLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend 1 G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview 2 W. Samek et al.,  Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications"},{"id":132,"pagetitle":"Basic usage","title":"Basic usage of LRP","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-basics","content":" Basic usage of LRP Note This package is part the  Julia-XAI ecosystem . For an introduction to the ecosystem, please refer to the  Getting started  guide . We start out by loading a small convolutional neural network: using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16; pad=1),\n        BatchNorm(16, relu),\n        Conv((3, 3), 16 => 8, relu; pad=1),\n        BatchNorm(8, relu),\n    ),\n    Chain(Flux.flatten, Dense(2048 => 512, relu), Dropout(0.5), Dense(512 => 100, softmax)),\n); This model contains two chains: the convolutional layers and the fully connected layers."},{"id":133,"pagetitle":"Basic usage","title":"Model preparation","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-model-prep","content":" Model preparation TLDR Use  strip_softmax  to strip the output softmax from your model. Otherwise  model checks  will fail. Use  canonize  to fuse linear layers. Don't just call  LRP(model) , instead use a  Composite  to apply LRP rules to your model. Read  Assigning rules to layers . By default,  LRP  will call  flatten_model  to flatten your model. This reduces computational overhead."},{"id":134,"pagetitle":"Basic usage","title":"Stripping the output softmax","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-strip-softmax","content":" Stripping the output softmax When using LRP, it is recommended to explain output logits instead of probabilities. This can be done by stripping the output softmax activation from the model using the  strip_softmax  function: model = strip_softmax(model) Chain(\n  Chain(\n    Conv((3, 3), 3 => 8, relu, pad=1),   # 224 parameters \n    Conv((3, 3), 8 => 8, relu, pad=1),   # 584 parameters \n    MaxPool((2, 2)),\n    Conv((3, 3), 8 => 16, pad=1),        # 1_168 parameters \n    BatchNorm(16, relu),                 # 32 parameters, plus 32 \n    Conv((3, 3), 16 => 8, relu, pad=1),   # 1_160 parameters \n    BatchNorm(8, relu),                  # 16 parameters, plus 16 \n  ),\n  Chain(\n    Flux.flatten,\n    Dense(2048 => 512, relu),            # 1_049_088 parameters \n    Dropout(0.5),\n    Dense(512 => 100),                   # 51_300 parameters \n  ),\n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.213 MiB. If you don't remove the output softmax,  model checks  will fail."},{"id":135,"pagetitle":"Basic usage","title":"Canonizing the model","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-canonization","content":" Canonizing the model LRP is not invariant to a model's implementation. Applying the  GammaRule  to two linear layers in a row will yield different results than first fusing the two layers into one linear layer and then applying the rule. This fusing is called \"canonization\" and can be done using the  canonize  function: model_canonized = canonize(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),    # 1_168 parameters \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 14 trainable arrays,  1_103_540 parameters,\n           # plus 2 non-trainable, 16 parameters, summarysize  4.212 MiB. After canonization, the first  BatchNorm  layer has been fused into the preceding  Conv  layer. The second  BatchNorm  layer wasn't fused since its preceding  Conv  layer has a ReLU activation function."},{"id":136,"pagetitle":"Basic usage","title":"Flattening the model","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-flatten-model","content":" Flattening the model RelevancePropagation.jl's LRP implementation supports nested Flux Chains and Parallel layers. However, it is recommended to flatten the model before analyzing it. LRP is implemented by first running a forward pass through the model, keeping track of the intermediate activations, followed by a backward pass that computes the relevances. To keep the LRP implementation simple and maintainable, RelevancePropagation.jl does not pre-compute \"nested\" activations. Instead, for every internal chain, a new forward pass is run to compute activations. By \"flattening\" a model, this overhead can be avoided. For this purpose, RelevancePropagation.jl provides the function  flatten_model : model_flat = flatten_model(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),          # 1_168 parameters \n  BatchNorm(16, relu),                   # 32 parameters, plus 32 \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.212 MiB. This function is called by default when creating an LRP analyzer. Note that we pass the unflattened model to the analyzer, but  analyzer.model  is flattened: analyzer = LRP(model)\nanalyzer.model Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),          # 1_168 parameters \n  BatchNorm(16, relu),                   # 32 parameters, plus 32 \n  Conv((3, 3), 16 => 8, relu, pad=1),    # 1_160 parameters \n  BatchNorm(8, relu),                    # 16 parameters, plus 16 \n  Flux.flatten,\n  Dense(2048 => 512, relu),              # 1_049_088 parameters \n  Dropout(0.5),\n  Dense(512 => 100),                     # 51_300 parameters \n)          # Total: 16 trainable arrays,  1_103_572 parameters,\n           # plus 4 non-trainable, 48 parameters, summarysize  4.212 MiB. If this flattening is not desired, it can be disabled by passing the keyword argument  flatten=false  to the  LRP  constructor."},{"id":137,"pagetitle":"Basic usage","title":"LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#LRP-rules","content":" LRP rules By default, the  LRP  constructor will assign the  ZeroRule  to all layers. LRP(model) LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1)   =>  ZeroRule() ,\n  Conv((3, 3), 8 => 8, relu, pad=1)   =>  ZeroRule() ,\n  MaxPool((2, 2))                     =>  ZeroRule() ,\n  Conv((3, 3), 8 => 16, pad=1)        =>  ZeroRule() ,\n  BatchNorm(16, relu)                 =>  ZeroRule() ,\n  Conv((3, 3), 16 => 8, relu, pad=1)  =>  ZeroRule() ,\n  BatchNorm(8, relu)                  =>  ZeroRule() ,\n  Flux.flatten                        =>  ZeroRule() ,\n  Dense(2048 => 512, relu)            =>  ZeroRule() ,\n  Dropout(0.5)                        =>  ZeroRule() ,\n  Dense(512 => 100)                   =>  ZeroRule() ,\n) This analyzer will return heatmaps that look identical to the  InputTimesGradient  analyzer from  ExplainableAI.jl . LRP's strength lies in assigning different rules to different layers, based on their functionality in the neural network [1] . RelevancePropagation.jl  implements many LRP rules out of the box , but it is also possible to  implement custom rules . To assign different rules to different layers, use one of the  composites presets , or create your own composite, as described in  Assigning rules to layers . composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat Composite(\n  GlobalTypeMap(   # all layers \n     Flux.Conv                =>  ZPlusRule() ,\n     Flux.ConvTranspose       =>  ZPlusRule() ,\n     Flux.CrossCor            =>  ZPlusRule() ,\n     Flux.Dense               =>  EpsilonRule{Float32}(1.0f-6) ,\n     typeof(NNlib.dropout)    =>  PassRule() ,\n     Flux.AlphaDropout        =>  PassRule() ,\n     Flux.Dropout             =>  PassRule() ,\n     Flux.BatchNorm           =>  PassRule() ,\n     typeof(Flux.flatten)     =>  PassRule() ,\n     typeof(MLUtils.flatten)  =>  PassRule() ,\n     typeof(identity)         =>  PassRule() ,\n ),\n  FirstLayerTypeMap(   # first layer \n     Flux.Conv           =>  FlatRule() ,\n     Flux.ConvTranspose  =>  FlatRule() ,\n     Flux.CrossCor       =>  FlatRule() ,\n ),\n) LRP(model, composite) LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1)   =>  FlatRule() ,\n  Conv((3, 3), 8 => 8, relu, pad=1)   =>  ZPlusRule() ,\n  MaxPool((2, 2))                     =>  ZeroRule() ,\n  Conv((3, 3), 8 => 16, pad=1)        =>  ZPlusRule() ,\n  BatchNorm(16, relu)                 =>  PassRule() ,\n  Conv((3, 3), 16 => 8, relu, pad=1)  =>  ZPlusRule() ,\n  BatchNorm(8, relu)                  =>  PassRule() ,\n  Flux.flatten                        =>  PassRule() ,\n  Dense(2048 => 512, relu)            =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dropout(0.5)                        =>  PassRule() ,\n  Dense(512 => 100)                   =>  EpsilonRule{Float32}(1.0f-6) ,\n)"},{"id":138,"pagetitle":"Basic usage","title":"Computing layerwise relevances","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-layerwise","content":" Computing layerwise relevances If you are interested in computing layerwise relevances, call  analyze  with an LRP analyzer and the keyword argument  layerwise_relevances=true . The layerwise relevances can be accessed in the  extras  field of the returned  Explanation : input = rand(Float32, 32, 32, 3, 1) # dummy input for our convolutional neural network\n\nexpl = analyze(input, analyzer; layerwise_relevances=true)\nexpl.extras.layerwise_relevances (Float32[8.327308 -7.83097 … 20.22971 -3.7166505; 8.963654 -5.9993386 … -2.928864 -3.4936154; … ; 1.2683306 3.3820424 … 24.60917 -7.21482; 15.214648 -8.884898 … 2.6456695 -10.390884;;; 4.011874 -4.0590324 … -4.616354 3.9734259; -2.9656606 6.912936 … -13.802339 5.239159; … ; -6.3620853 -2.6684556 … -26.411573 -0.39939696; -3.799637 0.24409898 … -0.08097203 1.577045;;; 2.1186552 8.112977 … 12.724606 -8.0426; 1.0737268 1.3032237 … 0.3346576 -28.026342; … ; 15.932969 -26.070225 … -5.7292414 2.5399013; -5.9140425 -0.0013468717 … -1.5723354 5.2093186;;;;], Float32[-0.0 -2.7200859 … -0.0 -0.5623542; 0.14131893 -0.0 … -0.0 8.032345; … ; -15.622283 -0.0 … 0.7954757 -2.2643433; 0.0 -0.0 … 0.0 2.634971;;; -0.0 -0.0 … -0.0 0.7987479; -0.0 -0.0 … 0.0 -2.5468924; … ; 2.3494885 -0.0 … -0.0 2.8960185; 3.7750728 1.3592004 … -1.4962444 -0.0;;; 0.0 -0.95559543 … -0.0 -0.0; -0.0 9.400769 … 0.0 -0.0; … ; -0.0 -0.78302914 … -0.0 0.0; -0.0 0.0 … 0.0 0.74188423;;; 0.0 0.0 … 0.0 0.0; -0.0 0.0 … 0.0 -0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.0;;; -0.0 0.0 … 0.0 0.0; -0.0 0.0 … -0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; 0.0 -0.0 … -0.0 -0.0; 1.6911445 0.0 … 0.0 4.1343575; … ; 2.2103014 0.0 … 0.0 -4.2544937; -0.77148944 -0.0 … -0.0 -0.0;;; -0.16688322 0.0 … 0.0 -2.8568013; -0.9833772 1.6663531 … -0.0 3.2834246; … ; 7.074368 -0.0 … -0.0 10.086847; 0.0 15.6836405 … -0.0 -0.0;;; 0.61481476 -2.24196 … -4.2152233 0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 0.0 … -0.0 -0.0;;;;], Float32[0.0 4.57914 … -6.3799706 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … -3.847128 0.0; 0.0 -2.757442 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 2.9550445 … 0.8068923 0.0; … ; 0.0 0.0 … -25.800358 0.0; -8.005194 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 12.121355; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 5.191336; 0.0 17.847282 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 5.0604405 0.0; … ; 0.0 0.0 … 8.109103 0.0; 8.328043 0.0 … 0.0 0.0;;; 0.0 4.795353 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 -3.7019901; 0.27488425 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; -3.6776745 0.0 … 0.0 1.2166417; … ; 0.0 0.0 … 2.3930213 0.0; -2.4171185 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … -0.4562028 0.0;;; 0.0 0.0 … 0.0 1.665124; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 1.1807004; 0.0 -3.801413 … 0.0 0.0;;;;], Float32[4.57914 2.0232563 … -0.0 -6.3799706; 4.5803576 5.656276 … 11.23432 -22.163029; … ; 4.842777 5.1135373 … 6.1951456 -18.28472; -2.757442 -8.9625225 … -8.651633 -3.8471284;;; 2.9550445 3.6525686 … -0.7383915 0.8068923; -0.0 -34.63811 … -6.431445 -17.909248; … ; -0.57381976 -21.939602 … -19.713642 -31.770304; -8.005194 -34.281742 … -39.40382 -25.800358;;; 0.0 5.3098335 … 0.0 12.121355; 0.16628794 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 17.847282 0.0 … 18.527046 5.191336;;; 0.0 1.1742975 … 2.7663443 5.0604405; 0.07974223 7.609131 … 4.751021 -0.0; … ; 3.7229264 11.253115 … 2.3452513 2.4331777; 8.328043 25.327337 … 13.882841 8.109103;;; 4.795353 -0.0 … 0.0 0.0; 0.0 -0.0 … 0.0 -0.0; … ; 0.0 -0.0 … -0.0 -0.0; 0.2748843 0.0 … 0.39339855 -3.7019901;;; -3.6776745 -1.5233933 … -1.7027856 1.2166418; 9.393424 10.514384 … 43.98021 12.046564; … ; 8.180141 6.774875 … 18.244902 27.00564; -2.4171185 4.886017 … 3.4954503 2.3930216;;; 0.0 1.0023761 … 0.0 0.0; 4.5314546 0.0 … 0.0 -0.0; … ; 3.8853288 0.0 … 20.68431 -0.3025496; 0.0 1.333253 … 0.4195981 -0.45620283;;; 0.0 0.8607781 … 0.0 1.6651243; -0.99727976 0.4735456 … 2.546403 17.9714; … ; -1.9010018 1.2183529 … 1.2070284 8.317717; -3.801413 -0.32261914 … 0.060320973 1.1807005;;;;], Float32[-0.0006117489 0.00021519868 … -0.000906484 1.0315989f-5; -0.00051562104 -0.00095367385 … -0.0009620695 -0.014612194; … ; 0.001337966 -0.00033287704 … -7.070966f-5 -0.00071956916; -0.00096954807 0.0012422366 … -0.0007115063 0.0007381511;;; -0.0051851366 -0.007856156 … 0.008978085 -0.0072933338; -0.00012250224 -0.006060506 … 0.026576811 -0.002645589; … ; -0.0038863965 0.0032866872 … -0.001181394 -0.005540036; 0.006715212 -0.004037242 … -0.007653008 -0.0010693887;;; -0.0011274469 -0.0014640973 … -0.0018788398 -0.0013461235; -0.00065485854 -0.001478855 … 0.0015346803 -0.0017216321; … ; 0.005715894 -0.0019151388 … -0.0015882591 -0.0018423795; -0.0011445136 -0.0011259448 … -0.0002880264 -0.0013962964;;; … ;;; -14.022386 -51.09481 … -17.470604 -5.585806; -4.1865044 -21.117615 … 6.8632774 25.058342; … ; 11.725198 15.252797 … -10.581841 -30.092524; 13.584241 10.497882 … 51.16966 59.64862;;; -0.13575593 0.115785345 … -35.193317 -0.15976667; -0.0034891113 -0.08571102 … -0.41366 -0.18013765; … ; -0.14947948 1.3195324 … 0.103383444 -0.8149192; -0.7908136 0.079687156 … -0.44392246 0.10112273;;; 0.0019876533 -0.0031273647 … -0.06349881 -0.00040097107; 0.0018098768 0.027225392 … 0.0018558621 -0.12330594; … ; -0.21457447 1.425 … -0.04649991 -0.10824808; 0.0015909689 -0.078088574 … -0.10634443 -0.088618174;;;;], Float32[0.0 -0.0 … 0.0 0.00014444557; 0.0 0.0 … 0.0 -0.0396553; … ; -0.0 0.0 … -0.0 -0.0; -0.0 0.0 … 0.0 -0.0;;; -0.0 -0.0 … 0.0 -0.0; -0.0 0.0 … 0.0 -0.0; … ; 0.0 0.0 … 0.0 -0.0; -0.023557935 -0.0 … 0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … 0.0 0.0; … ; 0.0 0.0 … -0.0 -0.0; -0.0 -0.0 … 0.0 0.0;;; … ;;; 0.0 -0.0 … -0.0 0.0; -0.0 0.0 … 0.004188955 0.08219464; … ; 0.115124375 0.015430466 … 0.0 0.0; -0.007304755 -0.07837329 … 0.17979364 0.093243584;;; -0.0 -0.015859297 … -0.050240967 -0.03634484; -0.0 0.016877607 … 0.03675985 -0.09808747; … ; 0.0 0.2665269 … -0.11570973 -0.1896007; -0.0 -0.041975457 … -0.0 -0.029947914;;; 0.0072853933 0.0 … 0.0 -0.0; 0.009273345 0.045343313 … -0.008227661 -0.0377172; … ; -0.115657285 0.08448729 … -0.0 -0.052653473; 0.02609525 -0.027460065 … -0.051418282 0.020150276;;;;], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.0037998108; … ; 0.00969996 -0.0068425178 … 0.0 0.0; -0.0029727046 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 -0.0003948384; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.001353812; 0.0 -0.009965459 … 0.0 0.0;;; 0.0 0.011332786 … 0.0 0.0; 0.0 0.0 … 0.0 0.01981112; … ; 0.02070136 0.10034685 … 0.16956423 0.08017855; 0.0 0.0969891 … 0.08950577 0.033947203;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0007155704 0.0; … ; 0.14044715 0.22864783 … 0.0 0.0256909; -0.35980445 0.076369286 … 0.08120247 0.03137468;;; 0.0 0.0 … 0.0 0.0; 0.0 0.002646107 … 0.021507844 0.0; … ; 0.04508819 0.097512625 … 0.026110465 0.0; 0.045098875 -0.096956596 … 0.09794691 0.022801066;;; -0.017484874 -0.021590482 … 0.0033609718 -0.10531239; -0.0003285319 0.040350176 … 0.048364475 -0.0009820143; … ; -0.0 -0.0 … 0.01886814 0.013649188; 0.07851025 -0.0 … 0.03781729 -0.00042505015;;; -0.0 -0.0 … -0.0 -0.0; -0.0192193 0.012569417 … -0.003137269 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.00012417234 … -0.029398102 -0.004092352;;; 0.025973804 -0.0 … -0.018073926 -0.0010227859; 0.018205665 -0.011601795 … 0.010633885 -0.0; … ; 0.018103514 -0.0 … -0.0 -0.0; -0.026933072 -0.0 … 0.031375717 -0.022076707;;;;], Float32[0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0039723334; … ; 0.008767475 -0.008078173 … -0.0 -0.0; -0.0028092365 -0.0 … -0.0 0.0;;; -0.0 -0.0 … -0.0 1.33902495f-5; -0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0008733339; -0.0 -0.008959949 … 0.0 0.0;;; 0.0 -0.0026320005 … 0.0 -0.0; -0.0 0.0 … 0.0 -0.021781597; … ; -0.0076322416 0.008802434 … 0.031181267 -0.018381499; 0.0 -0.0060957703 … 0.0017477524 -0.004788824;;; -0.0 0.0 … 0.0 -0.0; 0.0 -0.0 … 6.4086846f-5 0.0; … ; 0.007395381 0.022416104 … 0.0 0.0008290763; -0.0066261915 0.0067268224 … 0.004018948 -0.0033523412;;; -0.0 -0.0 … 0.0 0.0; 0.0 -0.00031485877 … -0.012030382 0.0; … ; 0.0038220088 0.026231606 … -1.8129813f-5 0.0; 0.0041472283 -0.035086147 … 0.031724554 0.010119597;;; -0.0014256705 -0.012816689 … 0.0029711064 -0.0044658766; -0.0025405234 0.015641585 … 0.029247666 0.0032603152; … ; 0.0 -0.0 … 0.004178582 0.011036619; 0.0036167072 0.0 … 0.024250142 -0.00523355;;; -0.0 0.0 … 0.0 -0.0; -0.0045481087 0.011536867 … 0.0020235414 0.0; … ; 0.0 0.0 … -0.0 0.0; 0.0 -0.0009167506 … -0.016523432 -0.0012255838;;; 0.0030295074 -0.0 … -0.0015426872 -0.0055334033; 0.010720322 -0.0016994367 … 0.0030408464 0.0; … ; 0.013795362 -0.0 … -0.0 -0.0; -0.012482019 -0.0 … 0.030494832 -0.007282867;;;;], Float32[0.0; 0.0; … ; -0.0; -0.007282867;;], Float32[-0.0; 0.0; … ; 0.0; -0.0;;], Float32[-0.0; 0.0; … ; 0.0; -0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;]) Note that the layerwise relevances are only kept for layers in the outermost  Chain  of the model. When using our unflattened model, we only obtain three layerwise relevances, one for each chain in the model and the output relevance: analyzer = LRP(model; flatten=false) # use unflattened model\n\nexpl = analyze(input, analyzer; layerwise_relevances=true)\nexpl.extras.layerwise_relevances (Float32[0.1960582 -0.15411384 … -11.57062 -2.011442; 0.01880353 -0.003916721 … 3.40788 4.557889; … ; 1.5249896 0.09706643 … 0.035254557 -0.12084636; -1.1756896 -0.42424256 … 0.020654567 -0.4876383;;; -0.2775201 -0.0955955 … 1.8459821 8.416919; -0.00021444712 -0.026219355 … -10.091517 -0.5518057; … ; 1.3808483 -0.10030223 … -0.20419 -0.014242577; 0.14297286 0.00085867266 … 0.098073766 -0.029015189;;; -0.3122843 0.35415983 … 9.78052 -18.570946; -0.09579516 0.25360116 … 0.42155766 -2.7034986; … ; -1.5503724 -0.8221216 … -0.45758134 0.23653653; 0.24396542 -0.0048256963 … -0.48013076 0.16992503;;;;], Float32[0.0 0.0 … 0.0 -0.0; 0.0 0.0 … -0.0 -0.0031959377; … ; 0.010020404 -0.005846454 … -0.0 0.0; -0.0038638408 -0.0 … -0.0 0.0;;; -0.0 -0.0 … -0.0 -0.0004997425; -0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0010762725; -0.0 -0.01216915 … 0.0 0.0;;; 0.0 -0.0024901559 … 0.0 -0.0; -0.0 0.0 … -0.0 -0.023070497; … ; -0.0075005265 0.009888828 … 0.026142605 -0.011985789; 0.0 -0.014241133 … 0.0041673067 -0.0026029942;;; -0.0 -0.0 … 0.0 -0.0; 0.0 -0.0 … 0.0 0.0; … ; 0.0089091705 0.023063825 … 0.0 0.00068108604; -0.0063524125 0.01003483 … 0.00521952 -0.0027068933;;; -0.0 -0.0 … 0.0 0.0; 0.0 -0.0 … -0.010595257 0.0; … ; 0.002217615 0.023121541 … 0.008180201 0.0; 0.0042005447 -0.038086157 … 0.040440984 0.0114883175;;; -0.0028496794 -0.007859429 … 0.00017290808 -0.0062207673; -0.0043729884 0.007264631 … 0.027005874 0.0029822104; … ; 0.0 -0.0 … 0.0012533697 0.016646156; 0.004351355 0.0 … 0.030371943 -0.0054110815;;; -0.0 0.0 … 0.0 -0.0; -0.0040852036 0.009619381 … 0.00022223328 0.0; … ; 0.0 0.0 … -0.0 0.0; 0.0 -0.0 … -0.018390564 -0.0058159726;;; 0.00344227 -0.0 … -0.0018052001 -0.003402148; 0.015944706 -0.0021163563 … 0.0022007816 0.0; … ; 0.007882758 -0.0 … -0.0 -0.0; -0.020930655 -0.0 … 0.031153943 -0.006764625;;;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;])"},{"id":139,"pagetitle":"Basic usage","title":"Performance tips","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#docs-lrp-performance","content":" Performance tips"},{"id":140,"pagetitle":"Basic usage","title":"Using LRP with a GPU","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#gpu-docs","content":" Using LRP with a GPU All LRP analyzers support GPU backends, building on top of  Flux.jl's GPU support . Using a GPU only requires moving the input array and model weights to the GPU. For example, using  CUDA.jl : using CUDA, cuDNN\nusing Flux\nusing ExplainableAI\n\n# move input array and model weights to GPU\ninput = input |> gpu # or gpu(input)\nmodel = model |> gpu # or gpu(model)\n\n# analyzers don't require calling `gpu`\nanalyzer = LRP(model)\n\n# explanations are computed on the GPU\nexpl = analyze(input, analyzer) Some operations, like saving, require moving explanations back to the CPU. This can be done using Flux's  cpu  function: val = expl.val |> cpu # or cpu(expl.val)\n\nusing BSON\nBSON.@save \"explanation.bson\" val"},{"id":141,"pagetitle":"Basic usage","title":"Using LRP without a GPU","ref":"/XAIDocs/RelevancePropagation/stable/generated/basics/#Using-LRP-without-a-GPU","content":" Using LRP without a GPU Using Julia's package extension mechanism, RelevancePropagation.jl's LRP implementation can optionally make use of  Tullio.jl  and  LoopVectorization.jl  for faster LRP rules on dense layers. This only requires loading the packages before loading RelevancePropagation.jl: using LoopVectorization, Tullio\nusing RelevancePropagation This page was generated using  Literate.jl . 1 G. Montavon et al.,  Layer-Wise Relevance Propagation: An Overview"},{"id":144,"pagetitle":"Assigning rules to layers","title":"Assigning LRP rules to layers","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#docs-composites","content":" Assigning LRP rules to layers In this example, we will show how to assign LRP rules to specific layers. For this purpose, we first define a small VGG-like convolutional neural network: using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16, relu; pad=1),\n        Conv((3, 3), 16 => 16, relu; pad=1),\n        MaxPool((2, 2)),\n    ),\n    Chain(Flux.flatten, Dense(1024 => 512, relu), Dropout(0.5), Dense(512 => 100, relu)),\n);"},{"id":145,"pagetitle":"Assigning rules to layers","title":"Manually assigning rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#docs-composites-manual","content":" Manually assigning rules When creating an LRP-analyzer, we can assign individual rules to each layer. As we can see above, our model is a  Chain  of two Flux  Chain s. Using  flatten_model , we can flatten the model into a single  Chain : model_flat = flatten_model(model) Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),     # 224 parameters \n  Conv((3, 3), 8 => 8, relu, pad=1),     # 584 parameters \n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),    # 1_168 parameters \n  Conv((3, 3), 16 => 16, relu, pad=1),   # 2_320 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(1024 => 512, relu),              # 524_800 parameters \n  Dropout(0.5),\n  Dense(512 => 100, relu),               # 51_300 parameters \n)                    # Total: 12 arrays,  580_396 parameters, 2.216 MiB. This allows us to define an LRP analyzer using an array of rules matching the length of the Flux chain: rules = [\n    FlatRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    ZPlusRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    PassRule(),\n    EpsilonRule(),\n    PassRule(),\n    EpsilonRule(),\n]; The  LRP  analyzer will show a summary of how layers and rules got matched: LRP(model_flat, rules) LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n  Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n  MaxPool((2, 2))                      =>  ZeroRule() ,\n  Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n  Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n  MaxPool((2, 2))                      =>  ZeroRule() ,\n  Flux.flatten                         =>  PassRule() ,\n  Dense(1024 => 512, relu)             =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dropout(0.5)                         =>  PassRule() ,\n  Dense(512 => 100, relu)              =>  EpsilonRule{Float32}(1.0f-6) ,\n) However, this approach only works for models that can be fully flattened. For unflattened models and models containing  Parallel  layers, we can compose rules using  ChainTuple s and  ParallelTuple s which match the model structure: rules = ChainTuple(\n    ChainTuple(FlatRule(), ZPlusRule(), ZeroRule(), ZPlusRule(), ZPlusRule(), ZeroRule()),\n    ChainTuple(PassRule(), EpsilonRule(), PassRule(), EpsilonRule()),\n)\n\nanalyzer = LRP(model, rules; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) Keyword argument `flatten` We used the  LRP  keyword argument  flatten=false  to showcase that the structure of the model can be preserved. For performance reasons, the default  flatten=true  is recommended."},{"id":146,"pagetitle":"Assigning rules to layers","title":"Custom composites","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#docs-composites-custom","content":" Custom composites Instead of manually defining a list of rules, we can also define a  Composite . A composite constructs a list of LRP-rules by sequentially applying the  composite primitives  it contains. To obtain the same set of rules as in the previous example, we can define composite = Composite(\n    GlobalTypeMap( # the following maps of layer types to LRP rules are applied globally\n        Conv                 => ZPlusRule(),   # apply ZPlusRule on all Conv layers\n        Dense                => EpsilonRule(), # apply EpsilonRule on all Dense layers\n        Dropout              => PassRule(),    # apply PassRule on all Dropout layers\n        MaxPool              => ZeroRule(),    # apply ZeroRule on all MaxPool layers\n        typeof(Flux.flatten) => PassRule(),    # apply PassRule on all flatten layers\n    ),\n    FirstLayerMap( # the following rule is applied to the first layer\n        FlatRule(),\n    ),\n); We now construct an LRP analyzer from  composite analyzer = LRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) As you can see, this analyzer contains the same rules as our previous one. To compute rules for a model without creating an analyzer, use  lrp_rules : lrp_rules(model, composite) ChainTuple(\n  ChainTuple(\n    FlatRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    ZPlusRule(),\n    ZPlusRule(),\n    ZeroRule(),\n  ),\n  ChainTuple(\n    PassRule(),\n    EpsilonRule{Float32}(1.0f-6),\n    PassRule(),\n    EpsilonRule{Float32}(1.0f-6),\n  ),\n)\n"},{"id":147,"pagetitle":"Assigning rules to layers","title":"Composite primitives","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#Composite-primitives","content":" Composite primitives The following  Composite primitives  can used to construct a  Composite . To apply a single rule, use: LayerMap  to apply a rule to a layer at a given index GlobalMap  to apply a rule to all layers RangeMap  to apply a rule to a positional range of layers FirstLayerMap  to apply a rule to the first layer LastLayerMap  to apply a rule to the last layer To apply a set of rules to layers based on their type, use: GlobalTypeMap  to apply a dictionary that maps layer types to LRP-rules RangeTypeMap  for a  TypeMap  on generalized ranges FirstLayerTypeMap  for a  TypeMap  on the first layer of a model LastLayerTypeMap  for a  TypeMap  on the last layer FirstNTypeMap  for a  TypeMap  on the first  n  layers Primitives are called sequentially in the order the  Composite  was created with and overwrite rules specified by previous primitives."},{"id":148,"pagetitle":"Assigning rules to layers","title":"Assigning a rule to a specific layer","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#Assigning-a-rule-to-a-specific-layer","content":" Assigning a rule to a specific layer To assign a rule to a specific layer, we can use  LayerMap , which maps an LRP-rule to all layers in the model at the given index. To display indices, use the  show_layer_indices  helper function: show_layer_indices(model) ChainTuple(\n  ChainTuple(\n    (1, 1),\n    (1, 2),\n    (1, 3),\n    (1, 4),\n    (1, 5),\n    (1, 6),\n  ),\n  ChainTuple(\n    (2, 1),\n    (2, 2),\n    (2, 3),\n    (2, 4),\n  ),\n)\n Let's demonstrate  LayerMap  by assigning a specific rule to the last  Conv  layer at index  (1, 5) : composite = Composite(LayerMap((1, 5), EpsilonRule()))\n\nLRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  ZeroRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZeroRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZeroRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  ZeroRule() ,\n    Dense(1024 => 512, relu)  =>  ZeroRule() ,\n    Dropout(0.5)              =>  ZeroRule() ,\n    Dense(512 => 100, relu)   =>  ZeroRule() ,\n  ),\n) This approach also works with  Parallel  layers."},{"id":149,"pagetitle":"Assigning rules to layers","title":"Composite presets","ref":"/XAIDocs/RelevancePropagation/stable/generated/composites/#docs-composites-presets","content":" Composite presets RelevancePropagation.jl provides a set of default composites. A list of all implemented default composites can be found  in the API reference , e.g. the  EpsilonPlusFlat  composite: composite = EpsilonPlusFlat() Composite(\n  GlobalTypeMap(   # all layers \n     Flux.Conv                =>  ZPlusRule() ,\n     Flux.ConvTranspose       =>  ZPlusRule() ,\n     Flux.CrossCor            =>  ZPlusRule() ,\n     Flux.Dense               =>  EpsilonRule{Float32}(1.0f-6) ,\n     typeof(NNlib.dropout)    =>  PassRule() ,\n     Flux.AlphaDropout        =>  PassRule() ,\n     Flux.Dropout             =>  PassRule() ,\n     Flux.BatchNorm           =>  PassRule() ,\n     typeof(Flux.flatten)     =>  PassRule() ,\n     typeof(MLUtils.flatten)  =>  PassRule() ,\n     typeof(identity)         =>  PassRule() ,\n ),\n  FirstLayerTypeMap(   # first layer \n     Flux.Conv           =>  FlatRule() ,\n     Flux.ConvTranspose  =>  FlatRule() ,\n     Flux.CrossCor       =>  FlatRule() ,\n ),\n) analyzer = LRP(model, composite; flatten=false) LRP(\n  ChainTuple(\n    Conv((3, 3), 3 => 8, relu, pad=1)    =>  FlatRule() ,\n    Conv((3, 3), 8 => 8, relu, pad=1)    =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n    Conv((3, 3), 8 => 16, relu, pad=1)   =>  ZPlusRule() ,\n    Conv((3, 3), 16 => 16, relu, pad=1)  =>  ZPlusRule() ,\n    MaxPool((2, 2))                      =>  ZeroRule() ,\n  ),\n  ChainTuple(\n    Flux.flatten              =>  PassRule() ,\n    Dense(1024 => 512, relu)  =>  EpsilonRule{Float32}(1.0f-6) ,\n    Dropout(0.5)              =>  PassRule() ,\n    Dense(512 => 100, relu)   =>  EpsilonRule{Float32}(1.0f-6) ,\n  ),\n) This page was generated using  Literate.jl ."},{"id":152,"pagetitle":"Concept Relevance Propagation","title":"Concept relevance propagation","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#docs-crp","content":" Concept relevance propagation In  From attribution maps to human-understandable explanations through Concept Relevance Propagation  (CRP), Achtibat et al. propose the conditioning of LRP relevances on individual features of a model. Getting started This package is part the  Julia-XAI ecosystem  and builds on the basics shown in the  Getting started  guide from ExplainableAI.jl . We start out by loading the same pre-trained LeNet5 model and MNIST input data: using RelevancePropagation\nusing Flux\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.867 KiB. using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)"},{"id":153,"pagetitle":"Concept Relevance Propagation","title":"Step 1: Create LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-1:-Create-LRP-analyzer","content":" Step 1: Create LRP analyzer To create a CRP analyzer, first define an LRP analyzer with your desired rules: composite = EpsilonPlusFlat()\nlrp_analyzer = LRP(model, composite) LRP(\n  Conv((5, 5), 1 => 6, relu)   =>  FlatRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Conv((5, 5), 6 => 16, relu)  =>  ZPlusRule() ,\n  MaxPool((2, 2))              =>  ZeroRule() ,\n  Flux.flatten                 =>  PassRule() ,\n  Dense(256 => 120, relu)      =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(120 => 84, relu)       =>  EpsilonRule{Float32}(1.0f-6) ,\n  Dense(84 => 10)              =>  EpsilonRule{Float32}(1.0f-6) ,\n)"},{"id":154,"pagetitle":"Concept Relevance Propagation","title":"Step 2: Define concepts","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-2:-Define-concepts","content":" Step 2: Define concepts Then, specify the index of the layer on the outputs of which you want to condition the explanation. In this example, we are interested in the outputs of the last convolutional layer, layer 3: feature_layer = 3    # index of relevant layer in model\nmodel[feature_layer] # show layer Conv((5, 5), 6 => 16, relu)   # 2_416 parameters Then, specify the concepts / features you are interested in. To automatically select the  $n$  most relevant features, use  TopNFeatures . Note that for convolutional layers, a feature corresponds to an entire output channel of the layer. features = TopNFeatures(5) TopNFeatures(5) To manually specify features, use  IndexedFeatures . features = IndexedFeatures(1, 2, 10) IndexedFeatures(1, 2, 10)"},{"id":155,"pagetitle":"Concept Relevance Propagation","title":"Step 3: Use CRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Step-3:-Use-CRP-analyzer","content":" Step 3: Use CRP analyzer We can now create a  CRP  analyzer and use it like any other analyzer from RelevancePropagation.jl: analyzer = CRP(lrp_analyzer, feature_layer, features)\nheatmap(input, analyzer) (a vector displayed as a row to save space)"},{"id":156,"pagetitle":"Concept Relevance Propagation","title":"Using CRP on input batches","ref":"/XAIDocs/RelevancePropagation/stable/generated/crp/#Using-CRP-on-input-batches","content":" Using CRP on input batches Note that  CRP  uses the batch dimension to return explanations. When using CRP on batches, the explanations are first sorted by features, then inputs, e.g.  [c1_i1, c1_i2, c2_i1, c2_i2, c3_i1, c3_i2]  in the following example: x, y = MNIST(Float32, :test)[10:11]\nbatch = reshape(x, 28, 28, 1, :)\n\nheatmap(batch, analyzer) (a vector displayed as a row to save space) This page was generated using  Literate.jl ."},{"id":159,"pagetitle":"Supporting new layer types","title":"Supporting new layers and activation functions","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#docs-custom-layers","content":" Supporting new layers and activation functions One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research. This example will show you how to extent LRP to new layer types and activation functions. using Flux\nusing RelevancePropagation"},{"id":160,"pagetitle":"Supporting new layer types","title":"Model checks","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#docs-lrp-model-checks","content":" Model checks To assure that novice users use LRP according to best practices, RelevancePropagation.jl runs strict model checks when creating an  LRP  analyzer. Let's demonstrate this by defining a new layer type that doubles its input struct MyDoublingLayer end\n(::MyDoublingLayer)(x) = 2 * x\n\nmylayer = MyDoublingLayer()\nmylayer([1, 2, 3]) 3-element Vector{Int64}:\n 2\n 4\n 6 and by defining a model that uses this layer: model = Chain(Dense(100, 20), MyDoublingLayer()); Creating an LRP analyzer, e.g.  LRP(model) , will throw an  ArgumentError  and print a summary of the model check in the REPL: julia> LRP(model)\n  ChainTuple(\n    Dense(100 => 20)  => supported,\n    MyDoublingLayer() => unknown layer type,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model LRP should only be used on deep rectifier networks and RelevancePropagation doesn't recognize  MyDoublingLayer  as a compatible layer by default. It will therefore return an error and a model check summary instead of returning an incorrect explanation. However, if we know  MyDoublingLayer  is compatible with deep rectifier networks, we can register it to tell RelevancePropagation that it is ok to use. This will be shown in the following section."},{"id":161,"pagetitle":"Supporting new layer types","title":"Registering layers","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Registering-layers","content":" Registering layers The error in the model check will stop after registering our custom layer type  MyDoublingLayer  as \"supported\" by RelevancePropagation. This is done using the function  LRP_CONFIG.supports_layer , which should be set to return  true  for the type  MyDoublingLayer : LRP_CONFIG.supports_layer(::MyDoublingLayer) = true Now we can create and run an analyzer without getting an error: analyzer = LRP(model) LRP(\n  Dense(100 => 20)                                         =>  ZeroRule() ,\n  Main.MyDoublingLayer()  =>  ZeroRule() ,\n) Registering functions Flux's  Chains  can also contain functions, e.g.  flatten . This kind of layer can be registered as LRP_CONFIG.supports_layer(::typeof(flatten)) = true"},{"id":162,"pagetitle":"Supporting new layer types","title":"Registering activation functions","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Registering-activation-functions","content":" Registering activation functions The mechanism for registering custom activation functions is analogous to that of custom layers: myrelu(x) = max.(0, x)\n\nmodel = Chain(Dense(784, 100, myrelu), Dense(100, 10)); Once again, creating an LRP analyzer for this model will throw an  ArgumentError  and display the following model check summary: julia> LRP(model)\n  ChainTuple(\n    Dense(784 => 100, myrelu) => unsupported or unknown activation function myrelu,\n    Dense(100 => 10)          => supported,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model Registation works by defining the function  LRP_CONFIG.supports_activation  as  true : LRP_CONFIG.supports_activation(::typeof(myrelu)) = true now the analyzer can be created without error: analyzer = LRP(model) LRP(\n  Dense(784 => 100, myrelu)  =>  ZeroRule() ,\n  Dense(100 => 10)           =>  ZeroRule() ,\n)"},{"id":163,"pagetitle":"Supporting new layer types","title":"Skipping model checks","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_layer/#Skipping-model-checks","content":" Skipping model checks All model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument  skip_checks=true . struct UnknownLayer end\n(::UnknownLayer)(x) = x\n\nunknown_activation(x) = max.(0, x)\n\nmodel = Chain(Dense(100, 20, unknown_activation), MyDoublingLayer())\n\nLRP(model; skip_checks=true) LRP(\n  Dense(100 => 20, unknown_activation)                     =>  ZeroRule() ,\n  Main.MyDoublingLayer()  =>  ZeroRule() ,\n) Instead of throwing the usual  ERROR: Unknown layer or activation function found in model , the LRP analyzer was created without having to register either the layer  UnknownLayer  or the activation function  unknown_activation . This page was generated using  Literate.jl ."},{"id":166,"pagetitle":"Custom LRP rules","title":"Custom LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#docs-custom-rules","content":" Custom LRP rules One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research. This example will show you how to implement custom LRP rules. Getting started This package is part the  Julia-XAI ecosystem  and builds on the basics shown in the  Getting started  guide from ExplainableAI.jl . We start out by loading the same pre-trained LeNet5 model and MNIST input data: using RelevancePropagation\nusing Flux\nusing MLDatasets\nusing ImageCore\nusing BSON\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nmodel Chain(\n  Conv((5, 5), 1 => 6, relu),            # 156 parameters \n  MaxPool((2, 2)),\n  Conv((5, 5), 6 => 16, relu),           # 2_416 parameters \n  MaxPool((2, 2)),\n  Flux.flatten,\n  Dense(256 => 120, relu),               # 30_840 parameters \n  Dense(120 => 84, relu),                # 10_164 parameters \n  Dense(84 => 10),                       # 850 parameters \n)                    # Total: 10 arrays,  44_426 parameters, 174.867 KiB."},{"id":167,"pagetitle":"Custom LRP rules","title":"Implementing a custom rule","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Implementing-a-custom-rule","content":" Implementing a custom rule"},{"id":168,"pagetitle":"Custom LRP rules","title":"Step 1: Define rule struct","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Step-1:-Define-rule-struct","content":" Step 1: Define rule struct Let's define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of supertype  AbstractLRPRule . struct MyGammaRule <: AbstractLRPRule end"},{"id":169,"pagetitle":"Custom LRP rules","title":"Step 2: Implement rule behavior","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#docs-custom-rules-impl","content":" Step 2: Implement rule behavior It is then possible to dispatch on the following four utility functions with the rule type  MyCustomLRPRule  to define custom rules without writing boilerplate code. modify_input(rule::MyGammaRule, input) modify_parameters(rule::MyGammaRule, parameter) modify_denominator(rule::MyGammaRule, denominator) is_compatible(rule::MyGammaRule, layer) By default: modify_input  doesn't change the input modify_parameters  doesn't change the parameters modify_denominator  avoids division by zero by adding a small epsilon-term ( 1.0f-9 ) is_compatible  returns  true  if a layer has fields  weight  and  bias To extend internal functions, import them explicitly: import RelevancePropagation: modify_parameters\n\nmodify_parameters(::MyGammaRule, param) = param + 0.25f0 * relu.(param) modify_parameters (generic function with 7 methods) Note that we didn't implement three of the four functions. This is because the defaults are sufficient to implement the  GammaRule ."},{"id":170,"pagetitle":"Custom LRP rules","title":"Step 3: Use rule in LRP analyzer","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Step-3:-Use-rule-in-LRP-analyzer","content":" Step 3: Use rule in LRP analyzer We can directly use our rule to make an analyzer! rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    MyGammaRule(), # our custom GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer) We just implemented our own version of the  $γ$ -rule in 2 lines of code. The heatmap perfectly matches the pre-implemented  GammaRule : rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    GammaRule(), # XAI.jl's GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)"},{"id":171,"pagetitle":"Custom LRP rules","title":"Performance tips","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Performance-tips","content":" Performance tips Make sure functions like  modify_parameters  don't promote the type of weights (e.g. from  Float32  to  Float64 ). If your rule  MyRule  doesn't modify weights or biases, defining  modify_layer(::MyRule, layer) = nothing  can provide reduce memory allocations and improve performance."},{"id":172,"pagetitle":"Custom LRP rules","title":"Advanced layer modification","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#docs-custom-rules-advanced","content":" Advanced layer modification For more granular control over weights and biases,  modify_weight  and  modify_bias  can be used. If the layer doesn't use weights ( layer.weight ) and biases ( layer.bias ), RelevancePropagation provides a lower-level variant of  modify_parameters  called  modify_layer . This function is expected to take a layer and return a new, modified layer. To add compatibility checks between rule and layer types, extend  is_compatible . Extending modify_layer Use of a custom function  modify_layer  will overwrite functionality of  modify_parameters ,  modify_weight  and  modify_bias  for the implemented combination of rule and layer types. This is due to the fact that internally,  modify_weight  and  modify_bias  are called by the default implementation of  modify_layer .  modify_weight  and  modify_bias  in turn call  modify_parameters  by default. The default call structure looks as follows: ┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘ Therefore  modify_layer  should only be extended for a specific rule and a specific layer type."},{"id":173,"pagetitle":"Custom LRP rules","title":"Advanced LRP rules","ref":"/XAIDocs/RelevancePropagation/stable/generated/custom_rules/#Advanced-LRP-rules","content":" Advanced LRP rules To implement custom LRP rules that require more than  modify_layer ,  modify_input  and  modify_denominator , take a look at the  LRP developer documentation . This page was generated using  Literate.jl ."},{"id":176,"pagetitle":"Home","title":"VisionHeatmaps.jl","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.jl","content":" VisionHeatmaps.jl Documentation for  VisionHeatmaps.jl ."},{"id":177,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/VisionHeatmaps/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run ]add VisionHeatmaps"},{"id":178,"pagetitle":"Home","title":"API","ref":"/XAIDocs/VisionHeatmaps/stable/#API","content":" API"},{"id":179,"pagetitle":"Home","title":"VisionHeatmaps.heatmap","ref":"/XAIDocs/VisionHeatmaps/stable/#VisionHeatmaps.heatmap","content":" VisionHeatmaps.heatmap  —  Function heatmap(x) Visualize 4D arrays as heatmaps, assuming the WHCN convention for input array dimensions (width, height, color channels, batch dimension). Keyword arguments colorscheme::Union{ColorScheme,Symbol} : Color scheme from ColorSchemes.jl. Defaults to  seismic . reduce::Symbol : Selects how color channels are reduced to a single number to apply a color scheme. The following methods can be selected, which are then applied over the color channels for each \"pixel\" in the array: :sum : sum up color channels :norm : compute 2-norm over the color channels :maxabs : compute  maximum(abs, x)  over the color channels Defaults to  :sum . rangescale::Symbol : Selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered . process_batch::Bool : When heatmapping a batch, setting  process_batch=true  will apply the  rangescale  normalization to the entire batch instead of computing it individually for each sample in the batch. Defaults to  false . permute::Bool : Whether to flip W&H input channels. Default is  true . unpack_singleton::Bool : If false,  heatmap  will always return a vector of images. When heatmapping a batch with a single sample, setting  unpack_singleton=true  will unpack the singleton vector and directly return the image. Defaults to  true . source"},{"id":182,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Getting-started","content":" Getting started Let's assume you took the following image  img , reshaped it to WHCN format  (width, height, color channels, batch dimension)  and ran it through a vision model: using Images\nimg = load(joinpath(asset_dir, \"img1.png\")) # load image file You might use an input space attribution method  (for example from  ExplainableAI.jl ) to determine which parts of the input contributed most to the \"saxophone\" class. Let's load such an attribution  val  in WHCN format: val = load(data_heatmap, \"x\") # load precomputed array from file\ntypeof(val) Array{Float32, 4} size(val) (224, 224, 3, 1) To make this attribution more interpretable, we can visualize it as a heatmap: using VisionHeatmaps\nheatmap(val)"},{"id":183,"pagetitle":"Getting started","title":"Custom color schemes","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Custom-color-schemes","content":" Custom color schemes We can partially or fully override presets by passing keyword arguments to  heatmap . For example, we can use a custom color scheme from ColorSchemes.jl using the keyword argument  cs : using ColorSchemes\nheatmap(val; colorscheme=ColorSchemes.jet) heatmap(val; colorscheme=ColorSchemes.inferno) Refer to the  ColorSchemes.jl catalogue  for a gallery of available color schemes."},{"id":184,"pagetitle":"Getting started","title":"Custom color channel reduction","ref":"/XAIDocs/VisionHeatmaps/stable/example/#docs-heatmap-reduce","content":" Custom color channel reduction For arrays with multiple color channels, the channels need to be reduced to a single scalar value for each pixel, which is later mapped onto a color scheme. The following presets are available for this purpose: :sum : sum up color channels (default setting) :norm : compute 2-norm over color channels :maxabs : compute  maximum(abs, x)  over the color channels heatmap(val; reduce=:sum) heatmap(val; reduce=:norm) heatmap(val; reduce=:maxabs) Using the default  reduce=:sum  visibly leaves more negative values in the heatmap, highlighting only the saxophone."},{"id":185,"pagetitle":"Getting started","title":"Mapping reduced values onto a color scheme","ref":"/XAIDocs/VisionHeatmaps/stable/example/#docs-heatmap-rangescale","content":" Mapping reduced values onto a color scheme To map the now  color-channel-reduced  array onto a color scheme, we first need to normalize all values to the range  $[0, 1]$ . For this purpose, two presets are available through the  rangescale  keyword argument: :extrema : normalize to the minimum and maximum value in the array. :centered : normalize to the maximum absolute value of the array. Values of zero will be mapped to the center of the color scheme. Depending on the color scheme, one of these presets may be more suitable than the other. The default color scheme,  seismic , is centered around zero, making  :centered  a good choice: heatmap(val; rangescale=:centered) With centered color schemes such as  seismic ,   :extrema  should be avoided, as it leads to visual artifacts: heatmap(val; rangescale=:extrema) However, for the  inferno  color scheme, which is not centered around zero,  :extrema  can lead to a heatmap with higher contrast. heatmap(val; colorscheme=ColorSchemes.inferno, rangescale=:centered) heatmap(val; colorscheme=ColorSchemes.inferno, rangescale=:extrema) For the full list of  heatmap  keyword arguments, refer to the  heatmap  documentation."},{"id":186,"pagetitle":"Getting started","title":"Heatmapping batches","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Heatmapping-batches","content":" Heatmapping batches heatmap  can also be used to visualize input batches. Let's assume we computed an input space attribution  val_batch  for the following images: imgs = [load(joinpath(asset_dir, f)) for f in (\"img1.png\", \"img2.png\", \"img3.png\", \"img4.png\", \"img5.png\")] # load image files (a vector displayed as a row to save space) Once again, we assume that  val_batch  is in WHCN format: val_batch = load(data_heatmaps, \"x\") # load precomputed array from file\ntypeof(val_batch) Array{Float32, 4} size(val_batch) (224, 224, 3, 5) Calling  heatmap  will automatically return an vector of images: heatmap(val_batch) (a vector displayed as a row to save space) These heatmaps can be customized as usual: heatmap(val_batch; colorscheme=ColorSchemes.inferno, rangescale=:extrema) (a vector displayed as a row to save space)"},{"id":187,"pagetitle":"Getting started","title":"Processing batches","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Processing-batches","content":" Processing batches The normalization when  mapping values onto a color scheme   can optionally be computed for a batch.  Using the example of  rangescale=:extrema , this means that the minimum and maximum value will be computed over all images in the batch, instead of individually for each image. Note that this will lead to different heatmaps for each image, based on other images in the batch. heatmap(val_batch; process_batch=true) (a vector displayed as a row to save space) heatmap(val_batch; colorscheme=ColorSchemes.inferno, rangescale=:extrema, process_batch=true) (a vector displayed as a row to save space)"},{"id":188,"pagetitle":"Getting started","title":"Consistent output types","ref":"/XAIDocs/VisionHeatmaps/stable/example/#Consistent-output-types","content":" Consistent output types As we have seen, calling  heatmap  on an array of size  (W, H, C, 1)   will return a single heatmap image, while calling it on an array of size  (W, H, C, N)  will return a vector of heatmap.  This is due to the fact that VisionHeatmaps.jl will automatically \"unpack\" singleton vectors of heatmaps. If this behavior is not desired, the keyword argument  unpack_singleton  can be set to  false : heatmap(val; unpack_singleton=false) (a vector displayed as a row to save space)"},{"id":191,"pagetitle":"Home","title":"TextHeatmaps.jl","ref":"/XAIDocs/TextHeatmaps/stable/#TextHeatmaps.jl","content":" TextHeatmaps.jl Documentation for  TextHeatmaps.jl ."},{"id":192,"pagetitle":"Home","title":"Installation","ref":"/XAIDocs/TextHeatmaps/stable/#Installation","content":" Installation To install this package and its dependencies, open the Julia REPL and run ]add TextHeatmaps"},{"id":193,"pagetitle":"Home","title":"API","ref":"/XAIDocs/TextHeatmaps/stable/#API","content":" API"},{"id":194,"pagetitle":"Home","title":"TextHeatmaps.heatmap","ref":"/XAIDocs/TextHeatmaps/stable/#TextHeatmaps.heatmap","content":" TextHeatmaps.heatmap  —  Function heatmap(values, words) Create a heatmap of words where the background color of each word is determined by its corresponding value. Arguments  values  and  words  (and optionally  colors ) must have the same size. Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  ColorSchemes.seismic . rangescale::Symbol : selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered  for use with the default color scheme  seismic . source"},{"id":197,"pagetitle":"Getting started","title":"Getting started","ref":"/XAIDocs/TextHeatmaps/stable/example/#Getting-started","content":" Getting started Let's assume you put the following text into a sentiment analysis model: text = \"I loved the concert but not the opening act\"\nwords = split(text) 9-element Vector{SubString{String}}:\n \"I\"\n \"loved\"\n \"the\"\n \"concert\"\n \"but\"\n \"not\"\n \"the\"\n \"opening\"\n \"act\" The model returns a vector of sentiment scores for each word,  where positive values indicate positive sentiment  and negative values indicate negative sentiment: val = [0.1, 2.5, 0.0, 0.3, -0.6, -1.4, 0.0, 0.1, -0.1] To visualize the sentiment scores, we can use the  heatmap  function: using TextHeatmaps\n\nheatmap(val, words) I loved the concert but not the opening act"},{"id":198,"pagetitle":"Getting started","title":"Color schemes","ref":"/XAIDocs/TextHeatmaps/stable/example/#Color-schemes","content":" Color schemes We can use a custom color scheme from ColorSchemes.jl using the keyword argument cs: using ColorSchemes\nheatmap(val, words; colorscheme=ColorSchemes.jet) I loved the concert but not the opening act heatmap(val, words; colorscheme=ColorSchemes.inferno) I loved the concert but not the opening act"},{"id":199,"pagetitle":"Getting started","title":"Mapping values onto the color scheme","ref":"/XAIDocs/TextHeatmaps/stable/example/#Mapping-values-onto-the-color-scheme","content":" Mapping values onto the color scheme To map values onto a color scheme, we first need to normalize all values to the range  $[0,1]$ . For this purpose, two presets are available through the  rangescale  keyword argument: :extrema : normalize to the minimum and maximum value of the explanation :centered : normalize to the maximum absolute value of the explanation.    Values of zero will be mapped to the center of the color scheme. Depending on the color scheme, one of these presets may be more suitable than the other. The default color scheme,  seismic , is centered around zero, making  :centered  a good choice: heatmap(val, words; rangescale=:centered) I loved the concert but not the opening act With the  seismic  colorscheme, the  :extrema  rangescale should be avoided: Even though the word \"concert\" has a positive sentiment score of  0.3 , it is colored in blue: heatmap(val, words; rangescale=:extrema) I loved the concert but not the opening act However, for the  inferno  color scheme, which is not centered around zero,  :extrema  leads to a heatmap with higher contrast. heatmap(val, words; colorscheme=ColorSchemes.inferno, rangescale=:centered) I loved the concert but not the opening act heatmap(val, words; colorscheme=ColorSchemes.inferno, rangescale=:extrema) I loved the concert but not the opening act"},{"id":200,"pagetitle":"Getting started","title":"Terminal support","ref":"/XAIDocs/TextHeatmaps/stable/example/#Terminal-support","content":" Terminal support In the context of this documentation page and notebooks, heatmaps are rendered using HTML. TextHeatmaps.jl also supports rendering heatmaps in the terminal. Here we use the  print  function to force Documenter.jl to render the heatmap as raw text: heatmap(val, words) |> print I   loved   the   concert   but   not   the   opening   act"},{"id":203,"pagetitle":"XAIBase Interface","title":"Interface description","ref":"/XAIDocs/XAIBase/stable/#docs-interface","content":" Interface description XAIBase.jl is a light-weight dependency that defines the interface of XAI methods  in the  Julia-XAI ecosystem . Building on top of XAIBase  (or providing an interface via  package extensions ) makes your package compatible with the Julia-XAI ecosystem, allowing you to automatically compute heatmaps for vision and language models.  This only requires you to fulfill the following two requirements: An XAI method has to be a subtype of  AbstractXAIMethod An XAI method has to implement the following method:  (method::MyMethod)(input, output_selector::AbstractNeuronSelector) The method has to return an  Explanation The input is expected to have a batch dimensions as its last dimension When applied to a batch, the method returns a single  Explanation ,  which contains the batched output in the  val  field. AbstractNeuronSelector s are predefined callable structs  that select a single scalar value from a model's output,  e.g. the maximally activated output neuron of a classifier using  XAIBase.MaxActivationSelector  or a specific output neuron using  XAIBase.IndexSelector . Refer to the  Explanation  documentation for a description of the expected fields. For more information, take a look at  src/XAIBase.jl ."},{"id":204,"pagetitle":"XAIBase Interface","title":"Example implementation","ref":"/XAIDocs/XAIBase/stable/#Example-implementation","content":" Example implementation struct MyMethod{M} <: AbstractXAIMethod \n    model::M    \nend\n\nfunction (method::MyMethod)(input, output_selector::AbstractNeuronSelector)\n    output = method.model(input)\n    output_selection = output_selector(output)\n\n    val = ...         # your method's implementation\n    extras = nothing  # optionally add additional information using a named tuple\n    return Explanation(val, output, output_selection, :MyMethod, :attribution, extras)\nend"},{"id":207,"pagetitle":"API Reference","title":"Using analyzers","ref":"/XAIDocs/XAIBase/stable/api/#Using-analyzers","content":" Using analyzers Most methods in the Julia-XAI ecosystem work by calling  analyze  on an input and an analyzer:"},{"id":208,"pagetitle":"API Reference","title":"XAIBase.analyze","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.analyze","content":" XAIBase.analyze  —  Function analyze(input, method)\nanalyze(input, method, neuron_selection) Apply the analyzer  method  for the given input, returning an  Explanation . If  neuron_selection  is specified, e.g. the index of a specific output neuron, the explanation will be calculated for that neuron. Otherwise, the output neuron with the highest activation is automatically chosen. See also  Explanation  and  heatmap . Keyword arguments add_batch_dim : add batch dimension to the input without allocating. Default is  false . source The return type of  analyze  is an  Explanation :"},{"id":209,"pagetitle":"API Reference","title":"XAIBase.Explanation","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.Explanation","content":" XAIBase.Explanation  —  Type Return type of analyzers when calling  analyze . Fields val : numerical output of the analyzer, e.g. an attribution or gradient output : model output for the given analyzer input output_selection : index of the output used for the explanation analyzer : symbol corresponding the used analyzer, e.g.  :Gradient  or  :LRP heatmap : symbol indicating a preset heatmapping style,   e.g.  :attibution ,  :sensitivity  or  :cam extras : optional named tuple that can be used by analyzers   to return additional information. source"},{"id":210,"pagetitle":"API Reference","title":"Visualizing explanations","ref":"/XAIDocs/XAIBase/stable/api/#Visualizing-explanations","content":" Visualizing explanations Explanation s can be visualized using  heatmap :"},{"id":211,"pagetitle":"API Reference","title":"XAIBase.heatmap","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.heatmap","content":" XAIBase.heatmap  —  Function heatmap(explanation) Visualize  Explanation  from XAIBase as a vision heatmap. Assumes WHCN convention (width, height, channels, batchsize) for  explanation.val . Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  :seismic . reduce::Symbol : Selects how color channels are reduced to a single number to apply a color scheme. The following methods can be selected, which are then applied over the color channels for each \"pixel\" in the array: :sum : sum up color channels :norm : compute 2-norm over the color channels :maxabs : compute  maximum(abs, x)  over the color channels Defaults to  :sum . rangescale::Symbol : Selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered . process_batch::Bool : When heatmapping a batch, setting  process_batch=true  will apply the  rangescale  normalization to the entire batch instead of computing it individually for each sample in the batch. Defaults to  false . permute::Bool : Whether to flip W&H input channels. Default is  true . unpack_singleton::Bool : If false,  heatmap  will always return a vector of images. When heatmapping a batch with a single sample, setting  unpack_singleton=true  will unpack the singleton vector and directly return the image. Defaults to  true . source heatmap(input, analyzer) Compute an  Explanation  for a given  input  using the method  analyzer  and visualize it as a vision heatmap. Any additional arguments and keyword arguments are passed to the analyzer. Refer to  analyze  for more information on available keyword arguments. To customize the heatmapping style, first compute an explanation using  analyze  and then call  heatmap  on the explanation. source heatmap(explanation, text) Visualize  Explanation  from XAIBase as text heatmap. Text should be a vector containing vectors of strings, one for each input in the batched explanation. Keyword arguments colorscheme::Union{ColorScheme,Symbol} : color scheme from ColorSchemes.jl. Defaults to  :seismic . rangescale::Symbol : selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either  :extrema  or  :centered . Defaults to  :centered  for use with the default color scheme  :seismic . source"},{"id":212,"pagetitle":"API Reference","title":"Feature selection","ref":"/XAIDocs/XAIBase/stable/api/#Feature-selection","content":" Feature selection"},{"id":213,"pagetitle":"API Reference","title":"XAIBase.IndexedFeatures","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.IndexedFeatures","content":" XAIBase.IndexedFeatures  —  Type IndexedFeatures(indices...) Select features by indices. For outputs of convolutional layers, the index refers to a feature dimension. See also See also  TopNFeatures . source"},{"id":214,"pagetitle":"API Reference","title":"XAIBase.TopNFeatures","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.TopNFeatures","content":" XAIBase.TopNFeatures  —  Type TopNFeatures(n) Select top-n features. For outputs of convolutional layers, the relevance is summed across height and width channels for each feature. See also  IndexedFeatures . source"},{"id":215,"pagetitle":"API Reference","title":"Internals","ref":"/XAIDocs/XAIBase/stable/api/#Internals","content":" Internals Neuron selectors:"},{"id":216,"pagetitle":"API Reference","title":"XAIBase.MaxActivationSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.MaxActivationSelector","content":" XAIBase.MaxActivationSelector  —  Type MaxActivationSelector() Neuron selector that picks the output neuron with the highest activation. source"},{"id":217,"pagetitle":"API Reference","title":"XAIBase.IndexSelector","ref":"/XAIDocs/XAIBase/stable/api/#XAIBase.IndexSelector","content":" XAIBase.IndexSelector  —  Type IndexSelector(index) Neuron selector that picks the output neuron at the given index. source"},{"id":218,"pagetitle":"API Reference","title":"Index","ref":"/XAIDocs/XAIBase/stable/api/#Index","content":" Index XAIBase.Explanation XAIBase.IndexSelector XAIBase.IndexedFeatures XAIBase.MaxActivationSelector XAIBase.TopNFeatures XAIBase.analyze XAIBase.heatmap"}]